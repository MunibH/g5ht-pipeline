{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f49ca6",
   "metadata": {},
   "source": [
    "### CONDA ENVIRONMENTS\n",
    "\n",
    "For steps __1. preprocess__ and __2. mip__, `conda activate g5ht-pipeline`\n",
    "\n",
    "For step __3. segment__, `conda activate segment-torch` or `conda activate torchcu129`\n",
    "\n",
    "For step __4. spline, 5. orient, 6. warp, 7. reg__\n",
    "\n",
    "## TODO:\n",
    "\n",
    "1. I wonder if I computed a spline on each and every z slice and warped each, oriented each of them, and warped each of them, if the problem of weirdly sheared image stacks would be solved\n",
    "2. quick mp4 for all recordings\n",
    "   1. now working in engaging, works per one nd2 sbatch\n",
    "3. focus check for all recordings\n",
    "   1. maybe focus check can be used to specify which z slices are good to use and which frames are good to use\n",
    "4. for recordings starting in december 2025, need to trim first 2 rather than last 2 z slices\n",
    "5. flip worms so that VNC is always up\n",
    "6. fixed mask could be automated, but if not, make sure to save which index is fixed\n",
    "7. extract behavior\n",
    "8. posture similarity\n",
    "   1. posture might consist of the spline + thresholded z-stack\n",
    "      1. I'm thinking that the orientation shouldn't matter, but the z-planes in focus will, and curvature/spline of the head will\n",
    "      2. maybe need to actually interpolate to 117 z slices\n",
    "   2. sub registration problems\n",
    "   3. label each set of registered frames with one set of ROIs, or auto segment ROIs from each set of registered frames\n",
    "9.  track z over time, which zslices are consistent\n",
    "   1. focus + correlation\n",
    "10. beads -> train/test\n",
    "11. gfp+1 relative to rfp channel (might only apply to pre december 2025 recordings)\n",
    "12. wholistic \n",
    "    1.  parameter sweep, might change\n",
    "    2.  python version\n",
    "13. autocorr/scorr\n",
    "14. automate z slice trimming\n",
    "    1.  pre december 2025 (trim last 2 z slices)\n",
    "    2.  post december 2025 (trim first z slice)\n",
    "15. photobleaching estimation?\n",
    "    1.  record immo with serotonin\n",
    "    2.  at least do it for RFP\n",
    "16. try deltaF/F [ (F(t) - F0) / F0 ]\n",
    "17. coding directions (preencounter-baseline) (postencounter-baseline)\n",
    "    1.  then show voxel weights\n",
    "18. port everything to engaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60d2b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    is_torch_env = False\n",
    "except ImportError:\n",
    "    is_torch_env = True\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252d5dd",
   "metadata": {},
   "source": [
    "## SPECIFY DATA TO PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66403bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2\n",
      "Num z-slices:  41\n",
      "Number of frames:  1200\n",
      "Height:  512\n",
      "width:  512\n",
      "Number of channels:  2\n",
      "Beads alignment file:  None\n"
     ]
    }
   ],
   "source": [
    "DATA_PTH = r'D:\\DATA\\g5ht-free\\20251028'\n",
    "\n",
    "INPUT_ND2 = 'date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2'\n",
    "\n",
    "INPUT_ND2_PTH = os.path.join(DATA_PTH, INPUT_ND2)\n",
    "\n",
    "NOISE_PTH = r'C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\noise\\noise_042925.tif'\n",
    "\n",
    "OUT_DIR = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "STACK_LENGTH = 41\n",
    "\n",
    "if not is_torch_env:\n",
    "    noise_stack = utils.get_noise_stack(NOISE_PTH, STACK_LENGTH)\n",
    "    num_frames, height, width, num_channels = utils.get_range_from_nd2(INPUT_ND2_PTH, stack_length=STACK_LENGTH) \n",
    "    beads_alignment_file = utils.get_beads_alignment_file(INPUT_ND2_PTH)\n",
    "else:\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")\n",
    "\n",
    "print(INPUT_ND2)\n",
    "print('Num z-slices: ', STACK_LENGTH)\n",
    "if not is_torch_env:\n",
    "    print('Number of frames: ', num_frames)\n",
    "    print('Height: ', height)\n",
    "    print('width: ', width)\n",
    "    print('Number of channels: ', num_channels)\n",
    "    print('Beads alignment file: ', beads_alignment_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835cabb",
   "metadata": {},
   "source": [
    "## 10. LABEL ROIs\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- after this step, use `lbl` conda env to label ROI of fixed frame\n",
    "  - run `labelme` in terminal\n",
    "\n",
    "\n",
    "maybe also see here for video annotation: https://github.com/wkentaro/labelme/tree/main/examples/video_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1d48b",
   "metadata": {},
   "source": [
    "### EXPORT FIXED VOLUME AS PNGs for labeling with `labelme`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "683fca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that exports each z-slice of fixed.tif as a separate png\n",
    "import tifffile\n",
    "import os\n",
    "import glob\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "# in PTH directory, find a fixed_XXXX*.tif file, where XXXX are digits\n",
    "fixed_fn = glob.glob(os.path.join(PTH, 'fixed_[0-9][0-9][0-9][0-9]*.tif'))[0]\n",
    "fixed_pth = os.path.join(PTH, fixed_fn)\n",
    "\n",
    "# fixed_pth = os.path.join(PTH, 'fixed.tif')\n",
    "# fixed_stack = ndi.zoom(tifffile.imread(fixed_pth), zoom=(3,1,1,1))\n",
    "fixed_stack = tifffile.imread(fixed_pth)\n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_png')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for i in range(fixed_stack.shape[0]):\n",
    "    slice_pth = os.path.join(out_dir, f'fixed_z{i:02d}.png')\n",
    "    # make sure to save channel 1, and that it is visible, correct data type, clipped to 0-255\n",
    "    slice_img = fixed_stack[i,1,:,:]\n",
    "    slice_img = (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255\n",
    "    slice_img = slice_img.astype('uint8')\n",
    "    tifffile.imwrite(slice_pth, slice_img) \n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_xz_png')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for i in range(fixed_stack.shape[2]):\n",
    "    slice_pth = os.path.join(out_dir, f'fixed_xz{i:02d}.png')\n",
    "    # make sure to save channel 1, and that it is visible, correct data type, clipped to 0-255\n",
    "    slice_img = fixed_stack[:,1,i,:]\n",
    "    slice_img = (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255\n",
    "    slice_img = slice_img.astype('uint8')\n",
    "    tifffile.imwrite(slice_pth, slice_img) \n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_yz_png')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for i in range(fixed_stack.shape[3]):\n",
    "    slice_pth = os.path.join(out_dir, f'fixed_yz{i:02d}.png')\n",
    "    # make sure to save channel 1, and that it is visible, correct data type, clipped to 0-255\n",
    "    slice_img = fixed_stack[:,1,:,i]\n",
    "    slice_img = (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255\n",
    "    slice_img = slice_img.astype('uint8')\n",
    "    # save as high quality tiff\n",
    "    tifffile.imwrite(slice_pth, slice_img) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d62cd",
   "metadata": {},
   "source": [
    "### PARSE OUTPUT OF `labelme`\n",
    "\n",
    "- outputs `roi.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e1e46fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NR', 'isthmus', 'DNC', 'VNC']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from skimage.draw import polygon\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_png')\n",
    "fixed_fn = glob.glob(os.path.join(PTH, 'fixed_[0-9][0-9][0-9][0-9]*.tif'))[0]\n",
    "fixed_pth = os.path.join(PTH, fixed_fn)\n",
    "fixed_stack = tifffile.imread(fixed_pth)\n",
    "Z,C,H,W = fixed_stack.shape\n",
    "\n",
    "roi = np.zeros((Z, H, W), dtype=fixed_stack.dtype) # ZHW\n",
    "\n",
    "# get all unique roi_labels from json files\n",
    "roi_json_files = glob.glob(os.path.join(out_dir, 'fixed_z[0-9][0-9]*.json'))\n",
    "roi_labels = []\n",
    "for roi_json_file in roi_json_files:\n",
    "    with open(roi_json_file, 'r') as f:\n",
    "        roi_dict = json.load(f)\n",
    "        roi_labels.append([shape['label'] for shape in roi_dict['shapes']])\n",
    "# get all unique roi_labels\n",
    "roi_labels = list(set([item for sublist in roi_labels for item in sublist]))\n",
    "print(roi_labels)\n",
    "\n",
    "# # roi_labels = ['PC','MC','IM','TB','NR','VNC','DNC']\n",
    "# # procorpus, metacorpus, isthmus, terminal bulb, nerve ring, ventral nerve cord, dorsal nerve cord\n",
    "\n",
    "for i in range(Z):\n",
    "    slice_roi_json = os.path.join(out_dir, f'fixed_z{i:02d}.json')\n",
    "    # if slice_roi_json doesn't exist, continue\n",
    "    if not os.path.exists(slice_roi_json):\n",
    "        continue\n",
    "    with open(slice_roi_json, 'r') as f:\n",
    "        roi_dict = json.load(f)\n",
    "        # loop through each shape in roi_dict['shapes']\n",
    "        for shape in roi_dict['shapes']:\n",
    "            label = shape['label']\n",
    "            if label in roi_labels:\n",
    "                points = shape['points']\n",
    "                # get integer coordinates\n",
    "                points = [(int(round(p[1])), int(round(p[0]))) for p in points]\n",
    "                # create a mask for the polygon\n",
    "                \n",
    "                rr, cc = polygon([p[0] for p in points], [p[1] for p in points], shape=(H,W))\n",
    "                # should set to correct z slice\n",
    "                roi[i, rr, cc] = roi_labels.index(label) + 1 # start from 1\n",
    "\n",
    "# save roi stack as tif image, imagej=true and save the roi labels as metadata\n",
    "roi_pth = os.path.join(PTH, 'roi.tif')\n",
    "tifffile.imwrite(roi_pth, roi.astype(np.uint8), imagej=True, metadata={'Labels': roi_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982518c",
   "metadata": {},
   "source": [
    "## 11. QUANTIFY\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "Have to first label dorsal and ventral nerve rings and pharynx. See ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b467a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import quantify\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import matplotlib\n",
    "font = {'family' : 'Arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "_ = importlib.reload(sys.modules['quantify'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "REG_DIR = r'registered_elastix'\n",
    "PLOT_ONLY = True\n",
    "time_type = 'frame'  # 'min', 'sec', or 'frame'\n",
    "baseline_window = (0,60) # specify baseline frame range, default is (0, 60)\n",
    "\n",
    "# %matplotlib inline\n",
    "%matplotlib qt\n",
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "sys.argv = [\"\", PTH, REG_DIR, baseline_window, PLOT_ONLY, time_type]\n",
    "quantify.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f608866",
   "metadata": {},
   "source": [
    "## 12 QUANTIFY VOXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87cfdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1200 files using 24 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stacks: 100%|██████████| 1200/1200 [00:32<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data shape: (1200, 39, 100, 250)\n",
      "Binning factor: 2\n",
      "Saved normalized data (ratiometric) to D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001\\normalized_voxels.npy\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import quantify_voxels\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.morphology import erosion, disk\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "_ = importlib.reload(sys.modules['quantify_voxels'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "reg_dir = 'registered_elastix'\n",
    "bin_factor = 2\n",
    "fps = 1/0.533\n",
    "baseline_window = (0,60) # specify baseline frame range, default is (0, 60)\n",
    "\n",
    "sys.argv = [\"\", PTH, reg_dir, bin_factor, baseline_window]\n",
    "quantify_voxels.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalized_voxels.npy, also been binned\n",
    "g5 = np.load(os.path.join(PTH, 'normalized_voxels.npy'))\n",
    "rfp_mean = np.load(os.path.join(PTH, 'rfp_mean.npy'))\n",
    "gfp_mean = np.load(os.path.join(PTH, 'gfp_mean.npy'))\n",
    "baseline = np.load(os.path.join(PTH, 'baseline.npy'))\n",
    "\n",
    "# load mask, bin it\n",
    "# find the fixed_mask_*.tif file in PTH directory\n",
    "try:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask_*.tif'))[0]\n",
    "except:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask*.tif'))[0]\n",
    "\n",
    "mask = tifffile.imread(fixed_mask_fn)\n",
    "\n",
    "h, w = mask.shape\n",
    "h_binned = h // bin_factor\n",
    "w_binned = w // bin_factor\n",
    "# binning of mask\n",
    "mask_binned = mask.reshape(h_binned, bin_factor, w_binned, bin_factor).max(axis=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70462ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of time a voxel is zero within the worm mask\n",
    "g5_masked = g5 * mask_binned[np.newaxis, :, :]\n",
    "\n",
    "g5_masked.shape\n",
    "\n",
    "# g5_masked is shape (T, Z, H, W) array with zeros outside the worm region\n",
    "# find all voxels where g5_masked is zero for each time point\n",
    "zero_mask = g5_masked == 0\n",
    "# calculate the probability of a voxel being zero across time, but don't include time points where the voxel is masked out (i.e., outside the worm)\n",
    "zero_prob = np.mean(zero_mask, axis=0)\n",
    "# set zero probability to NaN for voxels outside the worm (mask binned needs a z dimension added)\n",
    "zero_prob[mask_binned[np.newaxis].repeat(zero_prob.shape[0], axis=0) == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g5.shape)\n",
    "print(rfp_mean.shape)\n",
    "print(gfp_mean.shape)\n",
    "print(baseline.shape)\n",
    "print(mask_binned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.close('all')\n",
    "%matplotlib qt\n",
    "# plot zero_prob as an image, with colorbar, for each z slice in subplots\n",
    "fig, axes = plt.subplots(4, 10, figsize=(20, 8), constrained_layout=True)\n",
    "for z in range(zero_prob.shape[0]):\n",
    "    ax = axes[z // 10, z % 10]\n",
    "    im = ax.imshow(zero_prob[z, :, :], cmap='viridis', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Z={z}')\n",
    "    ax.axis('off')\n",
    "    # replace last subplot with colorbar\n",
    "    if z == zero_prob.shape[0] - 1:\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "# delete last subplots if z slices are less than 40\n",
    "for z in range(zero_prob.shape[0], 40):\n",
    "    ax = axes[z // 10, z % 10]\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(zero_prob.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Probability of Voxel Being Zero')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Zero Probability Across Voxels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = 0.3 # if a voxel is zero more than this fraction of the time, consider it outside the worm or a bad voxel\n",
    "# create a mask of good voxels\n",
    "good_voxel_mask = zero_prob < probability_threshold\n",
    "normalized_data = g5_masked * good_voxel_mask[np.newaxis].repeat(g5_masked.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3303f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_mean = np.mean(normalized_data, axis=0)\n",
    "\n",
    "# clip norm_data mean between 0th and 99th percentiles\n",
    "p0 = np.percentile(norm_data_mean, 0)\n",
    "p99 = np.percentile(norm_data_mean, 99)\n",
    "norm_data_mean = np.clip(norm_data_mean, p0, p99)\n",
    "\n",
    "z2plot = [0,5,15,25,35]\n",
    "# z2plot = [0,5]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplots(len(z2plot), 3, figsize=(15, 9), constrained_layout=True)\n",
    "for i in z2plot: # for each z slice\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+1)\n",
    "    plt.pcolor(gfp_mean[i,:,:])\n",
    "    plt.title(f'GFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+2)\n",
    "    plt.pcolor(rfp_mean[i,:,:])\n",
    "    plt.title(f'RFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+3)\n",
    "    plt.pcolor(norm_data_mean[i,:,:])\n",
    "    # add colorbar outside to the right\n",
    "    plt.colorbar()\n",
    "    # plt.clim(0, 12)\n",
    "    # make all axes not be squished\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster normalized_data using k means\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "T, Z, H, W = normalized_data.shape\n",
    "normalized_data_reshaped = normalized_data.reshape(T, Z*H*W).T  # shape (Z*H*W, T)\n",
    "kmeans.fit(normalized_data_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster mean activity over time\n",
    "cluster_means = np.zeros((n_clusters, T))\n",
    "for c in range(n_clusters):\n",
    "    cluster_means[c, :] = normalized_data_reshaped[kmeans.labels_ == c, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ea679",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "t = np.arange(T) * (1/fps)\n",
    "ic = 0\n",
    "for c in range(6,9):#range(n_clusters):\n",
    "    plt.plot(t, cluster_means[c, :], label=f'Cluster {ic+1}')\n",
    "    ic += 1\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('$\\ R / R_{baseline}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ef667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster 1 and 2 spatial maps at each z slice\n",
    "plt.close('all')\n",
    "ic = 0\n",
    "for c in range(6,9):#range(n_clusters):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(20, 8), constrained_layout=True)\n",
    "    for z in range(Z):\n",
    "        ax = axes[z // 10, z % 10]\n",
    "        cluster_map = kmeans.labels_.reshape(Z, H, W)[z, :, :] == c\n",
    "        im = ax.imshow(cluster_map, cmap='gray')\n",
    "        ax.set_title(f'Cluster {ic+1} Z={z}')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'Spatial Map of Cluster {c+1} Across Z Slices')\n",
    "    ic += 1\n",
    "    # deleate last subplots if z slices are less than 40\n",
    "    for z in range(Z, 40):\n",
    "        ax = axes[z // 10, z % 10]\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# flatten normalized_data to (T, Z*H*W)\n",
    "T, Z, H, W = normalized_data.shape\n",
    "data_reshaped = normalized_data.reshape(T, Z*H*W)\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get scores, plot\n",
    "scores = pca.transform(data_reshaped)\n",
    "# plot the first 5 principal component scores\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    plt.plot(scores[:,i])\n",
    "    plt.show()\n",
    "\n",
    "# plot the first 5 principal component weights as images across z slices\n",
    "components = pca.components_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot gfp_mean, rfp_mean, dat in a (5,3) grid of subplots (5 z slices, 3 columns)\n",
    "\n",
    "\n",
    "# divide gfp_mean by rfp_mean, but only if rfp_mean is not zero\n",
    "\n",
    "dat = gfp_mean / rfp_mean\n",
    "# dat = np.divide(gfp_mean, rfp_mean, out=np.zeros_like(gfp_mean), where=rfp_mean!=0)\n",
    "# divide each voxel in dat by its baseline, but only if baseline is not zero\n",
    "dat = np.divide(dat, baseline, out=np.zeros_like(dat), where=baseline!=0)\n",
    "dat = dat * mask_binned[np.newaxis, :, :]\n",
    "\n",
    "# remove outliers by clipping to 1st and 99th percentile\n",
    "p1 = np.percentile(dat, 0)\n",
    "p99 = np.percentile(dat, 99.9)\n",
    "dat = np.clip(dat, p1, p99)\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(baseline.ravel(), bins=1000, range=(0,100))\n",
    "# plt.show()\n",
    "\n",
    "# z2plot = [0,5,15,25,35]\n",
    "z2plot = [0,5]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplots(len(z2plot), 3, figsize=(15, 5), constrained_layout=True)\n",
    "for i in z2plot: # for each z slice\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+1)\n",
    "    plt.pcolor(gfp_mean[i,:,:])\n",
    "    plt.title(f'GFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+2)\n",
    "    plt.pcolor(rfp_mean[i,:,:])\n",
    "    plt.title(f'RFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+3)\n",
    "    plt.pcolor(dat[i,:,:])\n",
    "    # add colorbar outside to the right\n",
    "    plt.colorbar()\n",
    "    plt.clim(0, 12)\n",
    "    # make all axes not be squished\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mask, bin\n",
    "# find the fixed_mask_*.tif file in PTH directory\n",
    "try:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask_*.tif'))[0]\n",
    "except:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask*.tif'))[0]\n",
    "\n",
    "mask = tifffile.imread(fixed_mask_fn)\n",
    "\n",
    "h, w = mask.shape\n",
    "h_binned = h // bin_factor\n",
    "w_binned = w // bin_factor\n",
    "# binning of mask\n",
    "mask_binned = mask.reshape(h_binned, bin_factor, w_binned, bin_factor).max(axis=(1,3))\n",
    "# shrink the mask slightly using erosion skimage\n",
    "mask_binned = erosion(mask_binned, disk(5))\n",
    "\n",
    "# zero out values outside the mask\n",
    "g5_masked = g5 * mask_binned[np.newaxis,np.newaxis,:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4596ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5_masked_trimmed  = g5_masked[:, 5:27, :,40:230]\n",
    "g5_masked_trimmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3602248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# get the intensity over time of the voxels in the middle of the (x,y) plane, definding a box, and all z slices\n",
    "middle_x = int(g5_masked_trimmed.shape[3] // 1.5)\n",
    "middle_y = int(g5_masked_trimmed.shape[2] // 1.95)\n",
    "box_size = 10  # Define the size of the box around the middle point\n",
    "middle_voxels = g5_masked_trimmed[:, 5:27, middle_y-box_size:middle_y+box_size, middle_x-box_size:middle_x+box_size].mean(axis=(2,3))\n",
    "# divide each z slice by the mean activity in the first 60 time points to get F/F0\n",
    "middle_voxels = middle_voxels / (middle_voxels[:60,:].mean(axis=0) + 1e-6)\n",
    "\n",
    "t = np.arange(middle_voxels.shape[0]) / fps\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(middle_voxels)\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('$F/F_{\\\\mathrm{baseline}}$')\n",
    "# plt.ylim(0, 5)\n",
    "plt.show()\n",
    "# plot same thing as heatmap and sort by mean signal across time for each z slice\n",
    "sorted_indices = np.argsort(middle_voxels.mean(axis=0))\n",
    "middle_voxels_sorted = middle_voxels[:, sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolor(t, np.arange(middle_voxels.shape[1]), middle_voxels_sorted.T, cmap='plasma',vmin=0,vmax=5)\n",
    "plt.colorbar(label='$F/F_{\\\\mathrm{baseline}}$')\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Z slice (sorted by mean intensity)')\n",
    "plt.show()\n",
    "\n",
    "# plot g5_masked and the box defined by box_size and middle_x, middle_y to confirm that the box is in the middle of the (x,y) plane and covers the expected area\n",
    "for i in range(g5_masked_trimmed.shape[1]):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.pcolor(g5_masked_trimmed[158,i,:,:],vmin=0, vmax=10)\n",
    "    plt.plot([middle_x-box_size, middle_x+box_size, middle_x+box_size, middle_x-box_size, middle_x-box_size],\n",
    "             [middle_y-box_size, middle_y-box_size, middle_y+box_size, middle_y+box_size, middle_y-box_size], color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962113d",
   "metadata": {},
   "source": [
    "# GRID ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid_analysis\n",
    "import sys\n",
    "import importlib\n",
    "_ = importlib.reload(sys.modules['grid_analysis'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "\n",
    "xspace = 10\n",
    "yspace = 10\n",
    "fps = 1/0.533\n",
    "bin_factor = 1\n",
    "z_start = 5\n",
    "z_end = 25\n",
    "baseline_start_sec = 5 # start baseline calculation here (relative to the start of the recording, not relative to start_time_sec)\n",
    "baseline_end_sec = 35 # end baseline calculation here (relative to the start of the recording, not relative to start_time_sec)\n",
    "start_time_sec = 0\n",
    "time_food_sec = 435\n",
    "sys.argv = [\"\", PTH, xspace, yspace, fps, bin_factor, z_start, z_end, baseline_start_sec, baseline_end_sec, start_time_sec]#time_food_sec] \n",
    "grid_analysis.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de75150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid_analysis\n",
    "_ = importlib.reload(sys.modules['grid_analysis'])\n",
    "# load the grid_timeseries_flat_smoothed.npz file and plot the heatmap using the plot_heatmap function\n",
    "data = np.load(os.path.join(PTH, 'grid_timeseries_flat_smoothed.npz'))\n",
    "flat_timeseries_smoothed = data['timeseries']\n",
    "z_labels = data['z_labels']\n",
    "grid_analysis.plot_heatmap(flat_timeseries_smoothed, z_labels, fps=1/0.533, output_path=os.path.join(PTH, 'grid_heatmap.png'), data_start_time=start_time_sec)\n",
    "# grid_analysis.plot_heatmap(flat_timeseries_smoothed, z_labels, fps=1/0.533, output_path=os.path.join(PTH, 'grid_heatmap.png'), data_start_time=start_time_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb67c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_timeseries_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0032469",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(flat_timeseries_smoothed.shape[0])/fps,flat_timeseries_smoothed[:,20:27])\n",
    "# plt.ylim(0.03,0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(z_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_timeseries_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = importlib.reload(sys.modules['grid_analysis'])\n",
    "# sort the grid squares by their mean signal across time, and plot the heatmap again with the grid squares in sorted order\n",
    "mean_signal_grid = flat_timeseries_smoothed.mean(axis=0)\n",
    "sorted_indices = np.argsort(mean_signal_grid)\n",
    "flat_timeseries_smoothed_sorted = flat_timeseries_smoothed[:, sorted_indices]\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# smooth some more\n",
    "from grid_analysis import causal_smooth\n",
    "toplot = causal_smooth(flat_timeseries_smoothed_sorted, sigma=1.0)\n",
    "grid_analysis.plot_heatmap(toplot, z_labels, fps=1/0.533, output_path=os.path.join(PTH, 'grid_heatmap_sorted.png'), data_start_time=start_time_sec)\n",
    "# plt.xlim(-50,125)\n",
    "# plt.ylim(50,770)\n",
    "\n",
    "# plot the mean signal across all grid squares for each z-slice over time, and mark the time of food addition\n",
    "mean_signal_grid = flat_timeseries_smoothed.mean(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(flat_timeseries_smoothed.shape[0])/fps - start_time_sec, mean_signal_grid)\n",
    "plt.axvline(0, color='red', linestyle='--', label='Food addition')\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Mean $F/F_{\\\\mathrm{baseline}}$ across grid squares')\n",
    "plt.legend()\n",
    "# plt.xlim(-50,125)\n",
    "# plt.ylim(0.03,0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_timeseries_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca on flat_timeseries_smoothed, which of size (time,features)\n",
    "from sklearn.decomposition import PCA\n",
    "import utils\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(flat_timeseries_smoothed)\n",
    "# get scores\n",
    "pc_scores = pca.transform(flat_timeseries_smoothed)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance\n",
    "\n",
    "\n",
    "line_colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "ylim = (-12,14)\n",
    "for i in range(5):\n",
    "    fig, ax = utils.pretty_plot()\n",
    "    ax.plot(np.arange(pc_scores.shape[0])/fps, pc_scores[:,i], label=f'PC{i+1}', lw=3, color=line_colors[i])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel('Time (sec)')\n",
    "    plt.ylabel(f'PC{i+1} projection')\n",
    "    plt.show()\n",
    "\n",
    "# scree plot\n",
    "fig, ax = utils.pretty_plot()\n",
    "ax.bar(np.arange(len(explained_variance))+1, explained_variance*100)\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('Variance explained (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster flat_timeseries_smoothed, then plot heatmap with clusters indicated\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(flat_timeseries_smoothed.T)\n",
    "cluster_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e034a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sorted_timeseries)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort flat_timeseries_smoothed by cluster labels\n",
    "sorted_indices = np.argsort(cluster_labels)\n",
    "sorted_timeseries = flat_timeseries_smoothed[:, sorted_indices]\n",
    "\n",
    "plt, ax = utils.pretty_plot(figsize=(10,6))\n",
    "im = ax.pcolormesh(sorted_timeseries.T, cmap='plasma')\n",
    "plt.show()\n",
    "\n",
    "# plot mean of each cluster over time on the same axis\n",
    "i = 0\n",
    "for c in range(n_clusters-1):\n",
    "    cluster_mean = flat_timeseries_smoothed[:, cluster_labels == c].mean(axis=1)\n",
    "    # normalize between 0 and 1\n",
    "    cluster_mean = (cluster_mean - cluster_mean.min()) / (cluster_mean.max() - cluster_mean.min())\n",
    "    # smooth cluste means\n",
    "    # cluster_mean = pca_analysis.causal_smooth(cluster_mean[:,np.newaxis], sigma=3.0)\n",
    "    if i==0:\n",
    "        fig, ax = utils.pretty_plot()\n",
    "    ax.plot(np.arange(cluster_mean.shape[0])/fps, cluster_mean, lw=1)\n",
    "    ax.set_xlabel('Time (sec)')\n",
    "    # plt.ylabel(f'Cluster {c+1} mean $F/F_{{\\\\mathrm{{baseline}}}}$')\n",
    "    i += 1\n",
    "plt.show()\n",
    "\n",
    "# cross correlation between cluster 1 and 2 means signals\n",
    "from scipy.signal import correlate\n",
    "cluster1_mean = flat_timeseries_smoothed[:, cluster_labels == 0].mean(axis=1)\n",
    "cluster2_mean = flat_timeseries_smoothed[:, cluster_labels == 1].mean(axis=1)\n",
    "# normalize means bwetween 0 and 1\n",
    "cluster1_mean = (cluster1_mean - cluster1_mean.min()) / (cluster1_mean.max() - cluster1_mean.min())\n",
    "cluster2_mean = (cluster2_mean - cluster2_mean.min()) / (cluster2_mean.max() - cluster2_mean.min())\n",
    "# compute cross-correlation\n",
    "corr = correlate(cluster1_mean - cluster1_mean.mean(), cluster2_mean - cluster2_mean.mean(), mode='full')\n",
    "lags = np.arange(-len(cluster1_mean)+1, len(cluster1_mean))\n",
    "# plot xcorr\n",
    "fig, ax = utils.pretty_plot()\n",
    "ax.plot(lags/fps, corr)\n",
    "ax.set_xlabel('Lag (sec)')\n",
    "ax.set_ylabel('Cross-correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab335d",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199062da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pca_analysis\n",
    "import sys\n",
    "_ = importlib.reload(sys.modules['pca_analysis'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "\n",
    "# Arguments: input_dir, fps, bin_factor, z_start, z_end, lowpass_freq, n_components,\n",
    "#            n_pcs_plot, baseline_start, baseline_end, start_time, time_offset\n",
    "sys.argv = [\"\", PTH, 1/0.533, 1, 7, 18, 0, 50, 6, 5, 35, 0]\n",
    "pca_analysis.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c504364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6e2641",
   "metadata": {},
   "source": [
    "# DMD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dmd_analysis\n",
    "import sys\n",
    "_ = importlib.reload(sys.modules['dmd_analysis'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "\n",
    "# Arguments: input_dir, fps, bin_factor, z_start, z_end, lowpass_freq, dmd_rank, \n",
    "#            n_modes_plot, baseline_start, baseline_end, start_time, time_offset\n",
    "sys.argv = [\"\", PTH, 1/0.533, 1, 7, 18, 0, 5, 6, 5, 35, 0]\n",
    "\n",
    "dmd_analysis.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affaf70",
   "metadata": {},
   "source": [
    "# DIFFUSION MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c21d2",
   "metadata": {},
   "source": [
    "- 20251223 worm005\n",
    "- choose a z slice (z=26)\n",
    "- choose frames (800-1000)\n",
    "- make movie\n",
    "- show gfp/rfp frame for 822"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d57647",
   "metadata": {},
   "source": [
    "- load 2x2 binned, R/R0 \n",
    "- load mask and bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579347b",
   "metadata": {},
   "source": [
    "## Conceptual Explanation of the Reaction-Diffusion Model\n",
    "\n",
    "### The Physical Picture\n",
    "\n",
    "Imagine you're watching serotonin (or another signaling molecule) being released, spreading, and getting taken back up in a worm's nervous system. The fluorescence intensity you measure at each pixel reflects this dynamic process. The code models three key phenomena:\n",
    "\n",
    "1. **Diffusion**: Molecules spread from high to low concentration regions, like ink dropped in water\n",
    "2. **Decay/Reuptake**: Molecules are removed from the extracellular space (via transporters, degradation, etc.) at a rate proportional to their local concentration\n",
    "3. **Release Sources**: Certain locations periodically release molecules into the system\n",
    "\n",
    "### The Mathematical Model\n",
    "\n",
    "The fluorescence field $i(x,y,t)$ evolves according to a **reaction-diffusion PDE**:\n",
    "\n",
    "$$\\frac{\\partial i}{\\partial t} = D \\nabla^2 i - k \\cdot i + s(x,y,t)$$\n",
    "\n",
    "Where:\n",
    "- $D$ is the **diffusion coefficient** (how fast molecules spread spatially)\n",
    "- $k$ is the **decay rate** (how fast molecules are removed)\n",
    "- $s(x,y,t)$ is the **source field** (where and when molecules are released)\n",
    "\n",
    "### Discretization on an Irregular Domain\n",
    "\n",
    "Since you have a tissue mask (not a regular rectangle), the code builds a **graph Laplacian**. Each masked pixel becomes a node, connected to its 4 neighbors. The Laplacian operator $L$ is a sparse matrix where:\n",
    "- Off-diagonal entries are +1 for connected pixels\n",
    "- Diagonal entries are −(number of neighbors)\n",
    "\n",
    "This naturally implements **no-flux boundary conditions**: at the mask edge, there are simply fewer neighbors, so diffusion stops there automatically.\n",
    "\n",
    "### Time Stepping (Semi-Implicit)\n",
    "\n",
    "To advance the simulation forward in time, the code uses a **backward Euler** (or optionally Crank-Nicolson) scheme. Instead of naively computing \"next = current + dt × derivative\" (which can blow up), it solves:\n",
    "\n",
    "$$(I - dt \\cdot A) \\cdot i_{t+1} = i_t + dt \\cdot s_t$$\n",
    "\n",
    "where $A = D \\cdot L - k \\cdot I$. This requires solving a linear system at each time step, but the matrix is **sparse** and gets **LU-factorized once**, making subsequent solves very fast.\n",
    "\n",
    "### Learning the Source Field from Data\n",
    "\n",
    "The source term $s(x,y,t)$ is unknown. The code learns it from the movie itself using **Non-negative Matrix Factorization (NMF)**:\n",
    "\n",
    "1. **Preprocessing**: High-pass filter the movie temporally (remove slow baseline drift), then rectify (keep only positive values, since we're interested in \"release-like\" transient increases)\n",
    "\n",
    "2. **NMF Decomposition**: Factor the preprocessed activity as:\n",
    "   $$A \\approx W \\cdot H$$\n",
    "   where $W$ is (time × M) and $H$ is (M × pixels). This finds M spatial patterns that, when weighted by time-varying amplitudes, reconstruct the activity.\n",
    "\n",
    "3. **Interpretation**: \n",
    "   - $\\Phi = H^T$ (N×M) are the **spatial source maps** — where release happens\n",
    "   - $a_t = W$ (T×M) are the **time courses** — when each source is active\n",
    "\n",
    "The source field is then: $s(t) = g \\cdot \\Phi \\cdot a_t$, where $g$ is a global gain parameter.\n",
    "\n",
    "### The Fitting Pipeline\n",
    "\n",
    "The full fitting process is **staged**:\n",
    "\n",
    "1. **Learn sources** (Φ, a_t) via NMF on the preprocessed movie — this identifies *where* and *when* release happens\n",
    "\n",
    "2. **Fit parameters** (D, k, g) with sources fixed — the optimizer simulates the model with candidate parameters and minimizes the mismatch to observed data\n",
    "\n",
    "3. **Optionally refine** the time courses with temporal smoothness regularization\n",
    "\n",
    "### Parameter Fitting Details\n",
    "\n",
    "The optimizer (L-BFGS-B) searches for D, k, g that minimize mean squared error between simulated and observed fluorescence. To enforce positivity, parameters are represented in log-space: optimizing $\\log(D)$ ensures $D > 0$.\n",
    "\n",
    "For efficiency with large images:\n",
    "- The Laplacian is **sparse** (only ~4N nonzero entries for N pixels)\n",
    "- The implicit matrix is **factorized once** per (D,k) pair\n",
    "- Loss is computed on a **random subset** of pixels\n",
    "\n",
    "### What the Results Tell You\n",
    "\n",
    "- **D (pixels²/s)**: How fast the signal spreads spatially. Larger D means faster diffusion. Remember to scale by pixel size² if comparing across binning levels.\n",
    "  - If your original pixel size is s (µm/pixel), and you bin by factor b, then fitted Dbinned​ relates to physical Dμm roughly by:\n",
    "    - Dμm2/s≈Dbinned⋅(b s)^2\n",
    "\n",
    "- **k (1/s)**: How fast the signal decays. The \"half-life\" is $\\ln(2)/k$ seconds.\n",
    "\n",
    "- **Φ (spatial maps)**: Where the sources are located. Each column is a probability-like distribution over pixels.\n",
    "\n",
    "- **a_t (time courses)**: When each source is active. Peaks indicate release events.\n",
    "\n",
    "- **R²**: Overall fit quality — how much variance the model explains.\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "1. **Phenomenological, not biophysical**: The model fits the *fluorescence* dynamics directly, not the underlying concentration. This avoids needing to know the sensor's binding kinetics.\n",
    "\n",
    "2. **Data-driven sources**: Rather than assuming source locations, NMF discovers them from the data's spatio-temporal structure.\n",
    "\n",
    "3. **Implicit time stepping**: Ensures numerical stability even with large D or dt.\n",
    "\n",
    "4. **Modular pipeline**: Each stage can be tuned independently (e.g., adjust NMF preprocessing, change number of sources, use Crank-Nicolson for better accuracy).\n",
    "\n",
    "### REGULARIZATION\n",
    "\n",
    "1. **L1 sparsity on source time courses** - encourages bursts rather than continuous activity\n",
    "2. **Total Variation (TV) on sources** - penalizes frequent changes, promoting sparse \"events\"  \n",
    "3. **D prior/regularization** - soft prior pushing D away from zero\n",
    "4. **Source energy penalty** - limits total source \"power\" to force more diffusion \n",
    "\n",
    "\n",
    "## Regularization Added to diffusion_v2.py\n",
    "\n",
    "I've added optional regularization to prevent D from collapsing to near-zero. The key additions:\n",
    "\n",
    "### `RegularizationConfig` dataclass\n",
    "Located around line 900, this controls all regularization terms:\n",
    "\n",
    "```python\n",
    "reg_config = RegularizationConfig(\n",
    "    # Source sparsity (temporal)\n",
    "    source_l1_weight=0.1,    # L1 penalty: ∑|a_t| — promotes sparse firing\n",
    "    source_tv_weight=0.05,   # TV penalty: ∑|a_t - a_{t-1}| — promotes discrete events\n",
    "    source_energy_weight=0.0, # L2 penalty: ∑a_t² — limits total source power\n",
    "    \n",
    "    # D priors (prevent collapse)\n",
    "    D_min_penalty_weight=0.001,  # 1/D penalty — blows up as D→0\n",
    "    D_prior_weight=0.5,          # Gaussian prior on log(D)\n",
    "    D_prior_mean=0.05,           # Prior mean for D\n",
    "    \n",
    "    # Source gain limit\n",
    "    g_max_penalty_weight=0.0,    # Penalty if g exceeds threshold\n",
    "    g_max_threshold=5.0          # Threshold for g penalty\n",
    ")\n",
    "```\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The regularized loss is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reg}} = \\mathcal{L}_{\\text{data}} + \\lambda_1 \\|a_t\\|_1 + \\lambda_{TV} \\text{TV}(a_t) + \\frac{\\alpha}{D} + \\beta (\\log D - \\log D_0)^2\n",
    "$$\n",
    "\n",
    "| Term | Effect |\n",
    "|------|--------|\n",
    "| `source_l1_weight` | Forces sources to be sparse in time (few frames active) |\n",
    "| `source_tv_weight` | Forces sources to have discrete on/off events |\n",
    "| `D_min_penalty_weight` | $\\frac{1}{D}$ penalty prevents D→0 |\n",
    "| `D_prior_weight` | Soft prior pulling D toward `D_prior_mean` |\n",
    "\n",
    "### Usage\n",
    "\n",
    "```python\n",
    "from diffusion_v2 import RegularizationConfig, fit_diffusion_model_2d\n",
    "\n",
    "reg_config = RegularizationConfig(\n",
    "    source_l1_weight=0.1,\n",
    "    D_prior_weight=0.5,\n",
    "    D_prior_mean=0.05  # Your expected D value\n",
    ")\n",
    "\n",
    "result = fit_diffusion_model_2d(\n",
    "    Y, mask, dt,\n",
    "    reg_config=reg_config,  # Pass the config\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "### Tuning Tips\n",
    "\n",
    "1. **If D still collapses**: Increase `D_min_penalty_weight` or `D_prior_weight`\n",
    "2. **If sources are too active**: Increase `source_l1_weight` \n",
    "3. **If sources are noisy/oscillatory**: Increase `source_tv_weight`\n",
    "4. **Set `D_prior_mean`** to your expected diffusion coefficient from physics/calibration\n",
    "\n",
    "Made changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30aea12",
   "metadata": {},
   "source": [
    "## SAVE TRIMMED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import quantify_voxels\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.morphology import erosion, disk\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "_ = importlib.reload(sys.modules['quantify_voxels'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251223\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm005'\n",
    "bin_factor = 2\n",
    "fps = 1/0.533\n",
    "frames = (800,1000)  # only analyze frames from 800 to 1000\n",
    "z = 26\n",
    "zero_prob_threshold = 0.3 # remove voxels that are zero more than this fraction of the time\n",
    "\n",
    "\n",
    "# load normalized_voxels.npy (R/R0), already been binned by bin_factor\n",
    "g5 = np.load(os.path.join(PTH, 'normalized_voxels.npy'))\n",
    "# load mask, bin it\n",
    "fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask_*.tif'))[0]\n",
    "mask = tifffile.imread(fixed_mask_fn)\n",
    "h, w = mask.shape\n",
    "h_binned = h // bin_factor\n",
    "w_binned = w // bin_factor\n",
    "mask_binned = mask.reshape(h_binned, bin_factor, w_binned, bin_factor).max(axis=(1,3))\n",
    "# mask g5 data\n",
    "g5_trimmed = g5[frames[0]:frames[1], z, :, :]\n",
    "\n",
    "# remove voxels that are only present a small percentage of the time\n",
    "# find percentage of time a voxel is zero within the worm mask\n",
    "g5_masked = g5 * mask_binned[np.newaxis, :, :]\n",
    "\n",
    "# g5_masked is shape (T, Z, H, W) array with zeros outside the worm region\n",
    "# find all voxels where g5_masked is zero for each time point\n",
    "zero_mask = g5_masked == 0\n",
    "# calculate the probability of a voxel being zero across time, but don't include time points where the voxel is masked out (i.e., outside the worm)\n",
    "zero_prob = np.mean(zero_mask, axis=0)\n",
    "# set zero probability to NaN for voxels outside the worm (mask binned needs a z dimension added)\n",
    "zero_prob[mask_binned[np.newaxis].repeat(zero_prob.shape[0], axis=0) == 0] = np.nan\n",
    "good_voxel_mask = zero_prob < zero_prob_threshold\n",
    "g5 = g5_masked * good_voxel_mask[np.newaxis].repeat(g5_masked.shape[0], axis=0)\n",
    "\n",
    "# save g5_trimmed and mask_binned to a npy file\n",
    "np.savez(os.path.join(PTH, f'g5_trimmed_frames{frames[0]}to{frames[1]}.npz'), g5=g5, mask_binned=mask_binned, zero_prob=zero_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08728d31",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2820f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 100, 250)\n",
      "(100, 250)\n",
      "(39, 100, 250)\n",
      "Number of voxels in updated mask: 8409\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import quantify_voxels\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.morphology import erosion, disk\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "_ = importlib.reload(sys.modules['quantify_voxels'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251223\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm005'\n",
    "bin_factor = 2\n",
    "fps = 1/0.533\n",
    "# frames = (800,1000)  # only analyze frames from 800 to 1000 (these time points correspond to start of food encounter and two serotonin cycles)\n",
    "frames = (800,1199)\n",
    "z = 26\n",
    "rfp_thresh = 50 # only include voxels where rfp_mean is above this threshold\n",
    "keep_width = (25,228) # only keep this x range before\n",
    "\n",
    "# load g5_trimmed\n",
    "data = np.load(os.path.join(PTH, f'g5_trimmed_frames.npz'))\n",
    "g5 = data['g5'][frames[0]:frames[1], z, :, :]  # g5 is R/R0 for frames 800 to 1000\n",
    "mask = data['mask_binned']\n",
    "print(g5.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "# load rfp_mean.npy\n",
    "rfp_mean = np.load(os.path.join(PTH, 'rfp_mean.npy'))\n",
    "print(rfp_mean.shape)\n",
    "rfp_mean_z = rfp_mean[z,:,:] # HW\n",
    "\n",
    "# update mask based on rfp_thresh\n",
    "mask_updated = mask.copy()\n",
    "mask_updated[rfp_mean_z < rfp_thresh] = 0\n",
    "\n",
    "# g5 is R/R0 for a single z slice, for frames 800 to 1000\n",
    "# mask_updated is the segmentation mask updated based on rfp_thresh\n",
    "g5 = g5 * mask_updated[np.newaxis, :, :]\n",
    "\n",
    "# trim width in g5 and mask_updated\n",
    "g5 = g5[:, :, keep_width[0]:keep_width[1]]\n",
    "mask_updated = mask_updated[:, keep_width[0]:keep_width[1]]\n",
    "print('Number of voxels in updated mask:', np.sum(mask_updated > 0))\n",
    "\n",
    "raise NotImplementedError('Need to implement temporal smoothing before moving on.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca0a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.hist(rfp_mean[z,:,:].ravel(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aeaa85",
   "metadata": {},
   "source": [
    "## TEST POINT SOURCE IDENTIFICATION\n",
    "\n",
    "Identify candidate source voxels for point source diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d70ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_point_sources(Y, mask, fps=1/0.533, n_sources=10, \n",
    "                          methods=['variance', 'early_response', 'peak_detection', 'gradient'],\n",
    "                          early_window=(10, 60), baseline_window=(0, 60),\n",
    "                          spatial_exclusion_radius=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Identify candidate point source locations from fluorescence movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : (T, H, W) array\n",
    "        Fluorescence movie (already F/F0 or R/R0).\n",
    "    mask : (H, W) bool array\n",
    "        Tissue mask.\n",
    "    fps : float\n",
    "        Frames per second.\n",
    "    n_sources : int\n",
    "        Number of candidate sources to identify per method.\n",
    "    methods : list of str\n",
    "        Which detection methods to use. Options:\n",
    "        - 'variance': High temporal variance voxels\n",
    "        - 'early_response': Voxels that respond early\n",
    "        - 'peak_detection': Voxels with strong transient peaks\n",
    "        - 'gradient': Spatial gradient maxima (likely centers of activity)\n",
    "    early_window : tuple (start_frame, end_frame)\n",
    "        Time window to detect early responders.\n",
    "    baseline_window : tuple (start_frame, end_frame)\n",
    "        Baseline period for computing response magnitude.\n",
    "    spatial_exclusion_radius : int\n",
    "        Minimum distance (pixels) between detected sources.\n",
    "    verbose : bool\n",
    "        Print diagnostic info.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sources_dict : dict\n",
    "        Dictionary with keys as method names, values as (M, 2) arrays of (row, col) coords.\n",
    "    metrics_dict : dict\n",
    "        Dictionary with metric values for each detected source.\n",
    "    \"\"\"\n",
    "    T, H, W = Y.shape\n",
    "    mask = mask.astype(bool)\n",
    "    \n",
    "    # Mask the data\n",
    "    Y_masked = Y * mask[np.newaxis, :, :]\n",
    "    \n",
    "    sources_dict = {}\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # =========================================================================\n",
    "    # METHOD 1: TEMPORAL VARIANCE\n",
    "    # =========================================================================\n",
    "    if 'variance' in methods:\n",
    "        # Compute temporal variance for each voxel (only where mask is True)\n",
    "        var_map = np.var(Y_masked, axis=0)\n",
    "        var_map[~mask] = 0  # Zero out outside mask\n",
    "        \n",
    "        # Find top n_sources local maxima\n",
    "        sources_var = _find_local_maxima_2d(var_map, n_sources, \n",
    "                                            exclusion_radius=spatial_exclusion_radius,\n",
    "                                            mask=mask)\n",
    "        sources_dict['variance'] = sources_var\n",
    "        metrics_dict['variance'] = var_map[sources_var[:, 0], sources_var[:, 1]]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Variance method: found {len(sources_var)} sources\")\n",
    "            print(f\"  Variance range: {metrics_dict['variance'].min():.3f} - {metrics_dict['variance'].max():.3f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # METHOD 2: EARLY RESPONDERS\n",
    "    # =========================================================================\n",
    "    if 'early_response' in methods:\n",
    "        # Compute baseline\n",
    "        baseline = np.mean(Y_masked[baseline_window[0]:baseline_window[1]], axis=0)\n",
    "        baseline[baseline == 0] = 1  # Avoid division by zero\n",
    "        \n",
    "        # Compute response magnitude in early window\n",
    "        early_mean = np.mean(Y_masked[early_window[0]:early_window[1]], axis=0)\n",
    "        response_mag = (early_mean - baseline) / baseline\n",
    "        response_mag[~mask] = -np.inf  # Exclude outside mask\n",
    "        \n",
    "        # Find voxels with strongest early response\n",
    "        sources_early = _find_local_maxima_2d(response_mag, n_sources,\n",
    "                                              exclusion_radius=spatial_exclusion_radius,\n",
    "                                              mask=mask)\n",
    "        sources_dict['early_response'] = sources_early\n",
    "        metrics_dict['early_response'] = response_mag[sources_early[:, 0], sources_early[:, 1]]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Early response method: found {len(sources_early)} sources\")\n",
    "            print(f\"  Response magnitude range: {metrics_dict['early_response'].min():.3f} - {metrics_dict['early_response'].max():.3f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # METHOD 3: PEAK DETECTION (transient activity)\n",
    "    # =========================================================================\n",
    "    if 'peak_detection' in methods:\n",
    "        from scipy.signal import find_peaks\n",
    "        \n",
    "        # For each voxel, count number of significant peaks\n",
    "        peak_count_map = np.zeros((H, W))\n",
    "        peak_height_map = np.zeros((H, W))\n",
    "        \n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                if not mask[i, j]:\n",
    "                    continue\n",
    "                trace = Y_masked[:, i, j]\n",
    "                # Find peaks with minimum prominence\n",
    "                peaks, properties = find_peaks(trace, prominence=0.3, distance=10)\n",
    "                peak_count_map[i, j] = len(peaks)\n",
    "                if len(peaks) > 0:\n",
    "                    peak_height_map[i, j] = np.max(properties['prominences'])\n",
    "        \n",
    "        # Use peak height as metric\n",
    "        peak_height_map[~mask] = 0\n",
    "        sources_peak = _find_local_maxima_2d(peak_height_map, n_sources,\n",
    "                                             exclusion_radius=spatial_exclusion_radius,\n",
    "                                             mask=mask)\n",
    "        sources_dict['peak_detection'] = sources_peak\n",
    "        metrics_dict['peak_detection'] = peak_height_map[sources_peak[:, 0], sources_peak[:, 1]]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Peak detection method: found {len(sources_peak)} sources\")\n",
    "            print(f\"  Peak height range: {metrics_dict['peak_detection'].min():.3f} - {metrics_dict['peak_detection'].max():.3f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # METHOD 4: SPATIAL GRADIENT (centers of activity)\n",
    "    # =========================================================================\n",
    "    if 'gradient' in methods:\n",
    "        # Compute mean intensity over time\n",
    "        mean_intensity = np.mean(Y_masked, axis=0)\n",
    "        \n",
    "        # Compute Laplacian (regions where intensity is locally maximal)\n",
    "        from scipy.ndimage import laplace\n",
    "        laplacian = -laplace(mean_intensity)  # Negative because we want local maxima\n",
    "        laplacian[~mask] = -np.inf\n",
    "        \n",
    "        sources_grad = _find_local_maxima_2d(laplacian, n_sources,\n",
    "                                             exclusion_radius=spatial_exclusion_radius,\n",
    "                                             mask=mask)\n",
    "        sources_dict['gradient'] = sources_grad\n",
    "        metrics_dict['gradient'] = laplacian[sources_grad[:, 0], sources_grad[:, 1]]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Gradient method: found {len(sources_grad)} sources\")\n",
    "            print(f\"  Laplacian range: {metrics_dict['gradient'].min():.3f} - {metrics_dict['gradient'].max():.3f}\")\n",
    "    \n",
    "    return sources_dict, metrics_dict\n",
    "\n",
    "\n",
    "def _find_local_maxima_2d(metric_map, n_sources, exclusion_radius=5, mask=None):\n",
    "    \"\"\"\n",
    "    Find n_sources local maxima in metric_map with spatial exclusion.\n",
    "    \n",
    "    Returns (n, 2) array of (row, col) coordinates.\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import maximum_filter\n",
    "    \n",
    "    if mask is not None:\n",
    "        metric_map = metric_map.copy()\n",
    "        metric_map[~mask] = -np.inf\n",
    "    \n",
    "    # Find local maxima using maximum filter\n",
    "    local_max = maximum_filter(metric_map, size=exclusion_radius) == metric_map\n",
    "    local_max[metric_map == -np.inf] = False\n",
    "    local_max[np.isnan(metric_map)] = False\n",
    "    \n",
    "    # Get coordinates and values\n",
    "    coords = np.argwhere(local_max)\n",
    "    values = metric_map[local_max]\n",
    "    \n",
    "    # Sort by value (descending)\n",
    "    sorted_idx = np.argsort(values)[::-1]\n",
    "    coords_sorted = coords[sorted_idx]\n",
    "    \n",
    "    # Take top n_sources\n",
    "    return coords_sorted[:n_sources]\n",
    "\n",
    "\n",
    "def visualize_point_sources(Y, mask, sources_dict, metrics_dict, \n",
    "                            time_point=0, figsize=(18, 12)):\n",
    "    \"\"\"\n",
    "    Visualize identified point sources overlaid on the image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : (T, H, W) array\n",
    "    mask : (H, W) bool\n",
    "    sources_dict : dict\n",
    "        Output from identify_point_sources.\n",
    "    metrics_dict : dict\n",
    "        Metrics from identify_point_sources.\n",
    "    time_point : int\n",
    "        Which time frame to show.\n",
    "    figsize : tuple\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.patches import Circle\n",
    "    \n",
    "    n_methods = len(sources_dict)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_methods, figsize=figsize, constrained_layout=True)\n",
    "    if n_methods == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    # Row 1: Sources overlaid on single time frame\n",
    "    # Row 2: Sources overlaid on temporal mean\n",
    "    \n",
    "    mean_img = np.mean(Y, axis=0)\n",
    "    frame_img = Y[time_point]\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for col_idx, (method, sources) in enumerate(sources_dict.items()):\n",
    "        # Top row: single frame\n",
    "        ax1 = axes[0, col_idx]\n",
    "        ax1.imshow(frame_img, cmap='gray', vmin=0, vmax=np.percentile(frame_img[mask], 99))\n",
    "        \n",
    "        for i, (row, col) in enumerate(sources):\n",
    "            circle = Circle((col, row), radius=3, color=colors[i % 10], \n",
    "                          fill=False, linewidth=2, label=f'S{i+1}')\n",
    "            ax1.add_patch(circle)\n",
    "            ax1.plot(col, row, 'x', color=colors[i % 10], markersize=8, markeredgewidth=2)\n",
    "        \n",
    "        ax1.set_title(f'{method.replace(\"_\", \" \").title()}\\n(Frame {time_point})')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Bottom row: temporal mean\n",
    "        ax2 = axes[1, col_idx]\n",
    "        ax2.imshow(mean_img, cmap='gray', vmin=0, vmax=np.percentile(mean_img[mask], 99))\n",
    "        \n",
    "        for i, (row, col) in enumerate(sources):\n",
    "            circle = Circle((col, row), radius=3, color=colors[i % 10], \n",
    "                          fill=False, linewidth=2)\n",
    "            ax2.add_patch(circle)\n",
    "            ax2.plot(col, row, 'x', color=colors[i % 10], markersize=8, markeredgewidth=2)\n",
    "        \n",
    "        ax2.set_title('Temporal Mean')\n",
    "        ax2.axis('off')\n",
    "    \n",
    "    plt.suptitle('Identified Point Source Candidates', fontsize=16, y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    # Also plot the time traces for each source\n",
    "    fig2, axes2 = plt.subplots(n_methods, 1, figsize=(12, 3*n_methods), constrained_layout=True)\n",
    "    if n_methods == 1:\n",
    "        axes2 = [axes2]\n",
    "    \n",
    "    t = np.arange(Y.shape[0]) / (1/0.533)  # Assume default fps\n",
    "    \n",
    "    for ax_idx, (method, sources) in enumerate(sources_dict.items()):\n",
    "        ax = axes2[ax_idx]\n",
    "        \n",
    "        for i, (row, col) in enumerate(sources):\n",
    "            trace = Y[:, row, col]\n",
    "            ax.plot(t, trace, label=f'S{i+1} ({row},{col})', alpha=0.7, linewidth=1.5)\n",
    "        \n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('R/R₀')\n",
    "        ax.set_title(f'{method.replace(\"_\", \" \").title()} - Source Time Traces')\n",
    "        ax.legend(ncol=3, fontsize=8, frameon=False)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print metric values\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SOURCE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    for method, sources in sources_dict.items():\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        for i, (row, col) in enumerate(sources):\n",
    "            metric_val = metrics_dict[method][i]\n",
    "            print(f\"  Source {i+1}: ({row:3d}, {col:3d}) - metric = {metric_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f66144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance method: found 4 sources\n",
      "  Variance range: 8.658 - 18.887\n",
      "Early response method: found 4 sources\n",
      "  Response magnitude range: 0.827 - 1.193\n",
      "Peak detection method: found 4 sources\n",
      "  Peak height range: 13.800 - 15.702\n",
      "Gradient method: found 4 sources\n",
      "  Laplacian range: 5.691 - 7.860\n",
      "\n",
      "============================================================\n",
      "SOURCE METRICS\n",
      "============================================================\n",
      "\n",
      "VARIANCE:\n",
      "  Source 1: ( 74, 170) - metric = 18.8867\n",
      "  Source 2: ( 68, 157) - metric = 16.3984\n",
      "  Source 3: ( 40, 142) - metric = 8.7281\n",
      "  Source 4: ( 44, 113) - metric = 8.6583\n",
      "\n",
      "EARLY_RESPONSE:\n",
      "  Source 1: ( 80, 193) - metric = 1.1931\n",
      "  Source 2: ( 43,  18) - metric = 1.1505\n",
      "  Source 3: ( 47, 168) - metric = 1.0452\n",
      "  Source 4: ( 44, 114) - metric = 0.8265\n",
      "\n",
      "PEAK_DETECTION:\n",
      "  Source 1: ( 74, 169) - metric = 15.7023\n",
      "  Source 2: ( 68, 157) - metric = 15.4045\n",
      "  Source 3: ( 44, 113) - metric = 14.4816\n",
      "  Source 4: ( 40, 142) - metric = 13.8001\n",
      "\n",
      "GRADIENT:\n",
      "  Source 1: ( 74, 169) - metric = 7.8595\n",
      "  Source 2: ( 43, 113) - metric = 6.9809\n",
      "  Source 3: ( 47, 169) - metric = 6.6254\n",
      "  Source 4: ( 30, 118) - metric = 5.6909\n"
     ]
    }
   ],
   "source": [
    "# Run point source identification on the loaded data\n",
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "# Identify candidate sources using multiple methods\n",
    "sources_dict, metrics_dict = identify_point_sources(\n",
    "    g5, \n",
    "    mask_updated, \n",
    "    fps=fps,\n",
    "    n_sources=4,  # Find top 4 candidates per method\n",
    "    methods=['variance', 'early_response', 'peak_detection', 'gradient'],\n",
    "    early_window=(10, 60),  # frames 10-60 for early response\n",
    "    baseline_window=(0, 30),  # frames 0-30 for baseline\n",
    "    spatial_exclusion_radius=20,  # min 8 pixels between sources\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualize the identified sources\n",
    "visualize_point_sources(\n",
    "    g5, \n",
    "    mask_updated, \n",
    "    sources_dict, \n",
    "    metrics_dict,\n",
    "    time_point=100,  # show sources on frame 100\n",
    "    figsize=(20, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf94545",
   "metadata": {},
   "source": [
    "## POINT SOURCE DIFFUSION MODEL (diffusion_v3)\n",
    "\n",
    "Uses fixed point sources instead of NMF-learned spatial maps.\n",
    "This addresses the identifiability problem where D → 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d092f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running diffusion_v3 smoke test...\n",
      "Variance method: found 5 sources\n",
      "  Variance range: 0.005 - 0.051\n",
      "Early response method: found 5 sources\n",
      "  Response magnitude range: 0.044 - 0.055\n",
      "Peak detection method: found 5 sources\n",
      "  Peak height range: 0.305 - 0.972\n",
      "Gradient method: found 5 sources\n",
      "  Laplacian range: 0.585 - 1.039\n",
      "Detected 3 sources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Fitted: D=50.0000, k=0.0100, g=50.0000, MSE=0.003841, R²=-0.2059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted: D=50.0000, k=0.0100, g=50.0000\n",
      "MSE=0.003841, R²=-0.2059\n",
      "Smoke test PASSED ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import diffusion_v3 as diff3\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "# Run smoke test to verify the module works\n",
    "diff3.run_smoke_test(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01973226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance method: found 3 sources\n",
      "  Variance range: 8.658 - 18.887\n",
      "Early response method: found 3 sources\n",
      "  Response magnitude range: 1.045 - 1.193\n",
      "Peak detection method: found 3 sources\n",
      "  Peak height range: 13.800 - 15.702\n",
      "Gradient method: found 3 sources\n",
      "  Laplacian range: 6.625 - 7.860\n"
     ]
    }
   ],
   "source": [
    "# Detect point sources using diffusion_v3\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "# Configure source detection\n",
    "source_config = diff3.SourceDetectionConfig(\n",
    "    n_sources=3,\n",
    "    methods=('variance', 'early_response', 'peak_detection', 'gradient'),\n",
    "    early_window=(10, 60),\n",
    "    baseline_window=(0, 30),\n",
    "    spatial_exclusion_radius=25,\n",
    "    peak_prominence=0.3,\n",
    "    peak_distance=10\n",
    ")\n",
    "\n",
    "# Detect sources\n",
    "sources_dict_v3, metrics_dict_v3 = diff3.identify_point_sources(\n",
    "    g5, mask_updated, config=source_config, verbose=True\n",
    ")\n",
    "\n",
    "# Visualize all methods\n",
    "diff3.visualize_point_sources(\n",
    "    g5, mask_updated, sources_dict_v3, metrics_dict_v3,\n",
    "    time_point=100, fps=fps, figsize=(20, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2a3ee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sources: 3 locations\n",
      "Source coordinates (row, col):\n",
      "  Source 1: (74, 170)\n",
      "  Source 2: (40, 142)\n",
      "  Source 3: (44, 113)\n"
     ]
    }
   ],
   "source": [
    "# Combine sources from different methods and visualize final selection\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "# Combine sources - use 'variance' method as primary (usually most reliable)\n",
    "# Other options: 'union', 'intersection', 'weighted'\n",
    "combined_sources = diff3.combine_source_candidates(\n",
    "    sources_dict_v3, metrics_dict_v3,\n",
    "    method='variance',  # Use variance-based sources\n",
    "    max_sources=3,  # Limit to 3 sources\n",
    "    exclusion_radius=25\n",
    ")\n",
    "\n",
    "print(f\"Combined sources: {len(combined_sources)} locations\")\n",
    "print(\"Source coordinates (row, col):\")\n",
    "for i, (r, c) in enumerate(combined_sources):\n",
    "    print(f\"  Source {i+1}: ({r}, {c})\")\n",
    "\n",
    "# Visualize combined sources\n",
    "diff3.visualize_combined_sources(\n",
    "    g5, mask_updated, combined_sources,\n",
    "    fps=fps, time_point=100, figsize=(14, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8d3e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "=== Outer iteration 1 ===\n",
      "INFO: Optimizing D, k, g...\n",
      "INFO: Fitted: D=50.0000, k=0.0100, g=50.0000, MSE=1.128625, R²=-0.1912\n",
      "INFO: Optimizing source time courses...\n",
      "INFO:   D=50.0000, k=0.0100, g=50.0000, MSE=1.407616\n",
      "c:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:1028: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if abs(prev_mse - mse) / (prev_mse + 1e-10) < 1e-4:\n",
      "INFO: \n",
      "=== Outer iteration 2 ===\n",
      "INFO: Optimizing D, k, g...\n",
      "INFO: Fitted: D=2.0136, k=0.0100, g=50.0000, MSE=1.340011, R²=-0.4143\n",
      "INFO: Optimizing source time courses...\n",
      "INFO:   D=2.0136, k=0.0100, g=50.0000, MSE=1.338758\n",
      "INFO: \n",
      "=== Outer iteration 3 ===\n",
      "INFO: Optimizing D, k, g...\n",
      "INFO: Fitted: D=2.0495, k=0.0100, g=50.0000, MSE=1.338746, R²=-0.4129\n",
      "INFO: Optimizing source time courses...\n",
      "INFO:   D=2.0495, k=0.0100, g=50.0000, MSE=1.338744\n",
      "INFO: Converged!\n",
      "INFO: \n",
      "Final: D=2.0495, k=0.0100, g=50.0000, MSE=1.338744, R²=-0.4129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "POINT SOURCE MODEL RESULTS\n",
      "============================================================\n",
      "Diffusion coefficient D = 2.0495 pixels²/s\n",
      "Decay rate k = 0.0100 s⁻¹\n",
      "Source gain g = 50.0000\n",
      "\n",
      "Fit quality:\n",
      "  MSE = 1.338744\n",
      "  R² = -0.4129\n"
     ]
    }
   ],
   "source": [
    "# Fit the point source diffusion model\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Fit with joint optimization of a(t), D, k, g\n",
    "result_v3 = diff3.fit_point_source_model(\n",
    "    g5, \n",
    "    mask_updated.astype(bool),\n",
    "    combined_sources,\n",
    "    dt=0.533,\n",
    "    D0=1.0,       # Initial guess for D (should be higher now!)\n",
    "    k0=0.5,       # Initial guess for k\n",
    "    g0=1.0,       # Initial guess for g\n",
    "    fit_a_t=True, # Jointly optimize time courses\n",
    "    loss_subsample=30000,\n",
    "    max_iter=100,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POINT SOURCE MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Diffusion coefficient D = {result_v3['D']:.4f} pixels²/s\")\n",
    "print(f\"Decay rate k = {result_v3['k']:.4f} s⁻¹\")\n",
    "print(f\"Source gain g = {result_v3['g']:.4f}\")\n",
    "print(f\"\\nFit quality:\")\n",
    "print(f\"  MSE = {result_v3['mse']:.6f}\")\n",
    "print(f\"  R² = {result_v3['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "180ec241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize source activity (locations and time courses)\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "# Create a PointSourceResult-like object for visualization\n",
    "# (since fit_point_source_model returns a dict, we need to construct the visualization manually)\n",
    "\n",
    "# Source locations and time courses\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), constrained_layout=True)\n",
    "\n",
    "# Left: Source locations on mean image\n",
    "mean_img = np.mean(g5, axis=0)\n",
    "vmax = np.percentile(mean_img[mask_updated], 99) if mask_updated.sum() > 0 else mean_img.max()\n",
    "\n",
    "axes[0].imshow(mean_img, cmap='gray', vmin=0, vmax=vmax)\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(combined_sources)))\n",
    "from matplotlib.patches import Circle\n",
    "for i, (row, col) in enumerate(combined_sources):\n",
    "    circle = Circle((col, row), radius=4, color=colors[i], fill=False, linewidth=2)\n",
    "    axes[0].add_patch(circle)\n",
    "    axes[0].text(col + 5, row, f'{i+1}', color=colors[i], fontsize=10, fontweight='bold')\n",
    "\n",
    "axes[0].set_title('Source Locations')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Right: Time courses\n",
    "t = np.arange(result_v3['a_t'].shape[0]) * 0.533\n",
    "for i in range(result_v3['a_t'].shape[1]):\n",
    "    axes[1].plot(t, result_v3['a_t'][:, i], label=f'Source {i+1}', \n",
    "                color=colors[i], linewidth=1.5)\n",
    "\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('Source Activity')\n",
    "axes[1].set_title('Fitted Source Time Courses')\n",
    "axes[1].legend(ncol=2, frameon=False)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Point Source Model\\nD={result_v3[\"D\"]:.3f} px²/s, k={result_v3[\"k\"]:.3f} s⁻¹, R²={result_v3[\"r_squared\"]:.3f}', \n",
    "             fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6daa75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare observed vs reconstructed at selected time points\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "time_points = [0, 50, 100, 150, 199]  # Select time points to visualize\n",
    "n_times = len(time_points)\n",
    "\n",
    "fig, axes = plt.subplots(3, n_times, figsize=(15, 10), constrained_layout=True)\n",
    "\n",
    "Ihat = result_v3['Ihat']  # (T+1, N) reconstruction\n",
    "\n",
    "for col, t in enumerate(time_points):\n",
    "    # Row 1: Observed\n",
    "    obs = g5[t]\n",
    "    vmax = np.percentile(obs[mask_updated], 99) if mask_updated.sum() > 0 else obs.max()\n",
    "    axes[0, col].imshow(obs, cmap='viridis', vmin=0, vmax=vmax)\n",
    "    axes[0, col].set_title(f't = {t * 0.533:.1f}s')\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    # Row 2: Reconstructed\n",
    "    recon = np.zeros_like(obs)\n",
    "    recon[mask_updated] = Ihat[t+1]  # t+1 because Ihat[0] is initial condition\n",
    "    axes[1, col].imshow(recon, cmap='viridis', vmin=0, vmax=vmax)\n",
    "    axes[1, col].axis('off')\n",
    "    \n",
    "    # Row 3: Residual\n",
    "    resid = np.zeros_like(obs)\n",
    "    resid[mask_updated] = obs[mask_updated] - Ihat[t+1]\n",
    "    vlim = np.percentile(np.abs(resid[mask_updated]), 95) if mask_updated.sum() > 0 else 1\n",
    "    axes[2, col].imshow(resid, cmap='RdBu_r', vmin=-vlim, vmax=vlim)\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Observed', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Residual', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Point Source Model Reconstruction\\n'\n",
    "             f'D={result_v3[\"D\"]:.3f} px²/s, k={result_v3[\"k\"]:.3f} s⁻¹, g={result_v3[\"g\"]:.3f}, '\n",
    "             f'R²={result_v3[\"r_squared\"]:.3f}', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e277b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Binning movie 2x2...\n",
      "INFO: Detecting point sources...\n",
      "INFO: Using 8 point sources\n",
      "INFO: Fitting point source diffusion model...\n",
      "INFO: \n",
      "=== Outer iteration 1 ===\n",
      "INFO: Optimizing D, k, g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance method: found 8 sources\n",
      "  Variance range: 2.351 - 15.455\n",
      "Early response method: found 8 sources\n",
      "  Response magnitude range: 0.491 - 1.056\n",
      "Peak detection method: found 8 sources\n",
      "  Peak height range: 6.725 - 12.832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Fitted: D=50.0000, k=0.0163, g=21.3328, MSE=0.754277, R²=-0.0048\n",
      "INFO: Optimizing source time courses...\n",
      "INFO:   D=50.0000, k=0.0163, g=21.3328, MSE=0.748770\n",
      "c:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:1028: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if abs(prev_mse - mse) / (prev_mse + 1e-10) < 1e-4:\n",
      "INFO: \n",
      "=== Outer iteration 2 ===\n",
      "INFO: Optimizing D, k, g...\n",
      "INFO: Fitted: D=50.0000, k=0.0125, g=14.7717, MSE=0.727345, R²=0.0311\n",
      "INFO: Optimizing source time courses...\n",
      "INFO:   D=50.0000, k=0.0125, g=14.7717, MSE=0.860590\n",
      "INFO: \n",
      "=== Outer iteration 3 ===\n",
      "INFO: Optimizing D, k, g...\n",
      "INFO: Fitted: D=50.0000, k=0.0115, g=9.9734, MSE=0.726696, R²=0.0320\n",
      "INFO: Optimizing source time courses...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m _ = importlib.reload(sys.modules[\u001b[33m'\u001b[39m\u001b[33mdiffusion_v3\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This runs the complete pipeline: detection → fitting\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result_full = \u001b[43mdiff3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_diffusion_point_sources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_updated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.533\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_sources\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbin_factors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#  additional binning on top of existing (2,2)\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_detection_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiff3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSourceDetectionConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_sources\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvariance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mearly_response\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpeak_detection\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_exclusion_radius\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_combine_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvariance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_a_t\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_subsample\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFULL PIPELINE RESULTS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:1219\u001b[39m, in \u001b[36mfit_diffusion_point_sources\u001b[39m\u001b[34m(Y, mask, dt, n_sources, bin_factors, source_detection_config, source_combine_method, D0, k0, g0, fit_a_t, loss_subsample, theta, verbose)\u001b[39m\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1217\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mFitting point source diffusion model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m result = \u001b[43mfit_point_source_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_locations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mg0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_a_t\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_a_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_subsample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_subsample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[38;5;66;03m# Compute residuals on masked pixels\u001b[39;00m\n\u001b[32m   1229\u001b[39m Yv = Y_bin[:, mask_bin]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:881\u001b[39m, in \u001b[36mfit_point_source_model\u001b[39m\u001b[34m(Y, mask, source_locations, dt, D0, k0, g0, fit_a_t, a_t_init, bounds, loss_subsample, theta, max_iter, verbose)\u001b[39m\n\u001b[32m    876\u001b[39m best_params = [\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fit_a_t:\n\u001b[32m    879\u001b[39m     \u001b[38;5;66;03m# Joint optimization of D, k, g, and a_t\u001b[39;00m\n\u001b[32m    880\u001b[39m     \u001b[38;5;66;03m# This is more expensive but more accurate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fit_joint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYv_sub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma_t_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# Only optimize D, k, g with fixed a_t\u001b[39;00m\n\u001b[32m    887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _fit_transport_only(\n\u001b[32m    888\u001b[39m         sim, Yv, y0, pixel_idx, Yv_sub, a_t_init,\n\u001b[32m    889\u001b[39m         D0, k0, g0, bounds, max_iter, verbose\n\u001b[32m    890\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:1016\u001b[39m, in \u001b[36m_fit_joint\u001b[39m\u001b[34m(Y, mask, source_locations, sim, Yv, y0, pixel_idx, Yv_sub, a_t_init, D0, k0, g0, bounds, max_iter, verbose)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1014\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mOptimizing source time courses...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m a_t = \u001b[43m_optimize_time_courses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# Check convergence\u001b[39;00m\n\u001b[32m   1022\u001b[39m Ihat = sim.simulate(y0, a_t, D_hat, k_hat, g_hat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:1083\u001b[39m, in \u001b[36m_optimize_time_courses\u001b[39m\u001b[34m(sim, Yv, y0, D, k, g, a_t, n_iter, verbose)\u001b[39m\n\u001b[32m   1081\u001b[39m a_t_no_m = a_t.copy()\n\u001b[32m   1082\u001b[39m a_t_no_m[:, m] = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m Ihat_no_m = \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_t_no_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m residual = Yv - Ihat_no_m[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# (T, N)\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# Effect of source m at node j = source_nodes[m]\u001b[39;00m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# When a_t[t, m] = 1, it adds dt * g * delta(j) to the source term\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;66;03m# The response decays and diffuses from there\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1092\u001b[39m \u001b[38;5;66;03m# minimizes ||residual[t] - alpha * response||^2\u001b[39;00m\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# where response is the contribution from source m at time t\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\diffusion_v3.py:759\u001b[39m, in \u001b[36mPointSourceSimulator.simulate\u001b[39m\u001b[34m(self, y0, a_t, D, k, g)\u001b[39m\n\u001b[32m    756\u001b[39m     rhs = out[t] + \u001b[38;5;28mself\u001b[39m.dt * src\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._solve_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     out[t + \u001b[32m1\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_solve_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m     out[t + \u001b[32m1\u001b[39m], _ = spla.cg(\u001b[38;5;28mself\u001b[39m._M_left_backup, rhs, x0=out[t], tol=\u001b[32m1e-8\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Use the full pipeline function for convenience\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "# This runs the complete pipeline: detection → fitting\n",
    "result_full = diff3.fit_diffusion_point_sources(\n",
    "    g5,\n",
    "    mask_updated.astype(bool),\n",
    "    dt=0.533,\n",
    "    n_sources=20,\n",
    "    bin_factors=(2, 2),  #  additional binning on top of existing (2,2)\n",
    "    source_detection_config=diff3.SourceDetectionConfig(\n",
    "        n_sources=8,\n",
    "        methods=('variance', 'early_response', 'peak_detection'),\n",
    "        spatial_exclusion_radius=8\n",
    "    ),\n",
    "    source_combine_method='variance',\n",
    "    D0=0.01,\n",
    "    k0=1.0,\n",
    "    g0=1.0,\n",
    "    fit_a_t=True,\n",
    "    loss_subsample=30000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL PIPELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of sources: {result_full.n_sources}\")\n",
    "print(f\"Diffusion coefficient D = {result_full.D:.4f} pixels²/s\")\n",
    "print(f\"Decay rate k = {result_full.k:.4f} s⁻¹\")\n",
    "print(f\"Source gain g = {result_full.g:.4f}\")\n",
    "print(f\"\\nFit quality:\")\n",
    "print(f\"  MSE = {result_full.mse:.6f}\")\n",
    "print(f\"  R² = {result_full.r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "018595b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results using built-in functions\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "# Visualize fit quality\n",
    "diff3.visualize_fit_result(result_full, time_points=[0, 50, 100, 150, 199], figsize=(15, 10))\n",
    "\n",
    "# Visualize source activity\n",
    "diff3.visualize_source_activity(result_full, figsize=(14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "576abf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how diffusion spreads from a single source (impulse response)\n",
    "_ = importlib.reload(sys.modules['diffusion_v3'])\n",
    "\n",
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "# Show diffusion spread from source 0\n",
    "diff3.plot_diffusion_spread(\n",
    "    result_full, \n",
    "    source_idx=0, \n",
    "    time_after_pulse=[0.5, 1.0, 2.0, 5.0, 20.0],\n",
    "    figsize=(16, 4)\n",
    ")\n",
    "\n",
    "# Also try source 1 if it exists\n",
    "if result_full.n_sources > 1:\n",
    "    diff3.plot_diffusion_spread(\n",
    "        result_full, \n",
    "        source_idx=1, \n",
    "        time_after_pulse=[0.5, 1.0, 2.0, 5.0],\n",
    "        figsize=(16, 4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae01ca",
   "metadata": {},
   "source": [
    "## NMF SOURCE DIFFUSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daecf675",
   "metadata": {},
   "source": [
    "### SMOKE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusion_v2 as diff\n",
    "_ = importlib.reload(sys.modules['diffusion_v2'])\n",
    "\n",
    "diff.run_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3a1e7",
   "metadata": {},
   "source": [
    "### FIT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b920207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Input: T=499, H=100, W=250, N_masked=6746\n",
      "INFO: Binning by (2, 2)\n",
      "INFO: After binning: T=499, H=50, W=125, N_masked=1594\n",
      "INFO: Building Laplacian...\n",
      "INFO: Stability: λ_max≈7.94, D_max_explicit=0.47\n",
      "INFO: Learning 3 sources via NMF...\n",
      "INFO: NMF sources learned: M=3, recon_error=0.8281\n",
      "INFO: NMF reconstruction error: 0.8281\n",
      "INFO: Fitting D, k, g...\n",
      "INFO: Eval 10: D=0.4971, k=0.1437, g=1.4012, MSE=0.701969\n",
      "INFO: Eval 20: D=0.4778, k=0.1032, g=2.1255, MSE=1.136951\n",
      "INFO: Eval 30: D=0.4843, k=0.1262, g=1.6800, MSE=0.562474\n",
      "INFO: Eval 40: D=0.4711, k=0.1298, g=1.7115, MSE=0.560623\n",
      "INFO: Eval 50: D=0.0890, k=0.5607, g=6.6263, MSE=0.499055\n",
      "INFO: Eval 60: D=0.0510, k=0.5178, g=6.9496, MSE=0.471288\n",
      "INFO: Eval 70: D=0.0452, k=0.4583, g=5.9911, MSE=0.468576\n",
      "INFO: Eval 80: D=0.0167, k=0.3059, g=3.9554, MSE=0.462175\n",
      "INFO: Eval 90: D=0.0026, k=0.2784, g=3.7025, MSE=0.457139\n",
      "INFO: Eval 100: D=0.0008, k=0.2738, g=3.6406, MSE=0.456718\n",
      "INFO: Eval 110: D=0.0001, k=0.2807, g=3.7173, MSE=0.456531\n",
      "INFO: Eval 120: D=0.0001, k=0.2804, g=3.7169, MSE=0.456530\n",
      "INFO: Eval 130: D=0.0001, k=0.2794, g=3.7030, MSE=0.456525\n",
      "INFO: Fitted: D=0.0001, k=0.2794, g=3.7030, MSE=0.456525\n",
      "INFO: Computing final reconstruction...\n",
      "INFO: Final: MSE=0.456525, R²=0.4379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FITTED PARAMETERS\n",
      "==================================================\n",
      "Diffusion coefficient D = 0.0001 pixels²/s\n",
      "Decay rate k = 0.2794 s⁻¹\n",
      "Source gain g = 3.7030\n",
      "\n",
      "Fit quality:\n",
      "  MSE = 0.456525\n",
      "  R² = 0.4379\n",
      "\n",
      "Data dimensions:\n",
      "  T = 499 frames\n",
      "  N = 1594 masked pixels\n",
      "  M = 3 source components\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPROVED DIFFUSION MODEL FITTING (diffusion_v2)\n",
    "# ============================================================\n",
    "# Uses the new diffusion_v2 module with:\n",
    "# - Vectorized Laplacian construction with validation\n",
    "# - Cached LU factorization for efficient simulation\n",
    "# - Cleaner NMF source learning with preprocessing options\n",
    "# - Staged fitting: learn sources → fit D,k,g → optional refinement\n",
    "# - Comprehensive result dataclass with diagnostics\n",
    "\n",
    "import diffusion_v2 as diff\n",
    "_ = importlib.reload(sys.modules['diffusion_v2'])\n",
    "\n",
    "\n",
    "import logging\n",
    "# Enable logging to see progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Fit the reaction-diffusion model\n",
    "# Y: (T, H, W) fluorescence movie (already F/F0 or ΔR/R0)\n",
    "# mask: (H, W) boolean tissue mask\n",
    "\n",
    "reg_config = diff.RegularizationConfig(\n",
    "    source_l1_weight=0.1,\n",
    "    D_prior_weight=0.1,\n",
    "    D_prior_mean=0.05  # Your expected D value\n",
    ")\n",
    "\n",
    "result = diff.fit_diffusion_model_2d(\n",
    "    g5, \n",
    "    mask_updated.astype(bool),\n",
    "    reg_config=None,\n",
    "    dt=0.533,                    # seconds per frame\n",
    "    n_sources=3,                # number of source components (start with 3-8)\n",
    "    bin_factors=(2,2),          # (1,1) = no additional binning (data already 2x2 binned)\n",
    "    hp_sigma_frames=3.0,         # high-pass filter for source learning\n",
    "    loss_subsample=100000,        # subsample pixels for faster optimization\n",
    "    theta=1.0,                   # 1.0=backward Euler (stable), 0.5=Crank-Nicolson\n",
    "    refine_sources=False,        # set True to refine time courses after fitting\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print fitted parameters\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FITTED PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Diffusion coefficient D = {result.D:.4f} pixels²/s\")\n",
    "print(f\"Decay rate k = {result.k:.4f} s⁻¹\")\n",
    "print(f\"Source gain g = {result.g:.4f}\")\n",
    "print(f\"\\nFit quality:\")\n",
    "print(f\"  MSE = {result.mse:.6f}\")\n",
    "print(f\"  R² = {result.r_squared:.4f}\")\n",
    "print(f\"\\nData dimensions:\")\n",
    "print(f\"  T = {result.Y_used.shape[0]} frames\")\n",
    "print(f\"  N = {result.mask_used.sum()} masked pixels\")\n",
    "print(f\"  M = {result.n_sources} source components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aa74f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned source spatial maps and time courses\n",
    "%matplotlib qt\n",
    "_ = importlib.reload(sys.modules['diffusion_v2'])\n",
    "diff.visualize_sources(result, max_sources=5, figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c8ec787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare observed vs reconstructed at selected time points\n",
    "_ = importlib.reload(sys.modules['diffusion_v2'])\n",
    "diff.plot_reconstruction_comparison(result, time_points=[0, 50, 100, 150, 199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87a40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute detailed residual metrics\n",
    "_ = importlib.reload(sys.modules['diffusion_v2'])\n",
    "metrics = diff.compute_residual_metrics(result)\n",
    "print(\"Residual Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b25bc",
   "metadata": {},
   "source": [
    "### Select number of sources using elbow method (optional)\n",
    "Use this to determine the optimal number of source components M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12403bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow plot to select number of sources (optional - takes a few minutes)\n",
    "M_best, errors = diff.select_n_sources_elbow(g5, mask, M_range=range(2, 15), plot=True)\n",
    "print(f\"Suggested number of sources: M = {M_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape a source spatial map to image for custom visualization\n",
    "source_idx = 0  # which source to visualize\n",
    "phi_img = diff.reshape_to_image(result.Phi[:, source_idx], result.mask_used)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(phi_img, cmap='hot')\n",
    "plt.colorbar(label='Spatial weight')\n",
    "plt.title(f'Source {source_idx+1} spatial map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "t = np.arange(result.a_t.shape[0]) * result.dt\n",
    "plt.plot(t, result.a_t[:, source_idx], 'b-', lw=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(f'Source {source_idx+1} time course')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5ht-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
