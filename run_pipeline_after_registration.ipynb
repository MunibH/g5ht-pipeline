{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f49ca6",
   "metadata": {},
   "source": [
    "### CONDA ENVIRONMENTS\n",
    "\n",
    "For steps __1. preprocess__ and __2. mip__, `conda activate g5ht-pipeline`\n",
    "\n",
    "For step __3. segment__, `conda activate segment-torch` or `conda activate torchcu129`\n",
    "\n",
    "For step __4. spline, 5. orient, 6. warp, 7. reg__\n",
    "\n",
    "## TODO:\n",
    "\n",
    "1. I wonder if I computed a spline on each and every z slice and warped each, oriented each of them, and warped each of them, if the problem of weirdly sheared image stacks would be solved\n",
    "2. quick mp4 for all recordings\n",
    "   1. now working in engaging, works per one nd2 sbatch\n",
    "3. focus check for all recordings\n",
    "   1. maybe focus check can be used to specify which z slices are good to use and which frames are good to use\n",
    "4. for recordings starting in december 2025, need to trim first 2 rather than last 2 z slices\n",
    "5. flip worms so that VNC is always up\n",
    "6. fixed mask could be automated, but if not, make sure to save which index is fixed\n",
    "7. extract behavior\n",
    "8. posture similarity\n",
    "   1. posture might consist of the spline + thresholded z-stack\n",
    "      1. I'm thinking that the orientation shouldn't matter, but the z-planes in focus will, and curvature/spline of the head will\n",
    "      2. maybe need to actually interpolate to 117 z slices\n",
    "   2. sub registration problems\n",
    "   3. label each set of registered frames with one set of ROIs, or auto segment ROIs from each set of registered frames\n",
    "9.  track z over time, which zslices are consistent\n",
    "   1. focus + correlation\n",
    "10. beads -> train/test\n",
    "11. gfp+1 relative to rfp channel (might only apply to pre december 2025 recordings)\n",
    "12. wholistic \n",
    "    1.  parameter sweep, might change\n",
    "    2.  python version\n",
    "13. autocorr/scorr\n",
    "14. automate z slice trimming\n",
    "    1.  pre december 2025 (trim last 2 z slices)\n",
    "    2.  post december 2025 (trim first z slice)\n",
    "15. photobleaching estimation?\n",
    "    1.  record immo with serotonin\n",
    "    2.  at least do it for RFP\n",
    "16. try deltaF/F [ (F(t) - F0) / F0 ]\n",
    "17. coding directions (preencounter-baseline) (postencounter-baseline)\n",
    "    1.  then show voxel weights\n",
    "18. port everything to engaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d2b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    is_torch_env = False\n",
    "except ImportError:\n",
    "    is_torch_env = True\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252d5dd",
   "metadata": {},
   "source": [
    "## SPECIFY DATA TO PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66403bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm005.nd2\n",
      "Num z-slices:  41\n",
      "Number of frames:  715\n",
      "Height:  512\n",
      "width:  512\n",
      "Number of channels:  2\n",
      "Beads alignment file:  D:\\DATA\\g5ht-free\\20260123\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm005_chan_alignment.nd2\n"
     ]
    }
   ],
   "source": [
    "# DATA_PTH = r'C:\\Users\\munib\\POSTDOC\\DATA\\fluorescent_beads_ch_align\\20251219'\n",
    "DATA_PTH = r'D:\\DATA\\g5ht-free\\20260123'\n",
    "\n",
    "INPUT_ND2 = 'date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm005.nd2'\n",
    "\n",
    "INPUT_ND2_PTH = os.path.join(DATA_PTH, INPUT_ND2)\n",
    "\n",
    "NOISE_PTH = r'C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\noise\\noise_042925.tif'\n",
    "\n",
    "OUT_DIR = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "STACK_LENGTH = 41\n",
    "\n",
    "if not is_torch_env:\n",
    "    noise_stack = utils.get_noise_stack(NOISE_PTH, STACK_LENGTH)\n",
    "    num_frames, height, width, num_channels = utils.get_range_from_nd2(INPUT_ND2_PTH, stack_length=STACK_LENGTH) \n",
    "    beads_alignment_file = utils.get_beads_alignment_file(INPUT_ND2_PTH)\n",
    "else:\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")\n",
    "\n",
    "print(INPUT_ND2)\n",
    "print('Num z-slices: ', STACK_LENGTH)\n",
    "if not is_torch_env:\n",
    "    print('Number of frames: ', num_frames)\n",
    "    print('Height: ', height)\n",
    "    print('width: ', width)\n",
    "    print('Number of channels: ', num_channels)\n",
    "    print('Beads alignment file: ', beads_alignment_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835cabb",
   "metadata": {},
   "source": [
    "## 10. LABEL ROIs\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- after this step, use `lbl` conda env to label ROI of fixed frame\n",
    "  - run `labelme` in terminal\n",
    "\n",
    "\n",
    "maybe also see here for video annotation: https://github.com/wkentaro/labelme/tree/main/examples/video_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1d48b",
   "metadata": {},
   "source": [
    "### EXPORT FIXED VOLUME AS PNGs for labeling with `labelme`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683fca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that exports each z-slice of fixed.tif as a separate png\n",
    "import tifffile\n",
    "import os\n",
    "import glob\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "# in PTH directory, find a fixed_XXXX*.tif file, where XXXX are digits\n",
    "fixed_fn = glob.glob(os.path.join(PTH, 'fixed_[0-9][0-9][0-9][0-9]*.tif'))[0]\n",
    "fixed_pth = os.path.join(PTH, fixed_fn)\n",
    "\n",
    "# fixed_pth = os.path.join(PTH, 'fixed.tif')\n",
    "# fixed_stack = ndi.zoom(tifffile.imread(fixed_pth), zoom=(3,1,1,1))\n",
    "fixed_stack = tifffile.imread(fixed_pth)\n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_png')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for i in range(fixed_stack.shape[0]):\n",
    "    slice_pth = os.path.join(out_dir, f'fixed_z{i:02d}.png')\n",
    "    # make sure to save channel 1, and that it is visible, correct data type, clipped to 0-255\n",
    "    slice_img = fixed_stack[i,1,:,:]\n",
    "    slice_img = (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255\n",
    "    slice_img = slice_img.astype('uint8')\n",
    "    tifffile.imwrite(slice_pth, slice_img) \n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_xz_png')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for i in range(fixed_stack.shape[2]):\n",
    "    slice_pth = os.path.join(out_dir, f'fixed_xz{i:02d}.png')\n",
    "    # make sure to save channel 1, and that it is visible, correct data type, clipped to 0-255\n",
    "    slice_img = fixed_stack[:,1,i,:]\n",
    "    slice_img = (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255\n",
    "    slice_img = slice_img.astype('uint8')\n",
    "    tifffile.imwrite(slice_pth, slice_img) \n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_yz_png')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for i in range(fixed_stack.shape[3]):\n",
    "    slice_pth = os.path.join(out_dir, f'fixed_yz{i:02d}.png')\n",
    "    # make sure to save channel 1, and that it is visible, correct data type, clipped to 0-255\n",
    "    slice_img = fixed_stack[:,1,:,i]\n",
    "    slice_img = (slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255\n",
    "    slice_img = slice_img.astype('uint8')\n",
    "    # save as high quality tiff\n",
    "    tifffile.imwrite(slice_pth, slice_img) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d62cd",
   "metadata": {},
   "source": [
    "### PARSE OUTPUT OF `labelme`\n",
    "\n",
    "- outputs `roi.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebef4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dnc', 'vnc', 'nerve ring', 'isthmus']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1e46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from skimage.draw import polygon\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "out_dir = os.path.join(PTH, 'fixed_png')\n",
    "fixed_fn = glob.glob(os.path.join(PTH, 'fixed_[0-9][0-9][0-9][0-9]*.tif'))[0]\n",
    "fixed_pth = os.path.join(PTH, fixed_fn)\n",
    "fixed_stack = tifffile.imread(fixed_pth)\n",
    "Z,C,H,W = fixed_stack.shape\n",
    "\n",
    "roi = np.zeros((Z, H, W), dtype=fixed_stack.dtype) # ZHW\n",
    "\n",
    "# get all unique roi_labels from json files\n",
    "roi_json_files = glob.glob(os.path.join(out_dir, 'fixed_z[0-9][0-9]*.json'))\n",
    "roi_labels = []\n",
    "for roi_json_file in roi_json_files:\n",
    "    with open(roi_json_file, 'r') as f:\n",
    "        roi_dict = json.load(f)\n",
    "        roi_labels.append([shape['label'] for shape in roi_dict['shapes']])\n",
    "# get all unique roi_labels\n",
    "roi_labels = list(set([item for sublist in roi_labels for item in sublist]))\n",
    "\n",
    "# # roi_labels = ['PC','MC','IM','TB','NR','VNC','DNC']\n",
    "# # procorpus, metacorpus, isthmus, terminal bulb, nerve ring, ventral nerve cord, dorsal nerve cord\n",
    "\n",
    "for i in range(Z):\n",
    "    slice_roi_json = os.path.join(out_dir, f'fixed_z{i:02d}.json')\n",
    "    # if slice_roi_json doesn't exist, continue\n",
    "    if not os.path.exists(slice_roi_json):\n",
    "        continue\n",
    "    with open(slice_roi_json, 'r') as f:\n",
    "        roi_dict = json.load(f)\n",
    "        # loop through each shape in roi_dict['shapes']\n",
    "        for shape in roi_dict['shapes']:\n",
    "            label = shape['label']\n",
    "            if label in roi_labels:\n",
    "                points = shape['points']\n",
    "                # get integer coordinates\n",
    "                points = [(int(round(p[1])), int(round(p[0]))) for p in points]\n",
    "                # create a mask for the polygon\n",
    "                \n",
    "                rr, cc = polygon([p[0] for p in points], [p[1] for p in points], shape=(H,W))\n",
    "                # should set to correct z slice\n",
    "                roi[i, rr, cc] = roi_labels.index(label) + 1 # start from 1\n",
    "\n",
    "# save roi stack as tif image, imagej=true and save the roi labels as metadata\n",
    "roi_pth = os.path.join(PTH, 'roi.tif')\n",
    "tifffile.imwrite(roi_pth, roi.astype(np.uint8), imagej=True, metadata={'Labels': roi_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982518c",
   "metadata": {},
   "source": [
    "## 11. QUANTIFY\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "Have to first label dorsal and ventral nerve rings and pharynx. See ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import quantify\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import matplotlib\n",
    "font = {'family' : 'Arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "_ = importlib.reload(sys.modules['quantify'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "REG_DIR = r'registered_elastix'\n",
    "# PLOT_ONLY = True\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib qt\n",
    "\n",
    "\n",
    "sys.argv = [\"\", PTH, REG_DIR, PLOT_ONLY]\n",
    "quantify.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f608866",
   "metadata": {},
   "source": [
    "## 12 QUANTIFY VOXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import quantify_voxels\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.morphology import erosion, disk\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "_ = importlib.reload(sys.modules['quantify_voxels'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "reg_dir = 'registered'\n",
    "bin_factor = 2\n",
    "fps = 1/0.533\n",
    "\n",
    "sys.argv = [\"\", PTH, reg_dir, bin_factor]\n",
    "quantify_voxels.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalized_voxels.npy, also been binned\n",
    "g5 = np.load(os.path.join(PTH, 'normalized_voxels.npy'))\n",
    "rfp_mean = np.load(os.path.join(PTH, 'rfp_mean.npy'))\n",
    "gfp_mean = np.load(os.path.join(PTH, 'gfp_mean.npy'))\n",
    "baseline = np.load(os.path.join(PTH, 'baseline.npy'))\n",
    "\n",
    "# load mask, bin it\n",
    "# find the fixed_mask_*.tif file in PTH directory\n",
    "try:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask_*.tif'))[0]\n",
    "except:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask*.tif'))[0]\n",
    "\n",
    "mask = tifffile.imread(fixed_mask_fn)\n",
    "\n",
    "h, w = mask.shape\n",
    "h_binned = h // bin_factor\n",
    "w_binned = w // bin_factor\n",
    "# binning of mask\n",
    "mask_binned = mask.reshape(h_binned, bin_factor, w_binned, bin_factor).max(axis=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70462ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5_masked = g5 * mask_binned[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g5.shape)\n",
    "print(rfp_mean.shape)\n",
    "print(gfp_mean.shape)\n",
    "print(baseline.shape)\n",
    "print(mask_binned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5_masked.shape\n",
    "\n",
    "# g5_masked is shape (T, Z, H, W) array with zeros outside the worm region\n",
    "# find all voxels where g5_masked is zero for each time point\n",
    "zero_mask = g5_masked == 0\n",
    "# calculate the probability of a voxel being zero across time, but don't include time points where the voxel is masked out (i.e., outside the worm)\n",
    "zero_prob = np.mean(zero_mask, axis=0)\n",
    "# set zero probability to NaN for voxels outside the worm (mask binned needs a z dimension added)\n",
    "zero_prob[mask_binned[np.newaxis].repeat(zero_prob.shape[0], axis=0) == 0] = np.nan\n",
    "\n",
    "plt.close('all')\n",
    "%matplotlib qt\n",
    "# plot zero_prob as an image, with colorbar, for each z slice in subplots\n",
    "fig, axes = plt.subplots(4, 10, figsize=(20, 8), constrained_layout=True)\n",
    "for z in range(zero_prob.shape[0]):\n",
    "    ax = axes[z // 10, z % 10]\n",
    "    im = ax.imshow(zero_prob[z, :, :], cmap='viridis', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Z={z}')\n",
    "    ax.axis('off')\n",
    "    # replace last subplot with colorbar\n",
    "    if z == zero_prob.shape[0] - 1:\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "# delete last subplots if z slices are less than 40\n",
    "for z in range(zero_prob.shape[0], 40):\n",
    "    ax = axes[z // 10, z % 10]\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(zero_prob.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Probability of Voxel Being Zero')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Zero Probability Across Voxels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = 0.3 # if a voxel is zero more than this fraction of the time, consider it outside the worm or a bad voxel\n",
    "# create a mask of good voxels\n",
    "good_voxel_mask = zero_prob < probability_threshold\n",
    "normalized_data = g5_masked * good_voxel_mask[np.newaxis].repeat(g5_masked.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3303f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_mean = np.mean(normalized_data, axis=0)\n",
    "\n",
    "# clip norm_data mean between 0th and 99th percentiles\n",
    "p0 = np.percentile(norm_data_mean, 0)\n",
    "p99 = np.percentile(norm_data_mean, 99)\n",
    "norm_data_mean = np.clip(norm_data_mean, p0, p99)\n",
    "\n",
    "z2plot = [0,5,15,25,35]\n",
    "# z2plot = [0,5]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplots(len(z2plot), 3, figsize=(15, 9), constrained_layout=True)\n",
    "for i in z2plot: # for each z slice\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+1)\n",
    "    plt.pcolor(gfp_mean[i,:,:])\n",
    "    plt.title(f'GFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+2)\n",
    "    plt.pcolor(rfp_mean[i,:,:])\n",
    "    plt.title(f'RFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+3)\n",
    "    plt.pcolor(norm_data_mean[i,:,:])\n",
    "    # add colorbar outside to the right\n",
    "    plt.colorbar()\n",
    "    # plt.clim(0, 12)\n",
    "    # make all axes not be squished\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster normalized_data using k means\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "T, Z, H, W = normalized_data.shape\n",
    "normalized_data_reshaped = normalized_data.reshape(T, Z*H*W).T  # shape (Z*H*W, T)\n",
    "kmeans.fit(normalized_data_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster mean activity over time\n",
    "cluster_means = np.zeros((n_clusters, T))\n",
    "for c in range(n_clusters):\n",
    "    cluster_means[c, :] = normalized_data_reshaped[kmeans.labels_ == c, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ea679",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "t = np.arange(T) * (1/fps)\n",
    "ic = 0\n",
    "for c in range(6,9):#range(n_clusters):\n",
    "    plt.plot(t, cluster_means[c, :], label=f'Cluster {ic+1}')\n",
    "    ic += 1\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('$\\ R / R_{baseline}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ef667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster 1 and 2 spatial maps at each z slice\n",
    "plt.close('all')\n",
    "ic = 0\n",
    "for c in range(6,9):#range(n_clusters):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(20, 8), constrained_layout=True)\n",
    "    for z in range(Z):\n",
    "        ax = axes[z // 10, z % 10]\n",
    "        cluster_map = kmeans.labels_.reshape(Z, H, W)[z, :, :] == c\n",
    "        im = ax.imshow(cluster_map, cmap='gray')\n",
    "        ax.set_title(f'Cluster {ic+1} Z={z}')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'Spatial Map of Cluster {c+1} Across Z Slices')\n",
    "    ic += 1\n",
    "    # deleate last subplots if z slices are less than 40\n",
    "    for z in range(Z, 40):\n",
    "        ax = axes[z // 10, z % 10]\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# flatten normalized_data to (T, Z*H*W)\n",
    "T, Z, H, W = normalized_data.shape\n",
    "data_reshaped = normalized_data.reshape(T, Z*H*W)\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get scores, plot\n",
    "scores = pca.transform(data_reshaped)\n",
    "# plot the first 5 principal component scores\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    plt.plot(scores[:,i])\n",
    "    plt.show()\n",
    "\n",
    "# plot the first 5 principal component weights as images across z slices\n",
    "components = pca.components_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot gfp_mean, rfp_mean, dat in a (5,3) grid of subplots (5 z slices, 3 columns)\n",
    "\n",
    "\n",
    "# divide gfp_mean by rfp_mean, but only if rfp_mean is not zero\n",
    "\n",
    "dat = gfp_mean / rfp_mean\n",
    "# dat = np.divide(gfp_mean, rfp_mean, out=np.zeros_like(gfp_mean), where=rfp_mean!=0)\n",
    "# divide each voxel in dat by its baseline, but only if baseline is not zero\n",
    "dat = np.divide(dat, baseline, out=np.zeros_like(dat), where=baseline!=0)\n",
    "dat = dat * mask_binned[np.newaxis, :, :]\n",
    "\n",
    "# remove outliers by clipping to 1st and 99th percentile\n",
    "p1 = np.percentile(dat, 0)\n",
    "p99 = np.percentile(dat, 99.9)\n",
    "dat = np.clip(dat, p1, p99)\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(baseline.ravel(), bins=1000, range=(0,100))\n",
    "# plt.show()\n",
    "\n",
    "# z2plot = [0,5,15,25,35]\n",
    "z2plot = [0,5]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplots(len(z2plot), 3, figsize=(15, 5), constrained_layout=True)\n",
    "for i in z2plot: # for each z slice\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+1)\n",
    "    plt.pcolor(gfp_mean[i,:,:])\n",
    "    plt.title(f'GFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+2)\n",
    "    plt.pcolor(rfp_mean[i,:,:])\n",
    "    plt.title(f'RFP Mean Z={i}')\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(len(z2plot), 3, z2plot.index(i)*3+3)\n",
    "    plt.pcolor(dat[i,:,:])\n",
    "    # add colorbar outside to the right\n",
    "    plt.colorbar()\n",
    "    plt.clim(0, 12)\n",
    "    # make all axes not be squished\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mask, bin\n",
    "# find the fixed_mask_*.tif file in PTH directory\n",
    "try:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask_*.tif'))[0]\n",
    "except:\n",
    "    fixed_mask_fn = glob.glob(os.path.join(PTH, 'fixed_mask*.tif'))[0]\n",
    "\n",
    "mask = tifffile.imread(fixed_mask_fn)\n",
    "\n",
    "h, w = mask.shape\n",
    "h_binned = h // bin_factor\n",
    "w_binned = w // bin_factor\n",
    "# binning of mask\n",
    "mask_binned = mask.reshape(h_binned, bin_factor, w_binned, bin_factor).max(axis=(1,3))\n",
    "# shrink the mask slightly using erosion skimage\n",
    "mask_binned = erosion(mask_binned, disk(5))\n",
    "\n",
    "# zero out values outside the mask\n",
    "g5_masked = g5 * mask_binned[np.newaxis,np.newaxis,:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4596ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g5_masked_trimmed  = g5_masked[:, 5:27, :,40:230]\n",
    "g5_masked_trimmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3602248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# get the intensity over time of the voxels in the middle of the (x,y) plane, definding a box, and all z slices\n",
    "middle_x = int(g5_masked_trimmed.shape[3] // 1.5)\n",
    "middle_y = int(g5_masked_trimmed.shape[2] // 1.95)\n",
    "box_size = 10  # Define the size of the box around the middle point\n",
    "middle_voxels = g5_masked_trimmed[:, 5:27, middle_y-box_size:middle_y+box_size, middle_x-box_size:middle_x+box_size].mean(axis=(2,3))\n",
    "# divide each z slice by the mean activity in the first 60 time points to get F/F0\n",
    "middle_voxels = middle_voxels / (middle_voxels[:60,:].mean(axis=0) + 1e-6)\n",
    "\n",
    "t = np.arange(middle_voxels.shape[0]) / fps\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(middle_voxels)\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('$F/F_{\\\\mathrm{baseline}}$')\n",
    "# plt.ylim(0, 5)\n",
    "plt.show()\n",
    "# plot same thing as heatmap and sort by mean signal across time for each z slice\n",
    "sorted_indices = np.argsort(middle_voxels.mean(axis=0))\n",
    "middle_voxels_sorted = middle_voxels[:, sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolor(t, np.arange(middle_voxels.shape[1]), middle_voxels_sorted.T, cmap='plasma',vmin=0,vmax=5)\n",
    "plt.colorbar(label='$F/F_{\\\\mathrm{baseline}}$')\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Z slice (sorted by mean intensity)')\n",
    "plt.show()\n",
    "\n",
    "# plot g5_masked and the box defined by box_size and middle_x, middle_y to confirm that the box is in the middle of the (x,y) plane and covers the expected area\n",
    "for i in range(g5_masked_trimmed.shape[1]):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.pcolor(g5_masked_trimmed[158,i,:,:],vmin=0, vmax=10)\n",
    "    plt.plot([middle_x-box_size, middle_x+box_size, middle_x+box_size, middle_x-box_size, middle_x-box_size],\n",
    "             [middle_y-box_size, middle_y-box_size, middle_y+box_size, middle_y+box_size, middle_y-box_size], color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962113d",
   "metadata": {},
   "source": [
    "# GRID ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid_analysis\n",
    "import sys\n",
    "import importlib\n",
    "_ = importlib.reload(sys.modules['grid_analysis'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "\n",
    "xspace = 10\n",
    "yspace = 10\n",
    "fps = 1/0.533\n",
    "bin_factor = 1\n",
    "z_start = 5\n",
    "z_end = 25\n",
    "baseline_start_sec = 5 # start baseline calculation here (relative to the start of the recording, not relative to start_time_sec)\n",
    "baseline_end_sec = 35 # end baseline calculation here (relative to the start of the recording, not relative to start_time_sec)\n",
    "start_time_sec = 0\n",
    "time_food_sec = 435\n",
    "sys.argv = [\"\", PTH, xspace, yspace, fps, bin_factor, z_start, z_end, baseline_start_sec, baseline_end_sec, start_time_sec]#time_food_sec] \n",
    "grid_analysis.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de75150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid_analysis\n",
    "_ = importlib.reload(sys.modules['grid_analysis'])\n",
    "# load the grid_timeseries_flat_smoothed.npz file and plot the heatmap using the plot_heatmap function\n",
    "data = np.load(os.path.join(PTH, 'grid_timeseries_flat_smoothed.npz'))\n",
    "flat_timeseries_smoothed = data['timeseries']\n",
    "z_labels = data['z_labels']\n",
    "grid_analysis.plot_heatmap(flat_timeseries_smoothed, z_labels, fps=1/0.533, output_path=os.path.join(PTH, 'grid_heatmap.png'), data_start_time=start_time_sec)\n",
    "# grid_analysis.plot_heatmap(flat_timeseries_smoothed, z_labels, fps=1/0.533, output_path=os.path.join(PTH, 'grid_heatmap.png'), data_start_time=start_time_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb67c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_timeseries_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0032469",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(flat_timeseries_smoothed.shape[0])/fps,flat_timeseries_smoothed[:,20:27])\n",
    "# plt.ylim(0.03,0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(z_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_timeseries_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = importlib.reload(sys.modules['grid_analysis'])\n",
    "# sort the grid squares by their mean signal across time, and plot the heatmap again with the grid squares in sorted order\n",
    "mean_signal_grid = flat_timeseries_smoothed.mean(axis=0)\n",
    "sorted_indices = np.argsort(mean_signal_grid)\n",
    "flat_timeseries_smoothed_sorted = flat_timeseries_smoothed[:, sorted_indices]\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# smooth some more\n",
    "from grid_analysis import causal_smooth\n",
    "toplot = causal_smooth(flat_timeseries_smoothed_sorted, sigma=1.0)\n",
    "grid_analysis.plot_heatmap(toplot, z_labels, fps=1/0.533, output_path=os.path.join(PTH, 'grid_heatmap_sorted.png'), data_start_time=start_time_sec)\n",
    "# plt.xlim(-50,125)\n",
    "# plt.ylim(50,770)\n",
    "\n",
    "# plot the mean signal across all grid squares for each z-slice over time, and mark the time of food addition\n",
    "mean_signal_grid = flat_timeseries_smoothed.mean(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(flat_timeseries_smoothed.shape[0])/fps - start_time_sec, mean_signal_grid)\n",
    "plt.axvline(0, color='red', linestyle='--', label='Food addition')\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Mean $F/F_{\\\\mathrm{baseline}}$ across grid squares')\n",
    "plt.legend()\n",
    "# plt.xlim(-50,125)\n",
    "# plt.ylim(0.03,0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_timeseries_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca on flat_timeseries_smoothed, which of size (time,features)\n",
    "from sklearn.decomposition import PCA\n",
    "import utils\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(flat_timeseries_smoothed)\n",
    "# get scores\n",
    "pc_scores = pca.transform(flat_timeseries_smoothed)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance\n",
    "\n",
    "\n",
    "line_colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "ylim = (-12,14)\n",
    "for i in range(5):\n",
    "    fig, ax = utils.pretty_plot()\n",
    "    ax.plot(np.arange(pc_scores.shape[0])/fps, pc_scores[:,i], label=f'PC{i+1}', lw=3, color=line_colors[i])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel('Time (sec)')\n",
    "    plt.ylabel(f'PC{i+1} projection')\n",
    "    plt.show()\n",
    "\n",
    "# scree plot\n",
    "fig, ax = utils.pretty_plot()\n",
    "ax.bar(np.arange(len(explained_variance))+1, explained_variance*100)\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('Variance explained (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster flat_timeseries_smoothed, then plot heatmap with clusters indicated\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(flat_timeseries_smoothed.T)\n",
    "cluster_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e034a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sorted_timeseries)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort flat_timeseries_smoothed by cluster labels\n",
    "sorted_indices = np.argsort(cluster_labels)\n",
    "sorted_timeseries = flat_timeseries_smoothed[:, sorted_indices]\n",
    "\n",
    "plt, ax = utils.pretty_plot(figsize=(10,6))\n",
    "im = ax.pcolormesh(sorted_timeseries.T, cmap='plasma')\n",
    "plt.show()\n",
    "\n",
    "# plot mean of each cluster over time on the same axis\n",
    "i = 0\n",
    "for c in range(n_clusters-1):\n",
    "    cluster_mean = flat_timeseries_smoothed[:, cluster_labels == c].mean(axis=1)\n",
    "    # normalize between 0 and 1\n",
    "    cluster_mean = (cluster_mean - cluster_mean.min()) / (cluster_mean.max() - cluster_mean.min())\n",
    "    # smooth cluste means\n",
    "    # cluster_mean = pca_analysis.causal_smooth(cluster_mean[:,np.newaxis], sigma=3.0)\n",
    "    if i==0:\n",
    "        fig, ax = utils.pretty_plot()\n",
    "    ax.plot(np.arange(cluster_mean.shape[0])/fps, cluster_mean, lw=1)\n",
    "    ax.set_xlabel('Time (sec)')\n",
    "    # plt.ylabel(f'Cluster {c+1} mean $F/F_{{\\\\mathrm{{baseline}}}}$')\n",
    "    i += 1\n",
    "plt.show()\n",
    "\n",
    "# cross correlation between cluster 1 and 2 means signals\n",
    "from scipy.signal import correlate\n",
    "cluster1_mean = flat_timeseries_smoothed[:, cluster_labels == 0].mean(axis=1)\n",
    "cluster2_mean = flat_timeseries_smoothed[:, cluster_labels == 1].mean(axis=1)\n",
    "# normalize means bwetween 0 and 1\n",
    "cluster1_mean = (cluster1_mean - cluster1_mean.min()) / (cluster1_mean.max() - cluster1_mean.min())\n",
    "cluster2_mean = (cluster2_mean - cluster2_mean.min()) / (cluster2_mean.max() - cluster2_mean.min())\n",
    "# compute cross-correlation\n",
    "corr = correlate(cluster1_mean - cluster1_mean.mean(), cluster2_mean - cluster2_mean.mean(), mode='full')\n",
    "lags = np.arange(-len(cluster1_mean)+1, len(cluster1_mean))\n",
    "# plot xcorr\n",
    "fig, ax = utils.pretty_plot()\n",
    "ax.plot(lags/fps, corr)\n",
    "ax.set_xlabel('Lag (sec)')\n",
    "ax.set_ylabel('Cross-correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab335d",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199062da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pca_analysis\n",
    "import sys\n",
    "_ = importlib.reload(sys.modules['pca_analysis'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "\n",
    "# Arguments: input_dir, fps, bin_factor, z_start, z_end, lowpass_freq, n_components,\n",
    "#            n_pcs_plot, baseline_start, baseline_end, start_time, time_offset\n",
    "sys.argv = [\"\", PTH, 1/0.533, 1, 7, 18, 0, 50, 6, 5, 35, 0]\n",
    "pca_analysis.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c504364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6e2641",
   "metadata": {},
   "source": [
    "# DMD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dmd_analysis\n",
    "import sys\n",
    "_ = importlib.reload(sys.modules['dmd_analysis'])\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001'\n",
    "\n",
    "# Arguments: input_dir, fps, bin_factor, z_start, z_end, lowpass_freq, dmd_rank, \n",
    "#            n_modes_plot, baseline_start, baseline_end, start_time, time_offset\n",
    "sys.argv = [\"\", PTH, 1/0.533, 1, 7, 18, 0, 5, 6, 5, 35, 0]\n",
    "\n",
    "dmd_analysis.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5ht-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
