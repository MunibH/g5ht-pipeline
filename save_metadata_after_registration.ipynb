{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78db98c1",
   "metadata": {},
   "source": [
    "`conda activate imgpro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b357ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import utils\n",
    "utils.default_plt_params()\n",
    "\n",
    "import grid_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143d450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (200, 500)\n",
      "Dimensions of registered tifs: Z=39, C=2, H=200, W=500\n"
     ]
    }
   ],
   "source": [
    "DATA_PTH = r'D:\\DATA'\n",
    "PROJECT = 'g5ht-free' # g5ht-free or g5ht-immo\n",
    "OUT_PTH = 'date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004'\n",
    "reg_dir = 'registered_elastix'\n",
    "# reg_dir = 'registered_wholistic_smooth-0.200_patch-7' # for 20251223 worm005\n",
    "\n",
    "DATE = OUT_PTH.split('_')[0].split('-')[1] \n",
    "DATA_PTH = os.path.join(DATA_PTH, PROJECT, DATE)\n",
    "pth = os.path.join(DATA_PTH, OUT_PTH)\n",
    "\n",
    "reg_dir = os.path.join(pth, reg_dir)\n",
    "reg_tifs = [f for f in os.listdir(reg_dir) if f.endswith('.tif')]\n",
    "reg_tifs.sort() # ensure correct order\n",
    "reg_tifs_pths = [os.path.join(reg_dir, f) for f in reg_tifs]\n",
    "\n",
    "# load mask\n",
    "mask_fn = glob.glob(os.path.join(pth, 'fixed_mask_[0-9][0-9][0-9][0-9]*.tif'))[0]\n",
    "mask = tifffile.TiffFile(os.path.join(pth, mask_fn)).asarray()\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# load first frame to get dimensions\n",
    "with tifffile.TiffFile(reg_tifs_pths[0]) as tif:\n",
    "    Z,C,H,W = tif.asarray().shape\n",
    "print(f\"Dimensions of registered tifs: Z={Z}, C={C}, H={H}, W={W}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd8d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = 20\n",
    "yspace = 50\n",
    "\n",
    "grid_info, n_grids_y, n_grids_x = grid_analysis.create_grid(H,W,xspace,yspace) # (grid_info = (grid_id, y_start, y_end, x_start, x_end))\n",
    "grid_intensity = np.full((len(reg_tifs_pths), n_grids_y, n_grids_x), np.nan) # shape (frames, n_grids_y, n_grids_x)\n",
    "frame_indices = np.array([os.path.basename(p).split('.')[0] for p in reg_tifs_pths], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3ec7fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1108it [00:48, 22.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# grid up the volume, get mean across each grid square and , and plot the resulting grid\n",
    "\n",
    "xspace = 20\n",
    "yspace = 50\n",
    "\n",
    "grid_info, n_grids_y, n_grids_x = grid_analysis.create_grid(H,W,xspace,yspace) # (grid_info = (grid_id, y_start, y_end, x_start, x_end))\n",
    "grid_intensity = np.full((len(reg_tifs_pths), n_grids_y, n_grids_x), np.nan) # shape (frames, n_grids_y, n_grids_x)\n",
    "frame_indices = np.array([os.path.basename(p).split('.')[0] for p in reg_tifs_pths], dtype=int)\n",
    "\n",
    "for iframe,reg_tif_pth in tqdm(enumerate(reg_tifs_pths)):\n",
    "    with tifffile.TiffFile(reg_tif_pth) as tif:\n",
    "        frame = tif.asarray() # shape (C,Z,H,W)\n",
    "\n",
    "    # calculate ratiometric intensity for each grid square\n",
    "    for iy in range(n_grids_y):\n",
    "        for ix in range(n_grids_x):\n",
    "            y_start, y_end, x_start, x_end = grid_info[iy * n_grids_x + ix][1:] # get grid coordinates\n",
    "            \n",
    "            grid_mask = mask[y_start:y_end, x_start:x_end]\n",
    "            grid_data = frame[:, :, y_start:y_end, x_start:x_end]\n",
    "            grid_means = np.mean(grid_data, axis=(2,3)) # mean across Z,H,W dimensions (shape (C,Z))\n",
    "            grid_max = np.max(grid_means, axis=0) # max across Z dimension (shape (C,))\n",
    "            if grid_max[1] == 0: # avoid division by zero\n",
    "                grid_ratio = np.nan\n",
    "            else:\n",
    "                grid_ratio = grid_max[0] / grid_max[1] # ratiometric gfp/rfp\n",
    "            grid_intensity[iframe, iy, ix] = grid_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "787d1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_intensity_flat = grid_intensity.reshape(len(reg_tifs_pths), -1) # shape (frames, n_grids_y*n_grids_x)\n",
    "\n",
    "baseline_window = (201, 240) # frames, this is your intial guess, adjust based on plots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c549efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.close('all')\n",
    "\n",
    "fig, ax = utils.pretty_plot(figsize=(10,6))\n",
    "ax.plot(grid_intensity_flat, lw=0.1)\n",
    "ax.plot(np.nanmean(grid_intensity_flat, axis=1), color='k', lw=2)\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Mean GFP/RFP Ratio')\n",
    "plt.show()\n",
    "\n",
    "# plot as heatmap\n",
    "fig, ax = utils.pretty_plot(figsize=(10,6))\n",
    "im = ax.pcolormesh(grid_intensity_flat.T, shading='auto', cmap='viridis', vmin=np.nanpercentile(grid_intensity_flat, 0), vmax=np.nanpercentile(grid_intensity_flat, 90))\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Grid Square')\n",
    "fig.colorbar(im, ax=ax, label='Mean GFP/RFP Ratio')\n",
    "plt.show()\n",
    "\n",
    "# same plots but intensity divided by baseline window\n",
    "baseline_mean = np.nanmean(grid_intensity_flat[baseline_window[0]:baseline_window[1], :], axis=0) # shape (n_grids_y*n_grids_x,)\n",
    "grid_intensity_normalized = grid_intensity_flat / baseline_mean # shape (frames, n_grids_y*n_grids_x)\n",
    "\n",
    "# # calculate baseline as F/F20, where F20 is the 20th percentile of each grid square across the entire time series\n",
    "# baseline_f20 = np.nanpercentile(grid_intensity_flat, 20, axis=0) # shape (n_grids_y*n_grids_x,)\n",
    "# grid_intensity_normalized = grid_intensity_flat / baseline_f20 # shape (frames, n_grids_y*n_grids_x\n",
    "\n",
    "fig, ax = utils.pretty_plot(figsize=(10,6))\n",
    "ax.plot(grid_intensity_normalized, lw=0.1)\n",
    "ax.plot(np.nanmean(grid_intensity_normalized, axis=1), color='k', lw=2)\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Mean GFP/RFP Ratio')\n",
    "plt.show()\n",
    "\n",
    "# plot as heatmap\n",
    "fig, ax = utils.pretty_plot(figsize=(10,6))\n",
    "im = ax.pcolormesh(grid_intensity_normalized.T, shading='auto', cmap='viridis', vmin=np.nanpercentile(grid_intensity_normalized, 0), vmax=np.nanpercentile(grid_intensity_normalized, 99))\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Grid Square')\n",
    "fig.colorbar(im, ax=ax, label='Mean GFP/RFP Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f61862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to D:\\DATA\\g5ht-free\\20260123\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# metadata\n",
    "# NOTE: all frames are indexed from 0 to nframes-1, so frame_index[0] corresponds to frame 0, frame_index[1] to frame 1, etc. \n",
    "# This means that if you want to specify bad_frames, baseline_start_frame, baseline_end_frame, or encounter_frame, \n",
    "# you should use the indices corresponding to the frames in the registered tifs, \n",
    "# not the absolute frame numbers from the original acquisition (unless they happen to be the same). \n",
    "# For example, if your first registered tif corresponds to frame 100 in the original acquisition, \n",
    "# then frame_index[0] would be 100, and if you want to mark that as a bad frame, you would include 0 in bad_frames, not 100.\n",
    "fps = (1/0.533) # Hz\n",
    "nframes = len(reg_tifs_pths)\n",
    "frame_index = frame_indices\n",
    "baseline_start_frame = 490 # int, set to None if you don't have a clear baseline period, or if you want to use the entire time series to calculate baseline statistics like F20\n",
    "baseline_end_frame = 540 # int, set to None if you don't have a clear baseline period, or if you want to use the entire time series to calculate baseline statistics like F20\n",
    "encounter_frame = 675 # int, set to None if no encounter\n",
    "# bad_frames = np.array([])\n",
    "bad_frames = np.arange(0, 487)\n",
    "# bad_frames1 = np.arange(0,200)\n",
    "# bad_frames2 = np.arange(339,356)\n",
    "# bad_frames = np.concatenate([bad_frames1, bad_frames2]) \n",
    "# bad_frames frame indices to exclude (indexed into range(0, nframes))\n",
    "# # bad_frames are in range(0,nframes), not absolute frame numbers, so they can be used to index into range(nframes) or frame_index\n",
    "\n",
    "metadata = {\n",
    "    'fps': fps,\n",
    "    'nframes': nframes,\n",
    "    'baseline_start_frame': baseline_start_frame,\n",
    "    'baseline_end_frame': baseline_end_frame,\n",
    "    'encounter_frame': encounter_frame,\n",
    "    'bad_frames': bad_frames.tolist(), # convert numpy array to list for JSON\n",
    "    'frame_index': frame_index.tolist(),  # convert numpy array to list for JSON\n",
    "}\n",
    "\n",
    "meta_pth = os.path.join(pth, 'metadata.json')\n",
    "with open(meta_pth, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"Metadata saved to {meta_pth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading metadata in a subsequent analysis ---\n",
    "with open(meta_pth, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "metadata['bad_frames'] = np.array(metadata['bad_frames'])  # convert back to numpy array\n",
    "metadata['frame_index'] = np.array(metadata['frame_index'])  # convert back to numpy array\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d9cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\DATA\\g5ht-free\\022025_eft_41z_starved_worm002\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\022025_eft_41z_starved_worm002\\\\022025_eft_41z_starved_worm002']\n",
      "D:\\DATA\\g5ht-free\\20251223\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20251223\\\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm005', 'D:\\\\DATA\\\\g5ht-free\\\\20251223\\\\date-20251223_strain-ISg5HT_condition-fedpatch_worm001', 'D:\\\\DATA\\\\g5ht-free\\\\20251223\\\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm004']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20251223\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm005\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20251223_strain-ISg5HT_condition-starvedpatch_worm005.json\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20251223\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm004\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20251223_strain-ISg5HT_condition-starvedpatch_worm004.json\n",
      "D:\\DATA\\g5ht-free\\20260123\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm005', 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260123\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm005\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm005.json\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260123\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004.json\n",
      "D:\\DATA\\g5ht-free\\20251028\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20251028\\\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001', 'D:\\\\DATA\\\\g5ht-free\\\\20251028\\\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.json\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002.json\n",
      "D:\\DATA\\g5ht-free\\20260220\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20260220\\\\date-20260220_strain-ISg5HT-NSM-TeTx_condition-starvedpatch_worm008']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260220\\date-20260220_strain-ISg5HT-NSM-TeTx_condition-starvedpatch_worm008\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260220_strain-ISg5HT-NSM-TeTx_condition-starvedpatch_worm008.json\n",
      "D:\\DATA\\g5ht-free\\20260113\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20260113\\\\date-20260113_strain-ISg5HT-ADF-TeTx_condition-starvedpatch_worm003', 'D:\\\\DATA\\\\g5ht-free\\\\20260113\\\\date-20260113_strain-ISg5HT-ADF-TeTx_condition-starvedpatch_worm004']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260113\\date-20260113_strain-ISg5HT-ADF-TeTx_condition-starvedpatch_worm003\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260113_strain-ISg5HT-ADF-TeTx_condition-starvedpatch_worm003.json\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260113\\date-20260113_strain-ISg5HT-ADF-TeTx_condition-starvedpatch_worm004\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260113_strain-ISg5HT-ADF-TeTx_condition-starvedpatch_worm004.json\n",
      "D:\\DATA\\g5ht-free\\20260206\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20260206\\\\date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm001', 'D:\\\\DATA\\\\g5ht-free\\\\20260206\\\\date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260206\\date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm001\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm001.json\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260206\\date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003.json\n",
      "D:\\DATA\\g5ht-free\\20251121\n",
      "[]\n",
      "D:\\DATA\\g5ht-free\\20251106\n",
      "[]\n",
      "D:\\DATA\\g5ht-free\\20260114\n",
      "[]\n",
      "D:\\DATA\\g5ht-free\\20251112\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20251112\\\\date-20251112_strain-ISg5HT_condition-fedpatch_worm002']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20251112\\date-20251112_strain-ISg5HT_condition-fedpatch_worm002\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20251112_strain-ISg5HT_condition-fedpatch_worm002.json\n",
      "D:\\DATA\\g5ht-free\\20260217\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20260217\\\\date-20260217_strain-ISg5HT-nsIS180_condition-fedpatch_worm003']\n",
      "Copied metadata from D:\\DATA\\g5ht-free\\20260217\\date-20260217_strain-ISg5HT-nsIS180_condition-fedpatch_worm003\\metadata.json to D:\\DATA\\g5ht-free-metadata\\metadata_date-20260217_strain-ISg5HT-nsIS180_condition-fedpatch_worm003.json\n",
      "D:\\DATA\\g5ht-free\\20251107\n",
      "['D:\\\\DATA\\\\g5ht-free\\\\20251107\\\\date-20251107_strain-ISg5HT_condition-fedpatch_worm003']\n"
     ]
    }
   ],
   "source": [
    "# copy all metadata files to a new directory\n",
    "# when copying, rename them from metadata.json to metadata_{OUT_PTH}.json\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "dest_folder = r'D:\\DATA\\g5ht-free-metadata'\n",
    "\n",
    "# loop over all recordings\n",
    "DATA_PTH = r'D:\\DATA\\g5ht-free'\n",
    "worm_tuple = ('worm001', 'worm002', 'worm003', 'worm004', 'worm005', 'worm006', 'worm007', 'worm008', 'worm009', 'worm010', 'worm011')\n",
    "\n",
    "date_pths = [os.path.join(DATA_PTH, d) for d in os.listdir(DATA_PTH) if os.path.isdir(os.path.join(DATA_PTH, d))]\n",
    "\n",
    "# in each folder in date_pths, look for folders that end with 'wormXXX', where XXX is a three digit number, and then look for the 'registered_elastix' folder inside that folder to create the mp4\n",
    "for date_pth in date_pths:\n",
    "    print(date_pth)\n",
    "    worm_pths = [os.path.join(date_pth, d) for d in os.listdir(date_pth) if os.path.isdir(os.path.join(date_pth, d)) and d.endswith(worm_tuple)]\n",
    "    print(worm_pths)\n",
    "    for worm_pth in worm_pths:\n",
    "        meta_pth = os.path.join(worm_pth, 'metadata.json')\n",
    "        if os.path.exists(meta_pth):\n",
    "            with open(meta_pth, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            out_meta_pth = os.path.join(dest_folder, f'metadata_{os.path.basename(worm_pth)}.json')\n",
    "            with open(out_meta_pth, 'w') as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "            print(f\"Copied metadata from {meta_pth} to {out_meta_pth}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
