{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1b7a0b",
   "metadata": {},
   "source": [
    "# G5HT-PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fee86",
   "metadata": {},
   "source": [
    "first transfers data from flvc to local directory, then moves results back to flvc\n",
    "\n",
    "The transferring code assumes you have set up an ssh key\n",
    "\n",
    "__set up ssh key__\n",
    "\n",
    "https://github.com/flavell-lab/FlavellLabWiki/wiki/Setting-up-SSH-key-on-Windows\n",
    "- generate ssh key\n",
    "- add to github account\n",
    "- transfer authorized keys to flvc\n",
    "  - type $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh munib@flv-c3 \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"\n",
    "- done, should be able to ssh into flvc (ssh flvc or ssh munib@flv-c3) without entering password"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53048b",
   "metadata": {},
   "source": [
    "for transferring back, probably better to use rsync in msys2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c1c6a",
   "metadata": {},
   "source": [
    "## CONDA ENVIRONMENTS\n",
    "\n",
    "For steps __transfer files__, __1. preprocess__ and __2. mip__, `conda activate g5ht-pipeline`\n",
    "\n",
    "For step __3. segment__, `conda activate segment-torch` or `conda activate torchcu129`\n",
    "\n",
    "For step __4. spline, 5. orient, 6. warp, 7. reg__, `conda activate g5ht-pipeline`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e087d8",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21220aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    is_torch_env = False\n",
    "except ImportError:\n",
    "    is_torch_env = True\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7534ad",
   "metadata": {},
   "source": [
    "## TRANSFER FILES\n",
    "\n",
    "Takes about 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e36d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flvc = 'munib@flv-c3' # this is the username and hostname of the linux machine\n",
    "data_dir_flvc = r'/home/munib/store1/shared/g5ht/data' # this is a linux machine\n",
    "data_dir_local = Path(r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free') # this is a windows machine\n",
    "\n",
    "# dataset (see datasets.txt)\n",
    "dataset = 'date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc380980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh munib@flv-c3 \"test -e /home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2\"\n",
      "Transferring date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2 to C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028...\n",
      "scp \"munib@flv-c3:/home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2\" \"C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028\"\n",
      "ssh munib@flv-c3 \"test -e /home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.h5\"\n",
      "Transferring date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.h5 to C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028...\n",
      "scp \"munib@flv-c3:/home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.h5\" \"C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028\"\n",
      "ssh munib@flv-c3 \"test -e /home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001_chan_alignment.nd2\"\n",
      "Remote file not found or error: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use subprocess and scp to copy the file from the linux machine to the windows machine\n",
    "# additionally will transfer a file with same name but with .h5 extension and a file with same name, but with ...wormXXX _chan_alignment.nd2 as filename\n",
    "\n",
    "# def scp_from_flvc(filename, data_dir_flvc, data_dir_local, flvc):\n",
    "    \n",
    "#     # 1. Handle Local Path (Standard Windows style)\n",
    "#     date_str = filename.split('_')[0].split('-')[1]\n",
    "#     local_dir = data_dir_local / date_str\n",
    "#     local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # 2. Handle Remote Path (Force Linux/Posix style)\n",
    "#     remote_path = PurePosixPath(data_dir_flvc) / date_str / filename\n",
    "\n",
    "#     # 3. Check if remote file exists\n",
    "#     ssh_command = f'ssh {flvc} \"test -e {remote_path}\"'\n",
    "#     print(ssh_command)\n",
    "    \n",
    "#     result = subprocess.run(ssh_command, shell=True, capture_output=True, text=True)\n",
    "#     if result.returncode != 0:\n",
    "#         print(f\"Remote file not found or error: {result.stderr}\")\n",
    "#         return\n",
    "\n",
    "#     # 4. Transfer using scp (available on Windows via OpenSSH)\n",
    "#     print(f\"Transferring {filename} to {local_dir}...\")\n",
    "#     scp_command = f'scp \"{flvc}:{remote_path}\" \"{local_dir}\"'\n",
    "#     print(scp_command)\n",
    "#     subprocess.run(scp_command, shell=True, check=True)\n",
    "\n",
    "# transfer the files\n",
    "utils.scp_from_flvc(dataset, data_dir_flvc, data_dir_local, flvc) # ~10 mins from flvc to local windows\n",
    "utils.scp_from_flvc(os.path.splitext(dataset)[0] + '.h5', data_dir_flvc, data_dir_local, flvc) # ~ 3 mins\n",
    "utils.scp_from_flvc(os.path.splitext(dataset)[0] + '_chan_alignment.nd2', data_dir_flvc, data_dir_local, flvc) # ~ 3 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a415d0",
   "metadata": {},
   "source": [
    "## SPECIFY DATA TO PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcf1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2\n",
      "Num z-slices:  41\n",
      "Number of frames:  1200\n",
      "Height:  512\n",
      "width:  512\n",
      "Number of channels:  2\n",
      "Beads alignment file:  None\n"
     ]
    }
   ],
   "source": [
    "INPUT_ND2 = dataset\n",
    "date_str = INPUT_ND2.split('_')[0].split('-')[1]\n",
    "local_dir = data_dir_local / date_str\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_ND2_PTH = os.path.join(local_dir, INPUT_ND2)\n",
    "\n",
    "NOISE_PTH = r'C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\noise\\noise_111125.tif'\n",
    "\n",
    "OUT_DIR = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "STACK_LENGTH = 41 if 'immo' not in INPUT_ND2 else 122\n",
    "\n",
    "# for recordings prior to roughly December 2025, we want to keep all but the last two z-slices, during which the piezo position is unstable\n",
    "# after December 2025, we want to keep all but the first two z-slices, during which the piezo position is unstable at the beginning of the recording\n",
    "date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "if date_obj < datetime(2025, 12, 1):\n",
    "    z2keep =  (0,STACK_LENGTH-2) # tuple representing range of z-slices to keep, should keep all but the last two slices\n",
    "else:\n",
    "    z2keep =  (2,STACK_LENGTH) # tuple representing range of z-slices to keep, should keep all but the first two slices\n",
    "\n",
    "if not is_torch_env:\n",
    "    noise_stack = utils.get_noise_stack(NOISE_PTH, STACK_LENGTH)\n",
    "    num_frames, height, width, num_channels = utils.get_range_from_nd2(INPUT_ND2_PTH, stack_length=STACK_LENGTH) \n",
    "    beads_alignment_file = utils.get_beads_alignment_file(INPUT_ND2_PTH)\n",
    "else:\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")\n",
    "\n",
    "print(INPUT_ND2)\n",
    "print('Num z-slices: ', STACK_LENGTH)\n",
    "if not is_torch_env:\n",
    "    print('Number of frames: ', num_frames)\n",
    "    print('Height: ', height)\n",
    "    print('width: ', width)\n",
    "    print('Number of channels: ', num_channels)\n",
    "    print('Beads alignment file: ', beads_alignment_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d28b53",
   "metadata": {},
   "source": [
    "## 1. SHEAR CORRECTION \n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "~ 1 hour for 1200 image stacks with 2 color channels, 41 z, 512 h, 512 w\n",
    "\n",
    "- shear corrects each volume\n",
    "  - depending on each exposure time, it can take roughly half a second between the first and last frames of a volume, so any movements need to be corrected for\n",
    "- creates one `.tif` for each volume and stores it in the `shear_corrected` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1141 stacks (59-1199) using 10 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 490/1141 [26:19<34:33,  3.19s/it]  "
     ]
    }
   ],
   "source": [
    "import shear_correct\n",
    "_ = importlib.reload(sys.modules['shear_correct'])\n",
    "\n",
    "start_index = \"0\"\n",
    "end_index = str(num_frames-1)\n",
    "# start_index = \"59\"\n",
    "# end_index = \"803\"\n",
    "cpu_count = str(int(os.cpu_count() / 2))\n",
    "# cpu_count = str(int(os.cpu_count()))\n",
    "skip_shear_correction = False # if True, will just denoise and save as tif\n",
    "\n",
    "# sys.argv = [\"\", nd2 file, start_frame, end_frame, noise_pth, stack_length, n_workers, num_frames, height, width, num_channels]\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, z2keep, skip_shear_correction]\n",
    "\n",
    "# Call the main function\n",
    "shear_correct.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357bd32",
   "metadata": {},
   "source": [
    "## 2. CHANNEL ALIGNMENT\n",
    "\n",
    "` conda activate g5ht-pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0617b",
   "metadata": {},
   "source": [
    "### 2a. GET MEDIAN CHANNEL ALIGNMENT PARAMETERS FROM ALL FRAMES\n",
    "\n",
    "- If channel alignment file found, uses that, if not uses worm recording\n",
    "- creates a `.txt` file for each volume that contains elastix channel registration parameters\n",
    "- creates `chan_align_params.csv` and  `chan_align.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beads_alignment_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87844a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_channel_alignment\n",
    "import median_channel_alignment\n",
    "_ = importlib.reload(sys.modules['get_channel_alignment'])\n",
    "_ = importlib.reload(sys.modules['median_channel_alignment'])\n",
    "\n",
    "## set beads_alignment_file to None to use worm recording for channel alignment, even if beads file exists\n",
    "# beads_alignment_file = None\n",
    "\n",
    "start_index = \"0\"\n",
    "cpu_count = str(int(os.cpu_count() / 2))\n",
    "# cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "if beads_alignment_file is not None:\n",
    "    align_with_beads = True\n",
    "    num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "    sys.argv = [\"\", beads_alignment_file, start_index, str(num_frames_beads-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames_beads, height, width, num_channels, align_with_beads]\n",
    "else:\n",
    "    align_with_beads = False\n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, str(num_frames-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads]\n",
    "\n",
    "# # Call the main function\n",
    "get_channel_alignment.main()\n",
    "median_channel_alignment.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe582c3e",
   "metadata": {},
   "source": [
    "### 2b. APPLY MEDIAN CHANNEL ALIGNMENT PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b97453",
   "metadata": {},
   "source": [
    "- ouputs aligned volumes in `channel_aligned` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apply_channel_alignment\n",
    "_ = importlib.reload(sys.modules['apply_channel_alignment'])\n",
    "\n",
    "start_index = \"0\"\n",
    "cpu_count = str(int(os.cpu_count() / 2))\n",
    "# cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "# 0786 to 0799 are bad frames in worm005.nd2, copied 0785 for each of those frames\n",
    "\n",
    "if beads_alignment_file is not None:\n",
    "    align_with_beads = True\n",
    "    num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, str(num_frames-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads, beads_alignment_file]\n",
    "else:\n",
    "    align_with_beads = False\n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, str(num_frames-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads]\n",
    "\n",
    "\n",
    "# Call the main function\n",
    "apply_channel_alignment.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc706e",
   "metadata": {},
   "source": [
    "### 2c. PLOT CHANNEL ALIGNMENT PARAMETER DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# make font sizes larger for visibility\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "try:\n",
    "    out_dir = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "    df = pd.read_csv(os.path.join(out_dir, 'chan_align_params.csv'))\n",
    "    params = ['TransformParameter_0', 'TransformParameter_1', 'TransformParameter_2', 'TransformParameter_3', 'TransformParameter_4', 'TransformParameter_5']\n",
    "    labels = ['Rx', 'Ry', 'Rz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "    # the xaxis limits for each subplot should be the same across figures\n",
    "\n",
    "    xlims = np.zeros((6,2))\n",
    "\n",
    "    plt.figure(figsize=(12,8), tight_layout=True)\n",
    "    for i,param in enumerate(params):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.hist(df[param], bins=30, color='red', alpha=0.6)\n",
    "        # plot the median value as a vertical line\n",
    "        median_value = df[param].median()\n",
    "        plt.axvline(median_value, color='black', linestyle='dashed', linewidth=2)\n",
    "        plt.xlabel(labels[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        # get xaxis limits\n",
    "        xlims[i,:] = plt.xlim()\n",
    "        # title is median value\n",
    "        plt.title(f'Median: {np.round(median_value,3)}', fontsize=14)\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"No chan_align_params.csv found for worm recording\")\n",
    "\n",
    "out_dir = os.path.splitext(INPUT_ND2_PTH)[0] + '_chan_alignment'\n",
    "df = pd.read_csv(os.path.join(out_dir, 'chan_align_params.csv'))\n",
    "params = ['TransformParameter_0', 'TransformParameter_1', 'TransformParameter_2', 'TransformParameter_3', 'TransformParameter_4', 'TransformParameter_5']\n",
    "labels = ['Rx', 'Ry', 'Rz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "plt.figure(figsize=(12,8), tight_layout=True)\n",
    "for i,param in enumerate(params):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist(df[param], bins=30, color='blue', alpha=0.6)\n",
    "    # plot the median value as a vertical line\n",
    "    median_value = df[param].median()\n",
    "    plt.axvline(median_value, color='black', linestyle='dashed', linewidth=2)\n",
    "    plt.xlabel(labels[i])\n",
    "    plt.ylabel('Frequency')\n",
    "    # apply xlims\n",
    "    # plt.xlim(xlims[i,0], xlims[i,1])\n",
    "    # title is median value, font size 14\n",
    "    plt.title(f'Median: {np.round(median_value,3)}', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f4640",
   "metadata": {},
   "source": [
    "## 3. BLEACH CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b283de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import bleach_correct\n",
    "_ = importlib.reload(sys.modules['bleach_correct'])\n",
    "\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "REG_DIR = 'channel_aligned' # 'channel_aligned' or 'tif' \n",
    "channels = 1\n",
    "method = 'block' # 'block' or 'exponential'\n",
    "mode = 'total' # 'total' or 'median'\n",
    "output_dir = os.path.join(PTH, 'bleach_corrected')\n",
    "\n",
    "bleach_correct.correct_bleaching(os.path.join(PTH,REG_DIR), output_dir=output_dir, channels=channels, method=method, fbc=0.04, intensity_mode=mode)\n",
    "\n",
    "\n",
    "# # Correct RFP only with block method (default)\n",
    "# correct_bleaching(\"path/to/data\")\n",
    "\n",
    "# # Correct both channels with exponential fit\n",
    "# correct_bleaching(\"path/to/data\", channels=[0, 1], method='exponential')\n",
    "\n",
    "# # Command line\n",
    "# python bleach_correct.py path/to/data --channels 0 1 --method exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a097705",
   "metadata": {},
   "source": [
    "## 4. MIP\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `means.png`, `focus.png`, `mip.tif`, and `mip.mp4`, `focus_check.csv`\n",
    "\n",
    "##### TODO: \n",
    "- legend for focus.png, should be frame#\n",
    "- mip for xy, xz, zy\n",
    "- mip for several slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mip\n",
    "\n",
    "_ = importlib.reload(sys.modules['mip'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "framerate = 8\n",
    "# tif_dir = 'bleach_corrected_RFP_block' # one of 'shear_corrected' 'channel_aligned' 'bleach_corrected_RFP_block'\n",
    "tif_dir = 'shear_corrected'\n",
    "rmax = 850\n",
    "gmax = 150\n",
    "mp4_quality = 10\n",
    "do_focus = True\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, tif_dir, STACK_LENGTH, num_frames, framerate, rmax, gmax, mp4_quality, do_focus]\n",
    "\n",
    "# Call the main function\n",
    "mip.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563861dd",
   "metadata": {},
   "source": [
    "## 5 DRIFT ESTIMATION\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs  `z_selection.csv`, `z_selection_diagnostics.png`, `sharpness.csv`\n",
    "\n",
    "TODO:\n",
    "- use z selection going forward\n",
    "- also use sharpness/focus (and other things) to determine good/bad frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drift_estimation\n",
    "\n",
    "_ = importlib.reload(sys.modules['drift_estimation'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "tif_dir = 'bleach_corrected_RFP_block' # one of 'shear_corrected' 'channel_aligned' 'bleach_corrected_RFP_block'\n",
    "\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, tif_dir, STACK_LENGTH, num_frames]\n",
    "\n",
    "# Call the main function\n",
    "drift_estimation.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6588494",
   "metadata": {},
   "source": [
    "## 5. SEGMENT\n",
    "\n",
    "- outputs `label.tif`, contains segmented MIP for each volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f60d1",
   "metadata": {},
   "source": [
    "__on home pc__: \n",
    "`conda activate segment-torch`\n",
    "\n",
    "Uses a separate conda environment from the rest of the pipeline. create it using:\n",
    "`conda env create -f segment_torch.yml`\n",
    "\n",
    "__on lab pc__: \n",
    "`conda activate torchcu129`\n",
    "\n",
    "Uses a separate conda environment from the rest of the pipeline. create it following steps in:\n",
    "`segment_torch_cu129_environment.yml`\n",
    "\n",
    "### setup each time model weights change\n",
    "Need to set path to model weights as `CHECKPOINT` in `eval_torch.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segment.segment_torch\n",
    "_ = importlib.reload(sys.modules['segment.segment_torch'])\n",
    "\n",
    "# mip_tif = 'mip_bleach_corrected_RFP_block'\n",
    "mip_tif = 'mip_channel_aligned' \n",
    "\n",
    "MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], f'{mip_tif}.tif')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", MIP_PTH]\n",
    "\n",
    "segment.segment_torch.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989c725",
   "metadata": {},
   "source": [
    "## 6. SPLINE\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `spline.json`, `spline.tif`, and `dilated.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac034f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spline\n",
    "_ = importlib.reload(sys.modules['spline'])\n",
    "\n",
    "LABEL_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'label.tif')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", LABEL_PTH]\n",
    "\n",
    "spline.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb7360",
   "metadata": {},
   "source": [
    "## 7. ORIENT\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `oriented.json`, `oriented.png`, `oriented_stack.tif`\n",
    "\n",
    "NOTE: `orient_v2.py` automated the process of finding orientation completely, whereas `orient.py` requires you to input the (x,y) nose location on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orient\n",
    "_ = importlib.reload(sys.modules['orient'])\n",
    "\n",
    "SPLINE_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'spline.json')\n",
    "nose_y = 250\n",
    "nose_x = 45\n",
    "\n",
    "# apply constraints\n",
    "# might need this when there are frames where the spline fitting fails and orientation is lost intermittently\n",
    "constrain_frame = 515\n",
    "constrain_frame_nose_y = 288\n",
    "constrain_frame_nose_x = 180\n",
    "\n",
    "# command-line arguments\n",
    "# sys.argv = [\"\", SPLINE_PTH, str(nose_y), str(nose_x)]\n",
    "sys.argv = [\"\", SPLINE_PTH, str(nose_y), str(nose_x), str(constrain_frame), str(constrain_frame_nose_y), str(constrain_frame_nose_x)]\n",
    "\n",
    "orient.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orient_v2 # tried to automate finding nose point, not working well at the moment\n",
    "_ = importlib.reload(sys.modules['orient_v2'])\n",
    "\n",
    "SPLINE_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'spline.json')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", SPLINE_PTH]\n",
    "\n",
    "orient_v2.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae94ec6",
   "metadata": {},
   "source": [
    "## 8. WARP\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- ouputs: `warped/*.tif` and `masks/*.tif`\n",
    "\n",
    "TODO: parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1eac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warp\n",
    "_ = importlib.reload(sys.modules['warp'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "start_index = 516\n",
    "end_index = num_frames\n",
    "\n",
    "for i in tqdm(range(start_index, end_index)):\n",
    "    # command-line arguments\n",
    "    sys.argv = [\"\", PTH, i]\n",
    "\n",
    "    warp.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ddcbc",
   "metadata": {},
   "source": [
    "## 9. REGISTER\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "__ALTERNATIVELY__: register using the wholistic registration algorithm, currently in MATLAB\n",
    "\n",
    "TODO: parallelize / make faster\n",
    "\n",
    "- pick a good representative fixed frame that you want to register everything to\n",
    "  - copy it to the main output folder and name it `fixed_xxxx.tif`\n",
    "  - copy the corresponding mask and name it `fixed_mask_xxxx.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b722e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reg\n",
    "_ = importlib.reload(sys.modules['reg'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "start_index = 222\n",
    "end_index = num_frames\n",
    "zoom = 1 # albert was using 3\n",
    "# zoom = 3\n",
    "\n",
    "for i in tqdm(range(start_index, end_index)):\n",
    "    # command-line arguments\n",
    "    try:\n",
    "        sys.argv = [\"\", PTH, i, str(zoom)]\n",
    "        reg.main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {i}: {e}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd56099",
   "metadata": {},
   "source": [
    "### REGISTER WITH GFP+1 TO RFP\n",
    "\n",
    "TRIM LAST RFP ZSLICE, TRIM FIRST GFP ZSLICE\n",
    "\n",
    "seems to be that as of 20251204, all recordings were taken such that the i zslice in red channel corresponds to i+1 zslice in green channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "from reg_gfp_indexing import main as reg_worm\n",
    "\n",
    "PTH = r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001_aligned'\n",
    "\n",
    "for i in tqdm(range(1200)):\n",
    "    # command-line arguments\n",
    "    sys.argv = [\"\", PTH, i, \"1\"]\n",
    "    reg_worm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183d57b",
   "metadata": {},
   "source": [
    "### MAKE MOVIES OF REGISTERED DATA (see `reg_microfilm.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f91372",
   "metadata": {},
   "source": [
    "### REGISTER SINGLE FRAMES WITH ERROR LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "import itk\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from itk import image_view_from_array\n",
    "\n",
    "#get channels out of stacks\n",
    "def register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack):\n",
    "\tfixed_rfp = fixed_stack[:, 1].copy()\n",
    "\tmoving_gfp, moving_rfp = moving_stack[:, 0].copy(), moving_stack[:, 1].copy()\n",
    "\n",
    "\t#initialize registration parameters\n",
    "\tparameter_object = itk.ParameterObject.New()\n",
    "\tdefault_rigid_parameter_map = parameter_object.GetDefaultParameterMap('rigid', 4)\n",
    "\tparameter_object.AddParameterMap(default_rigid_parameter_map)\n",
    "\tdefault_affine_parameter_map = parameter_object.GetDefaultParameterMap('affine', 4)\n",
    "\tparameter_object.AddParameterMap(default_affine_parameter_map)\n",
    "\tdefault_bspline_128_parameter_map = parameter_object.GetDefaultParameterMap('bspline', 4, 128)\n",
    "\tparameter_object.AddParameterMap(default_bspline_128_parameter_map)\n",
    "\tdefault_bspline_64_parameter_map = parameter_object.GetDefaultParameterMap('bspline', 4, 64)\n",
    "\tparameter_object.AddParameterMap(default_bspline_64_parameter_map)\n",
    "\tdefault_bspline_32_parameter_map = parameter_object.GetDefaultParameterMap('bspline', 4, 32)\n",
    "\tparameter_object.AddParameterMap(default_bspline_32_parameter_map)\n",
    "\n",
    "\t#convert to itk images\n",
    "\tfixed_rfp = itk.image_view_from_array(fixed_rfp.astype(np.float32))\n",
    "\tmoving_rfp = itk.image_view_from_array(moving_rfp.astype(np.float32))\n",
    "\n",
    "\tfixed_mask_stack = itk.image_view_from_array(fixed_mask_stack.astype(np.ubyte))\n",
    "\tmoving_mask_stack = itk.image_view_from_array(moving_mask_stack.astype(np.ubyte))\n",
    "\n",
    "\t#register rfp first and then apply transform to gfp\n",
    "\n",
    "\tregistered_rfp, transform_parameters = itk.elastix_registration_method(fixed_rfp, moving_rfp, parameter_object,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   fixed_mask=fixed_mask_stack, moving_mask=moving_mask_stack,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   log_to_console=True)\n",
    "\tregistered_gfp = itk.transformix_filter(moving_gfp, transform_parameters)\n",
    "\n",
    "\t#initialize and fill output\n",
    "\toutput_stack = np.zeros((fixed_stack.shape[0], 2, 200, 500), np.uint16)\n",
    "\toutput_stack[:, 0] = np.clip(registered_gfp, 0, 4095)\n",
    "\toutput_stack[:, 1] = np.clip(registered_rfp, 0, 4095)\n",
    "\n",
    "\treturn output_stack\n",
    "    \n",
    "\t# # enablle elastic error logging\n",
    "\t# elastix_filter = itk.ElastixRegistrationMethod.New(fixed_rfp, moving_rfp, parameter_object,\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   fixed_mask=fixed_mask_stack, moving_mask=moving_mask_stack,\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   log_to_console=True) \n",
    "    \n",
    "\t# elastix_filter.SetParameterObject(parameter_object)\n",
    "\t# elastix_filter.SetNumberOfThreads(8)\n",
    "\t# elastix_filter.LogToConsoleOn()  # Enable console logging\n",
    "\t# elastix_filter.LogToFileOn()\n",
    "\t# elastix_filter.SetOutputDirectory(r\"C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\logs\")\n",
    "\t# elastix_filter.Update()\n",
    "\n",
    "\t# return elastix_filter.GetOutput(), elastix_filter.GetTransformParameterObject()\n",
    "\n",
    "input_dir = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "warped_path = os.path.join(input_dir, 'warped')\n",
    "output_path = os.path.join(input_dir, 'registered_fixed_sweep')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "fixed_list = np.arange(0, 400, 50)\n",
    "mov_list = np.arange(0, 400, 30)\n",
    "\n",
    "for fixed in fixed_list:\n",
    "    for mov in mov_list:\n",
    "        print(f'Processing fixed: {fixed}, moving: {mov}')\n",
    "        \n",
    "\t\t# load stacks\n",
    "\t\n",
    "        moving_path = os.path.join(warped_path,f'{mov:04d}.tif')\n",
    "        moving_stack = tifffile.imread(moving_path).astype(np.float32)\n",
    "        fixed_path = os.path.join(warped_path,f'{fixed:04d}.tif')\n",
    "        fixed_stack = tifffile.imread(fixed_path).astype(np.float32)\n",
    "        fixed_mask_path = os.path.join(input_dir, 'masks', f'{fixed:04d}.tif')\n",
    "        fixed_mask = tifffile.imread(fixed_mask_path)\n",
    "        fixed_mask_stack = np.stack([fixed_mask] * fixed_stack.shape[0])\n",
    "\n",
    "        moving_mask_path = os.path.join(input_dir, 'masks', f'{mov:04d}.tif')\n",
    "        moving_mask = tifffile.imread(moving_mask_path)\n",
    "        moving_mask_stack = np.stack([moving_mask] * fixed_stack.shape[0])\n",
    "\n",
    "\n",
    "        output_stack = register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack)\n",
    "        # save output stack with fixed and moving indices in filename\n",
    "        output_file_path = os.path.join(output_path, f'fixed_{fixed:04d}_mov_{mov:04d}.tif')\n",
    "        tifffile.imwrite(output_file_path, output_stack, imagej=True)\n",
    "\n",
    "# fixed = 100\n",
    "# mov = 200\n",
    "\n",
    "# # load stacks\n",
    "# moving_path = os.path.join(warped_path,f'{mov:04d}.tif')\n",
    "# moving_stack = tifffile.imread(moving_path).astype(np.float32)\n",
    "# fixed_path = os.path.join(warped_path,f'{fixed:04d}.tif')\n",
    "# fixed_stack = tifffile.imread(fixed_path).astype(np.float32)\n",
    "# fixed_mask_path = os.path.join(input_dir, 'masks', f'{fixed:04d}.tif')\n",
    "# fixed_mask = tifffile.imread(fixed_mask_path)\n",
    "# fixed_mask_stack = np.stack([fixed_mask] * fixed_stack.shape[0])\n",
    "\n",
    "# moving_mask_path = os.path.join(input_dir, 'masks', f'{mov:04d}.tif')\n",
    "# moving_mask = tifffile.imread(moving_mask_path)\n",
    "# moving_mask_stack = np.stack([moving_mask] * fixed_stack.shape[0])\n",
    "\n",
    "\n",
    "# output_stack = register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stack = register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(fixed_stack[10,1,:,:])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(output_stack[10,1,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324971cc",
   "metadata": {},
   "source": [
    "## COPY RESULTS TO FLVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e04caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace with the actual path to your MSYS2 rsync executable\n",
    "RSYNC_EXE = r\"C:\\msys64\\usr\\bin\\rsync.exe\" \n",
    "\n",
    "flvc = 'munib@flv-c3'\n",
    "# Remote base (Linux)\n",
    "data_dir_remote_base = PurePosixPath('/home/munib/store1/shared/g5ht/data')\n",
    "# Local base (Windows)\n",
    "data_dir_local_base = Path(r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free')\n",
    "\n",
    "dataset = 'date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001.nd2'\n",
    "\n",
    "def to_msys_path(win_path):\n",
    "    \"\"\"\n",
    "    Converts a Windows Path (C:\\Users\\...) to MSYS2 format (/c/Users/...).\n",
    "    \"\"\"\n",
    "    p = Path(win_path).absolute()\n",
    "    drive = p.drive.replace(\":\", \"\").lower()\n",
    "    # Join parts skipping the drive letter\n",
    "    parts = \"/\".join(p.parts[1:])\n",
    "    return f\"/{drive}/{parts}\"\n",
    "\n",
    "def sync_dataset_group(base_filename):\n",
    "    # 1. Extract date from filename (e.g., '20251028')\n",
    "    try:\n",
    "        date_str = base_filename.split('_')[0].split('-')[1]\n",
    "    except IndexError:\n",
    "        print(f\"Could not parse date from {base_filename}. Check naming convention.\")\n",
    "        return\n",
    "\n",
    "    # 2. Define the three files to transfer\n",
    "    file_list = [\n",
    "        base_filename,\n",
    "        base_filename.replace('.nd2', '.h5'),\n",
    "        base_filename.replace('.nd2', '_chan_alignment.nd2')\n",
    "    ]\n",
    "\n",
    "    # 3. Setup Local Directory\n",
    "    local_target_dir = data_dir_local_base / date_str\n",
    "    local_target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert local target to MSYS2 format for the rsync command\n",
    "    local_dest_msys = to_msys_path(local_target_dir)\n",
    "\n",
    "    print(f\"--- Starting Sync for Date: {date_str} ---\")\n",
    "\n",
    "    for file in file_list:\n",
    "        # Construct Remote Path (Linux uses forward slashes)\n",
    "        # Assuming the files are inside a date subfolder on Linux too:\n",
    "        remote_file_path = data_dir_remote_base / date_str / file\n",
    "        \n",
    "        # Build rsync source string\n",
    "        rsync_src = f\"{flvc}:{remote_file_path}\"\n",
    "\n",
    "        # 4. Execute Rsync\n",
    "        # -a: archive, -v: verbose, -z: compress, -P: progress/partial\n",
    "        print(f\"Syncing: {file}...\")\n",
    "        \n",
    "        cmd = [\n",
    "            RSYNC_EXE, \n",
    "            \"-avzP\", \n",
    "            \"-e\", \"ssh -o StrictHostKeyChecking=no\", # Uses SSH for the tunnel\n",
    "            rsync_src, \n",
    "            local_dest_msys\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Successfully synced {file}\")\n",
    "        else:\n",
    "            # If rsync fails, it might be because the file doesn't exist\n",
    "            if \"No such file\" in result.stderr:\n",
    "                print(f\"File not found on remote: {file}\")\n",
    "            else:\n",
    "                print(f\"Error syncing {file}: {result.stderr}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sync_dataset_group(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5ht-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
