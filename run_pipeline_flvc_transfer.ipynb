{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1b7a0b",
   "metadata": {},
   "source": [
    "# G5HT-PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fee86",
   "metadata": {},
   "source": [
    "first transfers data from flvc to local directory, then moves results back to flvc\n",
    "\n",
    "The transferring code assumes you have set up an ssh key\n",
    "\n",
    "__set up ssh key__\n",
    "\n",
    "https://github.com/flavell-lab/FlavellLabWiki/wiki/Setting-up-SSH-key-on-Windows\n",
    "- generate ssh key\n",
    "- add to github account\n",
    "- transfer authorized keys to flvc\n",
    "  - type in powershell:\n",
    "    - Get-Content \"$env:USERPROFILE\\.ssh\\id_rsa.pub\" | ssh munib@flv-c2.mit.edu \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"\n",
    "- done, should be able to ssh into flvc (ssh flvc or ssh munib@flv-c3) without entering password"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c1c6a",
   "metadata": {},
   "source": [
    "## CONDA ENVIRONMENTS\n",
    "\n",
    "For steps __transfer files__, __1. preprocess__ and __2. mip__, `conda activate g5ht-pipeline`\n",
    "\n",
    "For step __3. segment__, `conda activate segment-torch` or `conda activate torchcu129`\n",
    "\n",
    "For step __4. spline, 5. orient, 6. warp, 7. reg__, `conda activate g5ht-pipeline`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748d257",
   "metadata": {},
   "source": [
    "__NOTE__\n",
    "\n",
    "You will need ~200GB of storage space locally for a single recording. After registration, you can delete ~150GB\n",
    "- .nd2: 50GB  (~55GB if separate channel alignment recording taken)\n",
    "- .h5: 9GB\n",
    "- .tifs created from shear_correction, channel_alignment, bleach_correction: 47GB each     (shear_correction, channel_alignment can be deleted after running the full pipeline)\n",
    "- .tifs created from warping and registering: 18GB each      (warped can be deleted after running the full pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a58251",
   "metadata": {},
   "source": [
    "__NOTE:__\n",
    "\n",
    "For transferring from flvc to windows pc, just use scp:\n",
    "\n",
    "```\n",
    "scp munib@flv-c2.mit.edu:/home/munib/store1/shared/g5ht/data/20260206/date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003.h5 C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20260206\\date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003.h5\n",
    "\n",
    "scp munib@flv-c2.mit.edu:/home/munib/store1/shared/g5ht/data/20260206/date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003.nd2 C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20260206\\date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003.nd2\n",
    "\n",
    "```\n",
    "\n",
    "For transferring results back to flvc from windows pc, use rsync in msys. see __COPY RESULTS TO FLVC__ section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e087d8",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21220aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    is_torch_env = False\n",
    "except ImportError:\n",
    "    is_torch_env = True\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e987c8",
   "metadata": {},
   "source": [
    "## SPECIFY DATA TO PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e36d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flvc = 'munib@flv-c3' # this is the username and hostname of the linux machine (c2,c3,c4)\n",
    "data_dir_flvc = r'/home/munib/store1/shared/g5ht/data' # this is a linux machine\n",
    "data_dir_local = Path(r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free') # this is a windows machine\n",
    "\n",
    "# dataset (see datasets.txt)\n",
    "dataset = 'date-20260206_strain-ISg5HT-mod-5_condition-fedpatch_worm003.nd2'\n",
    "\n",
    "INPUT_ND2 = dataset\n",
    "date_str = INPUT_ND2.split('_')[0].split('-')[1]\n",
    "local_dir = data_dir_local / date_str\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_ND2_PTH = os.path.join(local_dir, INPUT_ND2)\n",
    "\n",
    "NOISE_PTH = r'C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\noise\\noise_111125.tif'\n",
    "\n",
    "OUT_DIR = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "STACK_LENGTH = 41 if 'immo' not in INPUT_ND2 else 122\n",
    "\n",
    "# for recordings prior to roughly December 2025, we want to keep all but the last two z-slices, during which the piezo position is unstable\n",
    "# after December 2025, we want to keep all but the first two z-slices, during which the piezo position is unstable at the beginning of the recording\n",
    "date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "if date_obj < datetime(2025, 12, 1):\n",
    "    z2keep =  (0,STACK_LENGTH-2) # tuple representing range of z-slices to keep, should keep all but the last two slices\n",
    "else:\n",
    "    z2keep =  (2,STACK_LENGTH) # tuple representing range of z-slices to keep, should keep all but the first two slices\n",
    "\n",
    "\n",
    "# if data already exists locally, then we can skip the copying step and just load the data. if not, then we need to copy the data from the linux machine to the local machine\n",
    "data_exists_locally = False\n",
    "if os.path.exists(INPUT_ND2_PTH):\n",
    "    data_exists_locally = True\n",
    "    print(f\"Data file {INPUT_ND2_PTH} already exists locally. Will load data from local path.\")\n",
    "    print('')\n",
    "    print(\"Jump to step 1. shear correction\")\n",
    "    print('')\n",
    "    if not is_torch_env:\n",
    "        noise_stack = utils.get_noise_stack(NOISE_PTH, STACK_LENGTH)\n",
    "        num_frames, height, width, num_channels = utils.get_range_from_nd2(INPUT_ND2_PTH, stack_length=STACK_LENGTH) \n",
    "        beads_alignment_file = utils.get_beads_alignment_file(INPUT_ND2_PTH)\n",
    "    else:\n",
    "        print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")\n",
    "\n",
    "    print(INPUT_ND2)\n",
    "    print('Num z-slices: ', STACK_LENGTH)\n",
    "    if not is_torch_env:\n",
    "        print('Number of frames: ', num_frames)\n",
    "        print('Height: ', height)\n",
    "        print('width: ', width)\n",
    "        print('Number of channels: ', num_channels)\n",
    "        print('Beads alignment file: ', beads_alignment_file)\n",
    "else:\n",
    "    print(f\"Data file {INPUT_ND2_PTH} does not exist locally. Will copy from {flvc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7534ad",
   "metadata": {},
   "source": [
    "## TRANSFER FILES\n",
    "\n",
    "Takes about 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc380980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the files\n",
    "if not data_exists_locally:\n",
    "    utils.scp_from_flvc(dataset, data_dir_flvc, data_dir_local, flvc) # ~10 mins from flvc to local windows\n",
    "    utils.scp_from_flvc(os.path.splitext(dataset)[0] + '.h5', data_dir_flvc, data_dir_local, flvc) # ~ 3 mins\n",
    "    utils.scp_from_flvc(os.path.splitext(dataset)[0] + '_chan_alignment.nd2', data_dir_flvc, data_dir_local, flvc) # ~ 3 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a415d0",
   "metadata": {},
   "source": [
    "## 0. GET META DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcf1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_torch_env:\n",
    "    noise_stack = utils.get_noise_stack(NOISE_PTH, STACK_LENGTH)\n",
    "    num_frames, height, width, num_channels = utils.get_range_from_nd2(INPUT_ND2_PTH, stack_length=STACK_LENGTH) \n",
    "    beads_alignment_file = utils.get_beads_alignment_file(INPUT_ND2_PTH)\n",
    "else:\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")\n",
    "\n",
    "print(INPUT_ND2)\n",
    "print('Num z-slices: ', STACK_LENGTH)\n",
    "if not is_torch_env:\n",
    "    print('Number of frames: ', num_frames)\n",
    "    print('Height: ', height)\n",
    "    print('width: ', width)\n",
    "    print('Number of channels: ', num_channels)\n",
    "    print('Beads alignment file: ', beads_alignment_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d28b53",
   "metadata": {},
   "source": [
    "## 1. SHEAR CORRECTION \n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "~ 1 hour for 1200 image stacks with 2 color channels, 41 z, 512 h, 512 w\n",
    "\n",
    "- shear corrects each volume\n",
    "  - depending on each exposure time, it can take roughly half a second between the first and last frames of a volume, so any movements need to be corrected for\n",
    "- creates one `.tif` for each volume and stores it in the `shear_corrected` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shear_correct\n",
    "_ = importlib.reload(sys.modules['shear_correct'])\n",
    "\n",
    "start_index = \"0\"\n",
    "end_index = str(num_frames-1)\n",
    "# start_index = \"59\"\n",
    "# end_index = \"803\"\n",
    "cpu_count = str(int(os.cpu_count() / 4))\n",
    "# cpu_count = str(int(os.cpu_count()))\n",
    "skip_shear_correction = False # if True, will just denoise and save as tif\n",
    "\n",
    "# sys.argv = [\"\", nd2 file, start_frame, end_frame, noise_pth, stack_length, n_workers, num_frames, height, width, num_channels]\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, z2keep, skip_shear_correction]\n",
    "\n",
    "# Call the main function\n",
    "shear_correct.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357bd32",
   "metadata": {},
   "source": [
    "## 2. CHANNEL ALIGNMENT\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "Takes around 30 minutes for 1200 image stacks with 2 color channels, 39 z, 512 h, 512 w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0617b",
   "metadata": {},
   "source": [
    "### 2a. GET MEDIAN CHANNEL ALIGNMENT PARAMETERS FROM ALL FRAMES\n",
    "\n",
    "- If channel alignment file found, uses that, if not uses worm recording\n",
    "- creates a `.txt` file for each volume that contains elastix channel registration parameters\n",
    "- creates `chan_align_params.csv` and  `chan_align.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beads_alignment_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87844a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_channel_alignment\n",
    "import median_channel_alignment\n",
    "_ = importlib.reload(sys.modules['get_channel_alignment'])\n",
    "_ = importlib.reload(sys.modules['median_channel_alignment'])\n",
    "\n",
    "## set beads_alignment_file to None to use worm recording for channel alignment, even if beads file exists\n",
    "# beads_alignment_file = None\n",
    "\n",
    "start_index = \"0\"\n",
    "end_index = str(num_frames-1)\n",
    "\n",
    "cpu_count = str(int(os.cpu_count() / 4))\n",
    "# cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "every_other = True # if True, will only use every other frame for channel alignment, which should be sufficient and will speed up the process by 2x\n",
    "\n",
    "if beads_alignment_file is not None:\n",
    "    align_with_beads = True\n",
    "    every_other = False # if using beads for alignment, we want to use all frames because there's only 50\n",
    "    num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "    sys.argv = [\"\", beads_alignment_file, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames_beads, height, width, num_channels, every_other, align_with_beads]\n",
    "else:\n",
    "    align_with_beads = False\n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, every_other, align_with_beads]\n",
    "\n",
    "# # Call the main function\n",
    "get_channel_alignment.main()\n",
    "median_channel_alignment.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe582c3e",
   "metadata": {},
   "source": [
    "### 2b. APPLY MEDIAN CHANNEL ALIGNMENT PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b97453",
   "metadata": {},
   "source": [
    "- ouputs aligned volumes in `channel_aligned` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apply_channel_alignment\n",
    "_ = importlib.reload(sys.modules['apply_channel_alignment'])\n",
    "\n",
    "start_index = \"0\"\n",
    "end_index = str(num_frames-1)\n",
    "cpu_count = str(int(os.cpu_count() / 4))\n",
    "# cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "# 0786 to 0799 are bad frames in worm005.nd2, copied 0785 for each of those frames\n",
    "\n",
    "if beads_alignment_file is not None:\n",
    "    align_with_beads = True\n",
    "    num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads, beads_alignment_file]\n",
    "else:\n",
    "    align_with_beads = False\n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads]\n",
    "\n",
    "\n",
    "# Call the main function\n",
    "apply_channel_alignment.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc706e",
   "metadata": {},
   "source": [
    "### 2c. PLOT CHANNEL ALIGNMENT PARAMETER DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# make font sizes larger for visibility\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "try:\n",
    "    out_dir = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "    df = pd.read_csv(os.path.join(out_dir, 'chan_align_params.csv'))\n",
    "    params = ['TransformParameter_0', 'TransformParameter_1', 'TransformParameter_2', 'TransformParameter_3', 'TransformParameter_4', 'TransformParameter_5']\n",
    "    labels = ['Rx', 'Ry', 'Rz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "    # the xaxis limits for each subplot should be the same across figures\n",
    "\n",
    "    xlims = np.zeros((6,2))\n",
    "\n",
    "    plt.figure(figsize=(12,8), tight_layout=True)\n",
    "    for i,param in enumerate(params):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.hist(df[param], bins=30, color='red', alpha=0.6)\n",
    "        # plot the median value as a vertical line\n",
    "        median_value = df[param].median()\n",
    "        plt.axvline(median_value, color='black', linestyle='dashed', linewidth=2)\n",
    "        plt.xlabel(labels[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        # get xaxis limits\n",
    "        xlims[i,:] = plt.xlim()\n",
    "        # title is median value\n",
    "        plt.title(f'Median: {np.round(median_value,3)}', fontsize=14)\n",
    "    plt.savefig(os.path.join(out_dir, 'chan_align_params_histograms.png'))\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"No chan_align_params.csv found for worm recording\")\n",
    "\n",
    "try:\n",
    "    out_dir = os.path.splitext(INPUT_ND2_PTH)[0] + '_chan_alignment'\n",
    "    df = pd.read_csv(os.path.join(out_dir, 'chan_align_params.csv'))\n",
    "    params = ['TransformParameter_0', 'TransformParameter_1', 'TransformParameter_2', 'TransformParameter_3', 'TransformParameter_4', 'TransformParameter_5']\n",
    "    labels = ['Rx', 'Ry', 'Rz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "    plt.figure(figsize=(12,8), tight_layout=True)\n",
    "    for i,param in enumerate(params):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.hist(df[param], bins=30, color='blue', alpha=0.6)\n",
    "        # plot the median value as a vertical line\n",
    "        median_value = df[param].median()\n",
    "        plt.axvline(median_value, color='black', linestyle='dashed', linewidth=2)\n",
    "        plt.xlabel(labels[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        # apply xlims\n",
    "        # plt.xlim(xlims[i,0], xlims[i,1])\n",
    "        # title is median value, font size 14\n",
    "        plt.title(f'Median: {np.round(median_value,3)}', fontsize=14)\n",
    "    plt.savefig(os.path.join(out_dir, 'chan_align_params_histograms.png'))\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"No chan_align_params.csv found for beads recording\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f4640",
   "metadata": {},
   "source": [
    "## 3. BLEACH CORRECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4ae34",
   "metadata": {},
   "source": [
    "Takes about 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b283de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import bleach_correct\n",
    "_ = importlib.reload(sys.modules['bleach_correct'])\n",
    "\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "REG_DIR = 'channel_aligned' # 'channel_aligned' or 'tif' \n",
    "channels = 1 # 0-gfp, 1-rfp\n",
    "method = 'block' # 'block' or 'exponential'\n",
    "mode = 'total' # 'total' or 'median'\n",
    "output_dir = os.path.join(PTH, 'bleach_corrected')\n",
    "\n",
    "bleach_correct.correct_bleaching(os.path.join(PTH,REG_DIR), output_dir=output_dir, channels=channels, method=method, fbc=0.04, intensity_mode=mode)\n",
    "\n",
    "\n",
    "# # Correct RFP only with block method (default)\n",
    "# correct_bleaching(\"path/to/data\")\n",
    "\n",
    "# # Correct both channels with exponential fit\n",
    "# correct_bleaching(\"path/to/data\", channels=[0, 1], method='exponential')\n",
    "\n",
    "# # Command line\n",
    "# python bleach_correct.py path/to/data --channels 0 1 --method exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a097705",
   "metadata": {},
   "source": [
    "## 4. MIP\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `means.png`, `focus.png`, `mip.tif`, and `mip.mp4`, `focus_check.csv`\n",
    "\n",
    "##### TODO: \n",
    "- mip for xy, xz, zy\n",
    "- mip for several slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mip\n",
    "\n",
    "_ = importlib.reload(sys.modules['mip'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "framerate = 8\n",
    "tif_dir = 'bleach_corrected' # one of 'shear_corrected' 'channel_aligned' 'bleach_corrected'\n",
    "# tif_dir = 'shear_corrected'\n",
    "rmax = 850\n",
    "gmax = 150\n",
    "mp4_quality = 10\n",
    "do_focus = True\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, tif_dir, STACK_LENGTH, num_frames, framerate, rmax, gmax, mp4_quality, do_focus]\n",
    "\n",
    "# Call the main function\n",
    "mip.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563861dd",
   "metadata": {},
   "source": [
    "## 5 DRIFT ESTIMATION\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs  `z_selection.csv`, `z_selection_diagnostics.png`, `sharpness.csv`\n",
    "\n",
    "Few minutes to run\n",
    "\n",
    "TODO:\n",
    "- use z selection going forward\n",
    "- also use sharpness/focus (and other things) to determine good/bad frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drift_estimation\n",
    "\n",
    "_ = importlib.reload(sys.modules['drift_estimation'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "tif_dir = 'bleach_corrected' # one of 'shear_corrected' 'channel_aligned' 'bleach_corrected'\n",
    "\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, tif_dir, STACK_LENGTH, num_frames]\n",
    "\n",
    "# Call the main function\n",
    "drift_estimation.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6588494",
   "metadata": {},
   "source": [
    "## 6. SEGMENT\n",
    "\n",
    "- outputs `label.tif`, contains segmented MIP for each volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f60d1",
   "metadata": {},
   "source": [
    "__on home pc__: \n",
    "`conda activate segment-torch`\n",
    "\n",
    "Uses a separate conda environment from the rest of the pipeline. create it using:\n",
    "`conda env create -f segment_torch.yml`\n",
    "\n",
    "__on lab pc__: \n",
    "`conda activate torchcu129`\n",
    "\n",
    "Uses a separate conda environment from the rest of the pipeline. create it following steps in:\n",
    "`segment_torch_cu129_environment.yml`\n",
    "\n",
    "__setup each time model weights change__\n",
    "Need to set path to model weights as `CHECKPOINT` in `eval_torch.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b374772",
   "metadata": {},
   "source": [
    "a minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segment.segment_torch\n",
    "_ = importlib.reload(sys.modules['segment.segment_torch'])\n",
    "\n",
    "mip_tif = 'mip_bleach_corrected'\n",
    "# mip_tif = 'mip_channel_aligned' \n",
    "\n",
    "MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], f'{mip_tif}.tif')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", MIP_PTH]\n",
    "\n",
    "segment.segment_torch.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989c725",
   "metadata": {},
   "source": [
    "## 7. SPLINE\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `spline.json`, `spline.tif`, and `dilated.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac034f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spline\n",
    "_ = importlib.reload(sys.modules['spline'])\n",
    "\n",
    "LABEL_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'label.tif')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", LABEL_PTH]\n",
    "\n",
    "spline.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb7360",
   "metadata": {},
   "source": [
    "## 8. ORIENT\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `oriented.json`, `oriented.png`, `oriented_stack.tif`\n",
    "\n",
    "NOTE: `orient_v2.py` automated the process of finding orientation completely, whereas `orient.py` requires you to input the (x,y) nose location on the first frame\n",
    "\n",
    "For `orient.py`, you have to specify the nose (x,y) coordinates for the first frame (and maybe others if needed). Look at `spline.tif` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orient\n",
    "_ = importlib.reload(sys.modules['orient'])\n",
    "\n",
    "SPLINE_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'spline.json')\n",
    "nose_y = 328\n",
    "nose_x = 494\n",
    "\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", SPLINE_PTH, str(nose_y), str(nose_x)]\n",
    "\n",
    "# # # apply constraints\n",
    "# # # might need this when there are frames where the spline fitting fails and orientation is lost intermittently\n",
    "# # # each constraint is a triplet: (frame, nose_y, nose_x)\n",
    "# constraints = [\n",
    "#     (455, 145, 236),\n",
    "# ]\n",
    "\n",
    "# # # with constraints: append triplets of (frame, nose_y, nose_x)\n",
    "# constraint_args = []\n",
    "# for frame, cy, cx in constraints:\n",
    "#     constraint_args.extend([str(frame), str(cy), str(cx)])\n",
    "# sys.argv = [\"\", SPLINE_PTH, str(nose_y), str(nose_x)] + constraint_args\n",
    "\n",
    "\n",
    "orient.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orient_v2 # tried to automate finding nose point, not working well at the moment\n",
    "_ = importlib.reload(sys.modules['orient_v2'])\n",
    "\n",
    "SPLINE_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'spline.json')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", SPLINE_PTH]\n",
    "\n",
    "orient_v2.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae94ec6",
   "metadata": {},
   "source": [
    "## 9. WARP\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- ouputs: `warped/*.tif` and `masks/*.tif`\n",
    "\n",
    "TODO: parallelize\n",
    "\n",
    "takes 1-2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1eac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warp\n",
    "_ = importlib.reload(sys.modules['warp'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "start_index = 0\n",
    "end_index = num_frames\n",
    "\n",
    "for i in tqdm(range(start_index, end_index)):\n",
    "    # command-line arguments\n",
    "    sys.argv = [\"\", PTH, i]\n",
    "\n",
    "    warp.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ddcbc",
   "metadata": {},
   "source": [
    "## 10. REGISTER\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "__ALTERNATIVELY__: register using the wholistic registration algorithm, currently in MATLAB\n",
    "\n",
    "TODO: parallelize / make faster\n",
    "\n",
    "- pick a good representative fixed frame that you want to register everything to\n",
    "  - copy it to the main output folder and name it `fixed_xxxx.tif`\n",
    "  - copy the corresponding mask and name it `fixed_mask_xxxx.tif`\n",
    "\n",
    "Takes around 5 hours for 1200 stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d789cbc",
   "metadata": {},
   "source": [
    "### MAKE COPIES OF FIXED FRAME AND MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04964e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_frame = '0315'\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "warped_dir = os.path.join(PTH, 'warped')\n",
    "mask_dir = os.path.join(PTH, 'masks')\n",
    "\n",
    "warped_fixed_frame_pth = os.path.join(warped_dir, f'{fixed_frame}.tif')\n",
    "mask_fixed_frame_pth = os.path.join(mask_dir, f'{fixed_frame}.tif')\n",
    "\n",
    "shutil.copy(warped_fixed_frame_pth, os.path.join(PTH, f'fixed_{fixed_frame}.tif'))\n",
    "shutil.copy(mask_fixed_frame_pth, os.path.join(PTH, f'fixed_mask_{fixed_frame}.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cefc4b",
   "metadata": {},
   "source": [
    "### REGISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ND2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b722e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reg\n",
    "_ = importlib.reload(sys.modules['reg'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "start_index = 0\n",
    "end_index = num_frames\n",
    "zoom = 1 # albert was using 3\n",
    "# zoom = 3\n",
    "\n",
    "for i in tqdm(range(start_index, end_index)):\n",
    "    # command-line arguments\n",
    "    try:\n",
    "        sys.argv = [\"\", PTH, i, str(zoom)]\n",
    "        reg.main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {i}: {e}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd56099",
   "metadata": {},
   "source": [
    "### REGISTER WITH GFP+1 TO RFP\n",
    "\n",
    "TRIM LAST RFP ZSLICE, TRIM FIRST GFP ZSLICE\n",
    "\n",
    "seems to be that as of 20251204, all recordings were taken such that the i zslice in red channel corresponds to i+1 zslice in green channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "from reg_gfp_indexing import main as reg_worm\n",
    "\n",
    "PTH = r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001_aligned'\n",
    "\n",
    "for i in tqdm(range(1200)):\n",
    "    # command-line arguments\n",
    "    sys.argv = [\"\", PTH, i, \"1\"]\n",
    "    reg_worm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183d57b",
   "metadata": {},
   "source": [
    "### MAKE MOVIES OF REGISTERED DATA (see `reg_microfilm.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a541fd9",
   "metadata": {},
   "source": [
    "## 10. EXTRACT BEHAVIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "FN = PTH + '.h5'\n",
    "\n",
    "f = h5py.File(FN, 'r')\n",
    "\n",
    "im = f.get('img_nir')[:] # THW\n",
    "sz = im.shape\n",
    "\n",
    "fps = 20.0  # frames per second\n",
    "\n",
    "print('Video shape: ', sz)\n",
    "\n",
    "nframes = im.shape[0]\n",
    "record_duration = nframes / fps # in seconds\n",
    "\n",
    "print('Recording duration (s): ', record_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842030c",
   "metadata": {},
   "source": [
    "### MAKE NIR MP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save im, which is of shape (frame, height, width) as a .mp4 video\n",
    "import cv2\n",
    "out_fn = os.path.join(PTH, 'nir_video.mp4')\n",
    "if not os.path.exists(PTH):\n",
    "    os.makedirs(PTH)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(out_fn, fourcc, fps, (sz[2], sz[1]), isColor=False)\n",
    "# save video\n",
    "for i in range(nframes):\n",
    "    # print status every 100 frames\n",
    "    if i % 100 == 0:\n",
    "        print('Saving frame: ', i, ' / ', nframes)\n",
    "    frame = im[i,:,:]\n",
    "    frame = (frame / np.max(frame) * 255).astype(np.uint8)\n",
    "    # overlay frame with text of frame number and time in seconds\n",
    "    time_sec = i / fps\n",
    "    text = f'Frame: {i}  Time: {time_sec:.2f} s'\n",
    "    cv2.putText(frame, text, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255), 2)\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "    out.write(frame)\n",
    "out.release()\n",
    "print('Saved video to: ', out_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324971cc",
   "metadata": {},
   "source": [
    "## COPY RESULTS TO FLVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994d95a",
   "metadata": {},
   "source": [
    "Files that should be copied back to flvc:\n",
    "- from wormXXX directory\n",
    "  - transfer everything except:\n",
    "    - channel_aligned/*\n",
    "    - shear_corrected/*\n",
    "    - warped/*\n",
    "- the wormXXX_chan_alignment directory and contents if it exists\n",
    "\n",
    "\n",
    "Takes about an hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8363740",
   "metadata": {},
   "source": [
    "In msys:\n",
    "\n",
    "A dry run:\n",
    "\n",
    "rsync -avzP -e ssh -o --dry-run --exclude=channel_aligned/ --exclude=shear_corrected/ /c/Users/munib/POSTDOC/DATA/g5ht-free/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002/ munib@flv-c2:/home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002\n",
    "\n",
    "real thing:\n",
    "rsync -avzP -e ssh -o --dry-run --exclude=channel_aligned/ --exclude=shear_corrected/ /c/Users/munib/POSTDOC/DATA/g5ht-free/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002/ munib@flv-c2:/home/munib/store1/shared/g5ht/data/20251028/date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e04caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this just prints the rsync command for you to run in terminal (msys on windows)\n",
    "import subprocess\n",
    "from pathlib import Path, PurePosixPath\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DRY_RUN = True  # Set to False to actually transfer files\n",
    "\n",
    "msys_path = r\"C:\\msys64\\usr\\bin\"\n",
    "os.environ[\"PATH\"] = msys_path + os.pathsep + os.environ[\"PATH\"]\n",
    "RSYNC_EXE = r\"C:\\msys64\\usr\\bin\\rsync.exe\"\n",
    "\n",
    "flvc = 'munib@flv-c2'\n",
    "data_dir_remote_base = PurePosixPath('/home/munib/store1/shared/g5ht/data')\n",
    "data_dir_local_base = Path(r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free')\n",
    "\n",
    "dataset_base = os.path.splitext(INPUT_ND2)[0]  # reuse from earlier cell\n",
    "date_str = dataset_base.split('_')[0].split('-')[1]\n",
    "\n",
    "EXCLUSIONS = [\"channel_aligned/\", \"shear_corrected/\"] # transfer everything in results output but these\n",
    "\n",
    "def to_msys_path(win_path):\n",
    "    \"\"\"Convert a Windows path to an MSYS2/Cygwin-style path for rsync.\"\"\"\n",
    "    p = Path(win_path).resolve()\n",
    "    drive = p.drive.replace(\":\", \"\").lower()\n",
    "    rest = \"/\".join(p.parts[1:])\n",
    "    return f\"/{drive}/{rest}\"\n",
    "\n",
    "def rsync_push(local_path, remote_path, exclusions=None):\n",
    "    \"\"\"Push a local directory to a remote host via rsync.\n",
    "    \n",
    "    Trailing slash on local_path means 'copy contents into remote_path'.\n",
    "    \"\"\"\n",
    "    local_msys = to_msys_path(local_path).rstrip(\"/\") + \"/\"\n",
    "    remote_dest = f\"{flvc}:{remote_path}\"\n",
    "\n",
    "    cmd = [\n",
    "        RSYNC_EXE,\n",
    "        \"-avzP\",\n",
    "        \"-e\", \"ssh -o\",\n",
    "    ]\n",
    "    if DRY_RUN:\n",
    "        cmd.append(\"--dry-run\")\n",
    "    for pattern in (exclusions or []):\n",
    "        cmd.append(f\"--exclude={pattern}\")\n",
    "    cmd.extend([local_msys, remote_dest])\n",
    "\n",
    "    print(f\"{'[DRY RUN] ' if DRY_RUN else ''}Pushing: {local_path}  -->  {remote_dest}\")\n",
    "    print(\" \".join(cmd), \"\\n\")\n",
    "\n",
    "    # result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    # print(result.stdout)\n",
    "    # if result.returncode != 0:\n",
    "    #     print(f\"*** rsync failed (exit {result.returncode}) ***\")\n",
    "    #     print(result.stderr)\n",
    "    # return result.returncode\n",
    "\n",
    "# --- SYNC MAIN DATASET DIRECTORY (excluding large intermediate dirs) ---\n",
    "local_dir = data_dir_local_base / date_str / dataset_base\n",
    "remote_dir = f\"{data_dir_remote_base}/{date_str}/{dataset_base}\"\n",
    "\n",
    "if local_dir.exists():\n",
    "    rsync_push(local_dir, remote_dir, exclusions=EXCLUSIONS)\n",
    "else:\n",
    "    print(f\"Local directory not found, skipping: {local_dir}\")\n",
    "\n",
    "# --- SYNC CHANNEL ALIGNMENT DIRECTORY (if it exists) ---\n",
    "align_dir = data_dir_local_base / date_str / (dataset_base + \"_chan_alignment\")\n",
    "remote_align_dir = f\"{data_dir_remote_base}/{date_str}/{dataset_base}_chan_alignment\"\n",
    "\n",
    "if align_dir.exists():\n",
    "    rsync_push(align_dir, remote_align_dir)\n",
    "else:\n",
    "    print(f\"No channel alignment directory found locally, skipping: {align_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aec566",
   "metadata": {},
   "source": [
    "## LOCAL FILES CAN NOW BE DELETED IF TRANSFER COMPLETED SUCCESSFULLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb4f93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5ht-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
