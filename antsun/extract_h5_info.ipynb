{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ad2d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for: D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002.h5\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# set default matplotlib parameters for better aesthetics\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "# fontsize\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "# ============================================================\n",
    "# SET YOUR DATA PATH HERE\n",
    "# ============================================================\n",
    "# PTH = r'D:\\DATA\\g5ht-free\\20251223'\n",
    "# FN = 'date-20251223_strain-ISg5HT_condition-starvedpatch_worm005'  # without .h5 extension\n",
    "\n",
    "PTH = r'D:\\DATA\\g5ht-free\\20251028'\n",
    "FN = 'date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002'  # without .h5 extension\n",
    "\n",
    "h5_path = os.path.join(PTH, FN + '.h5')\n",
    "\n",
    "# Number of z-slices per confocal volume\n",
    "N_STACK = 41\n",
    "\n",
    "print(f\"Looking for: {h5_path}\")\n",
    "print(f\"File exists: {os.path.exists(h5_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7723e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 file structure: D:\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm002.h5\n",
      "\n",
      "  daqmx_ai                                 shape=(3, 2029572)         dtype=float64\n",
      "  daqmx_di                                 shape=(2, 2029588)         dtype=uint32\n",
      "  img_metadata/  (group)\n",
      "  img_metadata/img_id                      shape=(15948,)             dtype=int64\n",
      "  img_metadata/img_timestamp               shape=(15948,)             dtype=int64\n",
      "  img_metadata/q_iter_save                 shape=(15948,)             dtype=uint8\n",
      "  img_metadata/q_recording                 shape=(15948,)             dtype=uint8\n",
      "  img_nir                                  shape=(7974, 732, 968)     dtype=uint8\n",
      "  pos_feature                              shape=(7974, 3, 3)         dtype=float32\n",
      "  pos_stage                                shape=(7974, 2)            dtype=float64\n",
      "  recording_start                          shape=()                   dtype=|S23\n",
      "\n",
      "--- Key Dataset Info ---\n",
      "  img_nir: (7974, 732, 968)  (n_frames=7974)\n",
      "  daqmx_ai: (3, 2029572)\n",
      "  daqmx_di: (2, 2029588)\n",
      "  img_metadata/img_id: (15948,)\n",
      "  img_metadata/img_timestamp: (15948,)\n",
      "  img_metadata/q_iter_save: (15948,)\n",
      "  img_metadata/q_recording: (15948,)\n"
     ]
    }
   ],
   "source": [
    "def print_h5_structure(h5_path):\n",
    "    \"\"\"Recursively print all datasets and groups in an HDF5 file.\"\"\"\n",
    "    def visitor(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  {name:40s} shape={str(obj.shape):20s} dtype={obj.dtype}\")\n",
    "        elif isinstance(obj, h5py.Group):\n",
    "            print(f\"  {name}/  (group)\")\n",
    "\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        print(f\"\\nHDF5 file structure: {h5_path}\\n\")\n",
    "        f.visititems(visitor)\n",
    "        \n",
    "        # Key checks\n",
    "        print(\"\\n--- Key Dataset Info ---\")\n",
    "        if 'img_nir' in f:\n",
    "            shape = f['img_nir'].shape\n",
    "            print(f\"  img_nir: {shape}  (n_frames={shape[0] if len(shape)==3 else shape[2]})\")\n",
    "        else:\n",
    "            print(\"  ⚠ img_nir NOT FOUND\")\n",
    "            \n",
    "        if 'daqmx_ai' in f:\n",
    "            print(f\"  daqmx_ai: {f['daqmx_ai'].shape}\")\n",
    "        else:\n",
    "            print(\"  ⚠ daqmx_ai NOT FOUND\")\n",
    "            \n",
    "        if 'daqmx_di' in f:\n",
    "            print(f\"  daqmx_di: {f['daqmx_di'].shape}\")\n",
    "        else:\n",
    "            print(\"  ⚠ daqmx_di NOT FOUND\")\n",
    "            \n",
    "        if 'img_metadata' in f:\n",
    "            for key in f['img_metadata'].keys():\n",
    "                print(f\"  img_metadata/{key}: {f['img_metadata'][key].shape}\")\n",
    "        else:\n",
    "            print(\"  ⚠ img_metadata NOT FOUND\")\n",
    "\n",
    "print_h5_structure(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd9237dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "daqmx_ai shape: (3, 2029572)\n",
      "daqmx_di shape: (2, 2029588)\n",
      "recording_start: b'2025-10-28T17:13:20.453'\n"
     ]
    }
   ],
   "source": [
    "# Quick visualization of the signals used for synchronization\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "lw = 2\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    recording_start = f['recording_start'][()]\n",
    "\n",
    "n_index = daqmx_ai.shape[1]\n",
    "\n",
    "# for each signal, concatenate in time, the first N and the last N samples, to visualize the beginning and end of the recording in one plot\n",
    "# pad with NaNs in between to separate the two segments visually\n",
    "Npad_pre = 10000 # skip the first 10000 samples, which are very noisy and not useful for visualization\n",
    "Npre = 40000\n",
    "Npost = 100000\n",
    "Npad = 5000\n",
    "signal_segments_ai = []\n",
    "signal_segments_di = []\n",
    "for i in range(daqmx_ai.shape[0]):\n",
    "    signal = daqmx_ai[:, i] if daqmx_ai.shape[0] > daqmx_ai.shape[1] else daqmx_ai[i, :]\n",
    "    segment = np.concatenate((signal[Npad_pre:Npad_pre+Npre], np.full(Npad, np.nan), signal[-Npost:]))\n",
    "    signal_segments_ai.append(segment)\n",
    "for i in range(daqmx_di.shape[0]):\n",
    "    signal = daqmx_di[:, i] if daqmx_di.shape[0] > daqmx_di.shape[1] else daqmx_di[i, :]\n",
    "    segment = np.concatenate((signal[Npad_pre:Npad_pre+Npre], np.full(Npad, np.nan), signal[-Npost:]))\n",
    "    signal_segments_di.append(segment)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 14), sharex=True)\n",
    "\n",
    "labels = [\n",
    "    ('daqmx_ai col 0', 'Laser Signal', 'orange'),\n",
    "    ('daqmx_ai col 1', 'Piezo Signal', 'blue'),\n",
    "    ('daqmx_di col 0', 'Confocal Camera DI', 'green'),\n",
    "    ('daqmx_di col 1', 'NIR Camera DI', 'red'),\n",
    "]\n",
    "\n",
    "# plot the analog and digital signal segments\n",
    "for i in range(2):\n",
    "    axes[i].plot(signal_segments_ai[i], color=labels[i][2], linewidth=lw)\n",
    "    axes[i].set_ylabel(labels[i][1])\n",
    "    axes[i].set_title(labels[i][0])\n",
    "for i in range(2):\n",
    "    axes[i+2].plot(signal_segments_di[i], color=labels[i+2][2], linewidth=lw)\n",
    "    axes[i+2].set_ylabel(labels[i+2][1])\n",
    "    axes[i+2].set_title(labels[i+2][0])\n",
    "\n",
    "# # Plot analog inputs\n",
    "# for i in range(2):\n",
    "#     axes[i].plot(daqmx_ai[i, index2plot[0]:index2plot[1]] if daqmx_ai.shape[0] < daqmx_ai.shape[1] else daqmx_ai[index2plot[0]:index2plot[1], i],\n",
    "#                  color=labels[i][2], linewidth=lw)\n",
    "#     axes[i].set_ylabel(labels[i][1])\n",
    "#     axes[i].set_title(labels[i][0])\n",
    "\n",
    "# # Plot digital inputs\n",
    "# for i in range(2):\n",
    "#     axes[i+2].plot(daqmx_di[i, index2plot[0]:index2plot[1]] if daqmx_di.shape[0] < daqmx_di.shape[1] else daqmx_di[index2plot[0]:index2plot[1], i],\n",
    "#                    color=labels[i+2][2], linewidth=lw)\n",
    "#     axes[i+2].set_ylabel(labels[i+2][1])\n",
    "#     axes[i+2].set_title(labels[i+2][0])\n",
    "\n",
    "axes[-1].set_xlabel('Sample index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\ndaqmx_ai shape: {daqmx_ai.shape}\")\n",
    "print(f\"daqmx_di shape: {daqmx_di.shape}\")\n",
    "print(f\"recording_start: {recording_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb89b85",
   "metadata": {},
   "source": [
    "# V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c56073",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65347b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'signal' is your numpy array \n",
    "# and 'peaks' is your array of peak indices\n",
    "def find_wave_starts(signal, peaks):\n",
    "    starts = []\n",
    "    \n",
    "    for i in range(len(peaks)):\n",
    "        if i == 0:\n",
    "            # For the first peak, search from the beginning of the signal\n",
    "            start_idx = np.argmin(signal[:peaks[i]])\n",
    "        else:\n",
    "            # Search between the previous peak and current peak\n",
    "            search_range = signal[peaks[i-1] : peaks[i]]\n",
    "            # Add the offset of the previous peak to get the absolute index\n",
    "            start_idx = np.argmin(search_range) + peaks[i-1]\n",
    "            \n",
    "        starts.append(start_idx)\n",
    "        \n",
    "    return np.array(starts)\n",
    "\n",
    "# Example usage:\n",
    "# start_indices = find_wave_starts(my_signal, peak_indices)\n",
    "\n",
    "def map_timestamps(source_timing, reference_timing):\n",
    "    \"\"\"\n",
    "    Finds which interval in reference_timing each source_start falls into.\n",
    "    Assumes reference_timing is sorted by start times.\n",
    "    \"\"\"\n",
    "    # Extract start times for binary search\n",
    "    ref_starts = reference_timing[:, 0]\n",
    "    ref_stops = reference_timing[:, 1]\n",
    "    source_starts = source_timing[:, 0]\n",
    "\n",
    "    # Find the index where each source_start would fit in ref_starts\n",
    "    # 'side=right' - 1 gives us the index j such that ref_starts[j] <= source_start\n",
    "    idx = np.searchsorted(ref_starts, source_starts, side='right') - 1\n",
    "\n",
    "    # Validation: Check if it actually falls within the [start, stop] window\n",
    "    # If the index is -1 or source_start > stop, it doesn't fit any interval\n",
    "    mask = (idx >= 0) & (source_starts <= ref_stops[idx])\n",
    "    \n",
    "    # Create result array (NaN for no match, 1-based indexing for matches)\n",
    "    result = np.full(source_starts.shape[0], np.nan)\n",
    "    result[mask] = idx[mask] + 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def generate_consecutive_counts(arr):\n",
    "    # Ensure input is a numpy array\n",
    "    arr = np.asarray(arr)\n",
    "    \n",
    "    # Identify where the values change (the first element is always a change)\n",
    "    # np.diff returns an array of differences between consecutive elements\n",
    "    # We check where that difference is not zero\n",
    "    change_indices = np.where(arr[:-1] != arr[1:])[0] + 1\n",
    "    \n",
    "    # Create an array of zeros to store the result\n",
    "    res = np.zeros(len(arr), dtype=int)\n",
    "    \n",
    "    # The starting positions of each new group are at index 0 and \n",
    "    # all positions identified by change_indices\n",
    "    starts = np.concatenate(([0], change_indices))\n",
    "    \n",
    "    # For each group, we subtract the start index from the current index\n",
    "    # to get the running count: [0, 1, 2, ...]\n",
    "    for i in range(len(starts)):\n",
    "        start = starts[i]\n",
    "        end = starts[i+1] if i + 1 < len(starts) else len(arr)\n",
    "        res[start:end] = np.arange(end - start)\n",
    "        \n",
    "    return res\n",
    "\n",
    "# # Example usage:\n",
    "# arr = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 5, 5, 5, 5])\n",
    "# print(generate_consecutive_counts(arr))\n",
    "# # Output: [0 1 2 3 4 0 1 2 3 4 0 1 2 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e40abf",
   "metadata": {},
   "source": [
    "### Plot all DAQ signals together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c420924",
   "metadata": {},
   "source": [
    "session.list_cam_info contains img_metadata\n",
    "\n",
    "It's using spinnaker.jl, so it's probably time since hardware powered on (img_timestamp)\n",
    "and it's probably in nanoseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67bf05f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.250008088)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_time_sec = (img_timestamp - img_timestamp[0]) / 1e9\n",
    "\n",
    "relative_time_sec[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e878d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d39cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "%matplotlib qt\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "N_STACK = 41\n",
    "\n",
    "NIDAQ_SAMPLE_RATE_AI = 5000.0 # Hz\n",
    "CONTROL_NIR_FPS = 40.0\n",
    "SAVE_NIR_FPS = 20.0\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    n_img_nir = f['img_nir'].shape[0]\n",
    "    img_metadata = f['img_metadata']\n",
    "    img_timestamp = img_metadata['img_timestamp'][:]\n",
    "    img_id = img_metadata[\"img_id\"][:]\n",
    "    q_iter_save = img_metadata[\"q_iter_save\"][:]\n",
    "\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "# ai_stim = daqmx_ai[2,:].astype(np.float32)\n",
    "img_timestamp_sec = (img_timestamp - img_timestamp[0]) / 1e9\n",
    "\n",
    "di_confocal_rise = np.where(np.diff(di_confocal) > 0.0)[0] + 1\n",
    "\n",
    "confocal_on_sample = np.where(np.diff(ai_laser) > 0.5)[0] + 1 # find rising edges (laser on)\n",
    "confocal_start_sample_ai = confocal_on_sample[0] # get first rising edge, which should correspond to confocal recording start\n",
    "confocal_off_sample = np.where(np.diff(ai_laser) < -0.5)[0] + 1 # find falling edges (laser off)\n",
    "confocal_stop_sample_ai = confocal_off_sample[1] # get second falling edge, which should correspond to confocal recording stop, first falling edge is at the beginning of the recording (see plots related to ai_laser)\n",
    "\n",
    "peaks, properties = find_peaks(\n",
    "    ai_piezo, \n",
    "    distance=100,\n",
    "    prominence=1.0\n",
    ") # find peaks in piezo signal, which should correspond to stack acquisition. distance is set to 400 samples\n",
    "stack_start_sample = peaks.copy() + 150 # +20 for visualization only\n",
    "stack_start_sample = np.insert(stack_start_sample, 0, confocal_start_sample_ai)  # prepend first element as confocal recording start\n",
    "stack_start_sample = stack_start_sample[:-1]  # delete last element\n",
    "\n",
    "stack_stop_sample = peaks.copy()\n",
    "stack_stop_sample[-1] = confocal_stop_sample_ai # set last stack stop to confocal recording stop\n",
    "piezo_timing = np.concatenate((stack_start_sample.reshape(-1,1), stack_stop_sample.reshape(-1,1)), axis=1) # combine piezo start and stop indices to get confocal stack timing\n",
    "\n",
    "# zoom = (0, 60000)\n",
    "# zoom = (3257743-100000,3257743)\n",
    "zoom = (0, len(ai_laser)-1)\n",
    "\n",
    "time = np.arange(len(ai_laser)) / NIDAQ_SAMPLE_RATE_AI\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(15,6), sharex=True)\n",
    "axs[0].plot(time[zoom[0]:zoom[1]], ai_laser[zoom[0]:zoom[1]], label='Laser Signal', color='orange')\n",
    "axs[1].plot(time[zoom[0]:zoom[1]], ai_piezo[zoom[0]:zoom[1]], label='Piezo Signal', color='blue')\n",
    "axs[2].plot(time[zoom[0]:zoom[1]], di_confocal[zoom[0]:zoom[1]], label='Confocal Camera DI', color='green')\n",
    "axs[3].plot(time[zoom[0]:zoom[1]], di_nir[zoom[0]:zoom[1]], label='NIR Camera DI', color='magenta')\n",
    "\n",
    "# plot every 41st di_confocal_rise index as vertical lines to see if they align with confocal stack timing\n",
    "# this is the start of the next confocal stack\n",
    "for idx in di_confocal_rise[::41]:\n",
    "    if zoom[0] <= idx < zoom[1]:\n",
    "        axs[2].axvline(x=time[idx]-time[zoom[0]], color='black', linestyle='-')\n",
    "\n",
    "# plot confocal recording start and stop\n",
    "axs[0].axvline(x=time[confocal_start_sample_ai]-time[zoom[0]], color='black', linestyle='--')\n",
    "axs[0].axvline(x=time[confocal_stop_sample_ai]-time[zoom[0]], color='black', linestyle='--')\n",
    "\n",
    "# plot stack start and stops as vertical lines to see if they align with confocal stack timing\n",
    "for (start,stop) in zip(piezo_timing[:,0], piezo_timing[:,1]):\n",
    "    if zoom[0] <= start < zoom[1]:\n",
    "        axs[1].axvline(x=time[start]-time[zoom[0]], color='green', linestyle='-')\n",
    "    if zoom[0] <= stop < zoom[1]:\n",
    "        axs[1].axvline(x=time[stop]-time[zoom[0]], color='red', linestyle='-')\n",
    "\n",
    "plt.xlim(0, time[zoom[1]]-time[zoom[0]])\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49a21da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "%matplotlib qt\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "N_STACK = 41\n",
    "\n",
    "NIDAQ_SAMPLE_RATE_AI = 5000.0 # Hz\n",
    "CONTROL_NIR_FPS = 40.0\n",
    "SAVE_NIR_FPS = 20.0\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    n_img_nir = f['img_nir'].shape[0]\n",
    "    img_metadata = f['img_metadata']\n",
    "    img_timestamp = img_metadata['img_timestamp'][:]\n",
    "    img_id = img_metadata[\"img_id\"][:]\n",
    "    q_iter_save = img_metadata[\"q_iter_save\"][:]\n",
    "\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "# ai_stim = daqmx_ai[2,:].astype(np.float32)\n",
    "\n",
    "di_confocal_rise = np.where(np.diff(di_confocal) > 0.0)[0] + 1\n",
    "\n",
    "confocal_on_sample = np.where(np.diff(ai_laser) > 0.5)[0] + 1 # find rising edges (laser on)\n",
    "confocal_start_sample_ai = confocal_on_sample[0] # get first rising edge, which should correspond to confocal recording start\n",
    "confocal_off_sample = np.where(np.diff(ai_laser) < -0.5)[0] + 1 # find falling edges (laser off)\n",
    "confocal_stop_sample_ai = confocal_off_sample[1] # get second falling edge, which should correspond to confocal recording stop, first falling edge is at the beginning of the recording (see plots related to ai_laser)\n",
    "\n",
    "peaks, properties = find_peaks(\n",
    "    ai_piezo, \n",
    "    distance=100,\n",
    "    prominence=1.0\n",
    ") # find peaks in piezo signal, which should correspond to stack acquisition. distance is set to 400 samples\n",
    "stack_start_sample = peaks.copy() + 150 # +20 for visualization only\n",
    "stack_start_sample = np.insert(stack_start_sample, 0, confocal_start_sample_ai)  # prepend first element as confocal recording start\n",
    "stack_start_sample = stack_start_sample[:-1]  # delete last element\n",
    "\n",
    "stack_stop_sample = peaks.copy()\n",
    "stack_stop_sample[-1] = confocal_stop_sample_ai # set last stack stop to confocal recording stop\n",
    "piezo_timing = np.concatenate((stack_start_sample.reshape(-1,1), stack_stop_sample.reshape(-1,1)), axis=1) # combine piezo start and stop indices to get confocal stack timing\n",
    "\n",
    "# zoom = (0, 60000)\n",
    "# zoom = (3257743-100000,3257743)\n",
    "zoom = (0, len(ai_laser))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(15,6), sharex=True)\n",
    "axs[0].plot(ai_laser[zoom[0]:zoom[1]], label='Laser Signal', color='orange')\n",
    "axs[1].plot(ai_piezo[zoom[0]:zoom[1]], label='Piezo Signal', color='blue')\n",
    "axs[2].plot(di_confocal[zoom[0]:zoom[1]], label='Confocal Camera DI', color='green')\n",
    "axs[3].plot(di_nir[zoom[0]:zoom[1]], label='NIR Camera DI', color='magenta')\n",
    "\n",
    "# plot every 41st di_confocal_rise index as vertical lines to see if they align with confocal stack timing\n",
    "# this is the start of the next confocal stack\n",
    "for idx in di_confocal_rise[::41]:\n",
    "    if zoom[0] <= idx < zoom[1]:\n",
    "        axs[2].axvline(x=idx-zoom[0], color='black', linestyle='-')\n",
    "\n",
    "# plot confocal recording start and stop\n",
    "axs[0].axvline(x=confocal_start_sample_ai-zoom[0], color='black', linestyle='--')\n",
    "axs[0].axvline(x=confocal_stop_sample_ai-zoom[0], color='black', linestyle='--')\n",
    "\n",
    "# plot stack start and stops as vertical lines to see if they align with confocal stack timing\n",
    "for (start,stop) in zip(piezo_timing[:,0], piezo_timing[:,1]):\n",
    "    if zoom[0] <= start < zoom[1]:\n",
    "        axs[1].axvline(x=start-zoom[0], color='green', linestyle='-')\n",
    "    if zoom[0] <= stop < zoom[1]:\n",
    "        axs[1].axvline(x=stop-zoom[0], color='red', linestyle='-')\n",
    "\n",
    "plt.xlim(0, zoom[1]-zoom[0])\n",
    "plt.xlabel('Sample index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec07f0",
   "metadata": {},
   "source": [
    "### AI Confocal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ea419b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from scipy.stats import mode\n",
    "from sync import detect_nir_timing\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    ai_laser = f['daqmx_ai'][:][0,:].astype(np.float32)\n",
    "\n",
    "# get confocal recording start and stop times\n",
    "#   when ai_laser goes from 0 to 1, recording on\n",
    "#   when ai_laser goes from 1 to 0, recording off\n",
    "#   note: there is additional on/off signlas after recording off in ai_laser, this corresponds to nir recording, not getting those timestamps\n",
    "confocal_on_sample = np.where(np.diff(ai_laser) > 0.5)[0] + 1 # find rising edges (laser on)\n",
    "confocal_on_sample = confocal_on_sample[0] # get first rising edge, which should correspond to confocal recording start\n",
    "confocal_off_sample = np.where(np.diff(ai_laser) < -0.5)[0] + 1 # find falling edges (laser off)\n",
    "confocal_off_sample = confocal_off_sample[1] # get second falling edge, which should correspond to confocal recording stop, first falling edge is at the beginning of the recording (see plots related to ai_laser)\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(11, 4), sharex=True)\n",
    "axs[0].plot(ai_laser, label='AI confocal (Start)', color='orange')\n",
    "axs[0].plot(confocal_on_sample, ai_laser[confocal_on_sample], 'go', label='Confocal On')\n",
    "axs[0].plot(confocal_off_sample, ai_laser[confocal_off_sample], 'ro', label='Confocal Off')\n",
    "axs[0].legend()\n",
    "axs[1].plot(np.diff(ai_laser))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527a460",
   "metadata": {},
   "source": [
    "### DI Confocal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e82bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_path, 'r') as f:\n",
    "    di_confocal = f['daqmx_di'][:][0,:].astype(np.float32)\n",
    "\n",
    "# get stack start (rise) and stop (fall) indices from di_confocal , which is the digital input from the confocal camera that should indicate when each stack starts and stops.\n",
    "di_confocal_rise = np.where(np.diff(di_confocal) > 0.0)[0] + 1\n",
    "di_confocal_fall = np.where(np.diff(di_confocal) < 0.0)[0] + 1\n",
    "confocal_start_sample_di = di_confocal_rise[0] # recording start in confocal\n",
    "confocal_timing = np.concatenate((di_confocal_rise.reshape(-1,1), di_confocal_fall.reshape(-1,1)), axis=1) # combine confocal rise and fall indices to get confocal stack timing\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(11, 4), sharex=True)\n",
    "axs[0].plot(di_confocal, label='DI confocal', color='orange')\n",
    "axs[0].plot(confocal_start_sample_di, di_confocal[confocal_start_sample_di], 'go', label='Confocal On')\n",
    "axs[0].legend()\n",
    "axs[1].plot(np.diff(di_confocal))\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(11, 4), sharex=True)\n",
    "axs[0].plot(di_confocal, label='DI confocal', color='orange')\n",
    "axs[0].plot(di_confocal_rise, di_confocal[di_confocal_rise], 'go', label='Confocal Stack Start')\n",
    "axs[0].plot(di_confocal_fall, di_confocal[di_confocal_fall], 'ro', label='Confocal Stack Stop')\n",
    "axs[0].legend()\n",
    "axs[1].plot(np.diff(di_confocal))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dddc463",
   "metadata": {},
   "source": [
    "### DI NIR plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triggers detected (raw): 26027\n",
      "Total expected from img_id:     26045\n",
      "Mismatch:                       -18\n",
      "Saved frames (n_img_nir):       12826\n",
      "Sum of q_iter_save:             12826\n",
      "\n",
      "Large gaps (>500 samples) at indices: [26025]\n",
      "Gap sizes: [3648]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "N_STACK = 41\n",
    "\n",
    "NIDAQ_SAMPLE_RATE_AI = 5000.0 # Hz\n",
    "CONTROL_NIR_FPS = 40.0\n",
    "SAVE_NIR_FPS = 20.0\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    n_img_nir = f['img_nir'].shape[0]\n",
    "    img_metadata = f['img_metadata']\n",
    "    img_timestamp = img_metadata['img_timestamp'][:]\n",
    "    img_id = img_metadata[\"img_id\"][:]\n",
    "    q_iter_save = img_metadata[\"q_iter_save\"][:]\n",
    "\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "# ai_stim = daqmx_ai[2,:].astype(np.float32)\n",
    "img_timestamp_sec = (img_timestamp - img_timestamp[0]) / 1e9\n",
    "\n",
    "list_nir_on  = np.where(np.diff(di_nir) >  1)[0] + 1\n",
    "list_nir_off = np.where(np.diff(di_nir) < -1)[0] + 1\n",
    "\n",
    "total_expected = int(np.sum(np.diff(img_id)))\n",
    "\n",
    "print(f\"Total triggers detected (raw): {len(list_nir_on)}\")\n",
    "print(f\"Total expected from img_id:     {total_expected}\")\n",
    "print(f\"Mismatch:                       {len(list_nir_on) - total_expected}\")\n",
    "print(f\"Saved frames (n_img_nir):       {n_img_nir}\")\n",
    "print(f\"Sum of q_iter_save:             {np.sum(q_iter_save)}\")\n",
    "\n",
    "# Check the recording window detection\n",
    "gaps = np.diff(list_nir_on)\n",
    "large_gaps = np.where(gaps > 500)[0]\n",
    "print(f\"\\nLarge gaps (>500 samples) at indices: {large_gaps}\")\n",
    "print(f\"Gap sizes: {gaps[large_gaps]}\")\n",
    "\n",
    "# Plot to visually inspect\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(di_nir, color='magenta')\n",
    "for g in large_gaps:\n",
    "    plt.axvline(list_nir_on[g], color='red', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(list_nir_on[g+1], color='green', linestyle='--', alpha=0.7)\n",
    "plt.title('NIR digital signal with large gaps marked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolated detect nir timing code for testing\n",
    "%matplotlib qt\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "_ = importlib.reload(sys.modules['sync'])\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "N_STACK = 41\n",
    "\n",
    "NIDAQ_SAMPLE_RATE_AI = 5000.0 # Hz\n",
    "CONTROL_NIR_FPS = 40.0\n",
    "SAVE_NIR_FPS = 20.0\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    n_img_nir = f['img_nir'].shape[0]\n",
    "    img_metadata = f['img_metadata']\n",
    "    img_timestamp = img_metadata['img_timestamp'][:]\n",
    "    img_id = img_metadata[\"img_id\"][:]\n",
    "    q_iter_save = img_metadata[\"q_iter_save\"][:]\n",
    "\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "# ai_stim = daqmx_ai[2,:].astype(np.float32)\n",
    "img_timestamp_sec = (img_timestamp - img_timestamp[0]) / 1e9\n",
    "\n",
    "# Get recording window from ai_laser (your existing logic)\n",
    "confocal_on_sample  = np.where(np.diff(ai_laser) > 0.5)[0] + 1\n",
    "confocal_recording_start     = confocal_on_sample[0]\n",
    "confocal_off_sample = np.where(np.diff(ai_laser) < -0.5)[0] + 1\n",
    "confocal_recording_stop      = confocal_off_sample[1]\n",
    "\n",
    "\n",
    "# Find all trigger edges\n",
    "list_nir_on  = np.where(np.diff(di_nir) >  1)[0] + 1\n",
    "list_nir_off = np.where(np.diff(di_nir) < -1)[0] + 1\n",
    "\n",
    "nir_record_on  = np.diff(list_nir_on)  > 500\n",
    "nir_record_off = np.diff(list_nir_off) > 500\n",
    "\n",
    "# --- recording start ---\n",
    "if list_nir_on[0] > 500:\n",
    "    s_nir_start = list_nir_on[0]\n",
    "elif np.sum(nir_record_on) == 2:\n",
    "    s_nir_start = list_nir_on[np.where(nir_record_on)[0][0] + 1]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"More than 2 recording-on transitions detected for FLIR camera\"\n",
    "    )\n",
    "\n",
    "# --- recording stop ---\n",
    "if list_nir_off[-1] < len(di_nir) - 500:\n",
    "    s_nir_stop = list_nir_off[-1]\n",
    "elif np.sum(nir_record_off) <= 2:\n",
    "    s_nir_stop = list_nir_off[\n",
    "        np.where(np.diff(list_nir_off) > 500)[0][-1]\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"More than 2 recording-off transitions detected for FLIR camera\"\n",
    "    )\n",
    "    \n",
    "# Keep triggers inside the recording window (±5 samples tolerance)\n",
    "mask_on  = (s_nir_start - 5 < list_nir_on)  & (list_nir_on  < s_nir_stop + 5)\n",
    "mask_off = (s_nir_start - 5 < list_nir_off) & (list_nir_off < s_nir_stop + 5)\n",
    "list_nir_on  = list_nir_on[mask_on]\n",
    "list_nir_off = list_nir_off[mask_off]\n",
    "\n",
    "# looks good so far\n",
    "\n",
    "img_id_diff = np.diff(img_id).tolist() # img_id_diff , how many camera triggers happened between consecutive iterations. Usually 1, if >1, that means triggers fired but were not saved (frame skips)\n",
    "img_id_diff.insert(0, 1)\n",
    "total_expected = int(np.sum(np.diff(img_id)))\n",
    "\n",
    "if abs(len(list_nir_on) - total_expected) > 3:\n",
    "    raise ValueError(\n",
    "        f\"Detected trigger count ({len(list_nir_on)}) differs from \"\n",
    "        f\"image-ID count ({total_expected}) by more than 3\"\n",
    "    )\n",
    "else:\n",
    "    img_id_diff[-1] += len(list_nir_on) - total_expected - 1\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(di_nir, label='DI NIR', color='magenta')\n",
    "plt.plot(list_nir_on, di_nir[list_nir_on], 'go', label='NIR On')\n",
    "plt.plot(list_nir_off, di_nir[list_nir_off], 'ro', label='NIR Off')\n",
    "plt.axvline(confocal_recording_start, color='blue', linestyle='--', label='Confocal Recording Start')\n",
    "plt.axvline(confocal_recording_stop, color='red', linestyle='--', label='Confocal Recording Stop')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of samples between saved nir frames\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.diff(img_timestamp[q_iter_save.astype(bool)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "45378311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot di nir with img timestamps\n",
    "%matplotlib qt\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "N_STACK = 41\n",
    "\n",
    "NIDAQ_SAMPLE_RATE_AI = 5000.0 # Hz\n",
    "CONTROL_NIR_FPS = 40.0\n",
    "SAVE_NIR_FPS = 20.0\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    n_img_nir = f['img_nir'].shape[0]\n",
    "    img_metadata = f['img_metadata']\n",
    "    img_timestamp = img_metadata['img_timestamp'][:]\n",
    "    img_id = img_metadata[\"img_id\"][:]\n",
    "    q_iter_save = img_metadata[\"q_iter_save\"][:]\n",
    "\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "# ai_stim = daqmx_ai[2,:].astype(np.float32)\n",
    "\n",
    "# Find all trigger edges\n",
    "list_nir_on  = np.where(np.diff(di_nir) >  1)[0] + 1\n",
    "list_nir_off = np.where(np.diff(di_nir) < -1)[0] + 1\n",
    "\n",
    "nir_record_on  = np.diff(list_nir_on)  > 500\n",
    "nir_record_off = np.diff(list_nir_off) > 500\n",
    "\n",
    "# --- recording start ---\n",
    "if list_nir_on[0] > 500:\n",
    "    s_nir_start = list_nir_on[0]\n",
    "elif np.sum(nir_record_on) == 2:\n",
    "    s_nir_start = list_nir_on[np.where(nir_record_on)[0][0] + 1]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"More than 2 recording-on transitions detected for FLIR camera\"\n",
    "    )\n",
    "\n",
    "# --- recording stop ---\n",
    "if list_nir_off[-1] < len(di_nir) - 500:\n",
    "    s_nir_stop = list_nir_off[-1]\n",
    "elif np.sum(nir_record_off) <= 2:\n",
    "    s_nir_stop = list_nir_off[\n",
    "        np.where(np.diff(list_nir_off) > 500)[0][-1]\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"More than 2 recording-off transitions detected for FLIR camera\"\n",
    "    )\n",
    "    \n",
    "# Keep triggers inside the recording window (±5 samples tolerance)\n",
    "mask_on  = (s_nir_start - 5 < list_nir_on)  & (list_nir_on  < s_nir_stop + 5)\n",
    "mask_off = (s_nir_start - 5 < list_nir_off) & (list_nir_off < s_nir_stop + 5)\n",
    "list_nir_on  = list_nir_on[mask_on]\n",
    "list_nir_off = list_nir_off[mask_off]\n",
    "\n",
    "\n",
    "\n",
    "img_timestamp_sec = (img_timestamp - img_timestamp[0]) / 1e9 + (list_nir_on[0]/NIDAQ_SAMPLE_RATE_AI) # 0.1617 #+ 0.1798\n",
    "\n",
    "img_timestamp_sec_saved = img_timestamp_sec[q_iter_save.astype(bool)]\n",
    "\n",
    "time = np.arange(len(di_nir)) / NIDAQ_SAMPLE_RATE_AI\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time, di_nir, label='DI NIR', color='magenta')\n",
    "# plot vertical lines for img_timestamp_sec\n",
    "for ts in img_timestamp_sec_saved:\n",
    "    plt.axvline(ts, color='blue', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('DI NIR')\n",
    "plt.title('NIR Digital Signal with Image Timestamps')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e4208590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1617"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".1874 - 0.0257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fecc84",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9721f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded daqmx_ai with shape: (3, 2029572)\n",
      "Loaded daqmx_di with shape: (2, 2029588)\n",
      "img_id with shape: (15948,)\n",
      "img_timestamp with shape: (15948,)\n",
      "q_iter_save with shape: (15948,)\n",
      "Number of NIR images: 7974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(confocal_timing.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m    121\u001b[39m     confocal_start, confocal_stop = confocal_timing[j]\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m confocal_start <= nir_start <= confocal_stop:\n\u001b[32m    123\u001b[39m         nir_to_confocal_trigger_number[i] = j + \u001b[32m1\u001b[39m \u001b[38;5;66;03m# add 1 to make it 1-based indexing like in Julia\u001b[39;00m\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# main code\n",
    "\n",
    "from scipy.stats import mode\n",
    "from sync import detect_nir_timing\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "N_STACK = 41\n",
    "\n",
    "NIDAQ_SAMPLE_RATE_AI = 5000.0 # Hz\n",
    "CONTROL_NIR_FPS = 40.0\n",
    "SAVE_NIR_FPS = 20.0\n",
    "\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "# ai_stim = daqmx_ai[2,:].astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    daqmx_ai = f['daqmx_ai'][:]\n",
    "    daqmx_di = f['daqmx_di'][:]\n",
    "    n_img_nir = f['img_nir'].shape[0]\n",
    "    img_metadata = f['img_metadata']\n",
    "    img_timestamp = img_metadata['img_timestamp'][:]\n",
    "    img_id = img_metadata[\"img_id\"][:]\n",
    "    q_iter_save = img_metadata[\"q_iter_save\"][:]\n",
    "        \n",
    "print(f\"Loaded daqmx_ai with shape: {daqmx_ai.shape}\")\n",
    "print(f\"Loaded daqmx_di with shape: {daqmx_di.shape}\")\n",
    "print(f\"img_id with shape: {img_id.shape}\")\n",
    "print(f\"img_timestamp with shape: {img_timestamp.shape}\")\n",
    "print(f\"q_iter_save with shape: {q_iter_save.shape}\")\n",
    "print(f\"Number of NIR images: {n_img_nir}\")\n",
    "\n",
    "di_confocal = daqmx_di[0,:].astype(np.float32)\n",
    "di_nir = daqmx_di[1,:].astype(np.float32)\n",
    "ai_laser = daqmx_ai[0,:].astype(np.float32)\n",
    "ai_piezo = daqmx_ai[1,:].astype(np.float32)\n",
    "\n",
    "\n",
    "# # get confocal recording start and stop times\n",
    "# #   when ai_laser goes from 0 to 1, recording on\n",
    "# #   when ai_laser goes from 1 to 0, recording off\n",
    "# #   note: there is additional on/off signlas after recording off in ai_laser, this corresponds to nir recording, not getting those timestamps\n",
    "# confocal_on_sample = np.where(np.diff(ai_laser) > 0.5)[0] + 1 # find rising edges (laser on)\n",
    "# confocal_start_sample_ai = confocal_on_sample[0] # get first rising edge, which should correspond to confocal recording start\n",
    "# confocal_off_sample = np.where(np.diff(ai_laser) < -0.5)[0] + 1 # find falling edges (laser off)\n",
    "# confocal_stop_sample_ai = confocal_off_sample[1] # get second falling edge, which should correspond to confocal recording stop, first falling edge is at the beginning of the recording (see plots related to ai_laser)\n",
    "\n",
    "# get stack start (rise) and stop (fall) indices from di_confocal , which is the digital input from the confocal camera that should indicate when each stack starts and stops.\n",
    "di_confocal_rise = np.where(np.diff(di_confocal) > 0.0)[0] + 1\n",
    "di_confocal_fall = np.where(np.diff(di_confocal) < 0.0)[0] + 1\n",
    "confocal_start_sample_di = di_confocal_rise[0] # recording start in confocal\n",
    "confocal_stop_sample_di = di_confocal_fall[-1] # recording stop in confocal, should be the last falling edge\n",
    "confocal_timing = np.concatenate((di_confocal_rise.reshape(-1,1), di_confocal_fall.reshape(-1,1)), axis=1) # combine confocal rise and fall indices to get confocal stack timing\n",
    "\n",
    "# # confocal_start_idx from di should be the same as confocal_start_idx from ai_laser, which is what we were using before. let's check that.\n",
    "# assert (confocal_start_sample_ai - confocal_start_sample_di) == 0, \"Confocal start index from DI and AI do not match\"\n",
    "\n",
    "# find piezo start stop (fall) times from ai_piezo. ai_piezo is a ramping signal that indicates when the piezo is moving, which should correspond to when the confocal camera is acquiring a stack. we can use this to check if the confocal stack timing from di_confocal matches the piezo movement timing from ai_piezo.\n",
    "# need to first find the peaks using a peak finding algorithm, since the piezo signal is not a perfect ramp and has some noise. we can use scipy.signal.find_peaks for this.\n",
    "peaks, properties = find_peaks(\n",
    "    ai_piezo, \n",
    "    distance=100,\n",
    "    prominence=1.0\n",
    ") # find peaks in piezo signal, which should correspond to stack acquisition. distance is set to 400 samples\n",
    "stack_start_sample = peaks.copy()\n",
    "stack_start_sample = np.insert(stack_start_sample, 0, confocal_start_sample_di)  # prepend first element as confocal recording start\n",
    "stack_start_sample = stack_start_sample[:-1]  # delete last element\n",
    "\n",
    "stack_stop_sample = peaks.copy()\n",
    "stack_stop_sample[-1] = confocal_stop_sample_di # set last stack stop to confocal recording stop\n",
    "piezo_timing = np.concatenate((stack_start_sample.reshape(-1,1), stack_stop_sample.reshape(-1,1)), axis=1) # combine piezo start and stop indices to get confocal stack timing\n",
    "\n",
    "\n",
    "# get nir recording start (rise) and stop (fall) indices from di_nir, which is the digital input from the NIR camera that should indicate when each NIR recording starts and stops.\n",
    "nir_timing = detect_nir_timing(di_nir, img_id, q_iter_save, n_img_nir)\n",
    "di_nir_rise = nir_timing[:, 0]\n",
    "di_nir_fall = nir_timing[:, 1]\n",
    "# di_nir_rise = np.where(np.diff(di_nir) > 0.5)[0]\n",
    "# di_nir_fall = np.where(np.diff(di_nir) < -0.5)[0]\n",
    "nir_start_idx = di_nir_rise[0] # recording start in NIR\n",
    "\n",
    "# validated this with working recording, that passed detect_nir_timing's checks\n",
    "# for recordings that don't pass, might have to trim out some img_timstamps at end of recording that seem to be artifacts, but not totally sure\n",
    "img_timestamp_sec = (img_timestamp - img_timestamp[0]) / 1e9 + (nir_start_idx/NIDAQ_SAMPLE_RATE_AI)\n",
    "img_timestamp_sec_saved = img_timestamp_sec[q_iter_save.astype(bool)]\n",
    "\n",
    "\n",
    "# # we have confocal_timing, piezo_timing, nir_timing\n",
    "# let's  now map them\n",
    "\n",
    "# 1. Confocal to Piezo mapping\n",
    "confocal_timing_stack_number = map_timestamps(confocal_timing, piezo_timing)\n",
    "\n",
    "# 2. NIR to Piezo mapping\n",
    "nir_timing_stack_number = map_timestamps(nir_timing, piezo_timing)\n",
    "\n",
    "# 3. NIR to Confocal mapping\n",
    "nir_to_confocal_trigger_number = map_timestamps(nir_timing, confocal_timing)\n",
    "\n",
    "# 4. Plane number (using the function we built previously)\n",
    "confocal_plane = generate_consecutive_counts(confocal_timing_stack_number)\n",
    "\n",
    "\n",
    "# # for each row in confocal_timing, determine the row in piezo_timing that it falls into. this will give us the confocal to piezo mapping, which we can use to check if the confocal stack timing from di_confocal matches the piezo movement timing from ai_piezo.\n",
    "# confocal_timing_stack_number = np.full(confocal_timing.shape[0], np.nan, dtype=float)\n",
    "# for i in range(confocal_timing.shape[0]):\n",
    "#     confocal_start, confocal_stop = confocal_timing[i]\n",
    "#     for j in range(piezo_timing.shape[0]):\n",
    "#         piezo_start, piezo_stop = piezo_timing[j]\n",
    "#         if piezo_start <= confocal_start <= piezo_stop:\n",
    "#             confocal_timing_stack_number[i] = j + 1 # add 1 to make it 1-based indexing like in Julia\n",
    "#             break\n",
    "\n",
    "# # for each row in nir_timing, determine the row in piezo_timing that it falls into. this will give us the nir to piezo mapping, which we can use to check if the nir recording timing from di_nir matches the piezo movement timing from ai_piezo.\n",
    "# nir_timing_stack_number = np.full(nir_timing.shape[0], np.nan, dtype=float)\n",
    "# for i in range(nir_timing.shape[0]):\n",
    "#     nir_start, nir_stop = nir_timing[i]\n",
    "#     for j in range(piezo_timing.shape[0]):\n",
    "#         piezo_start, piezo_stop = piezo_timing[j]\n",
    "#         if piezo_start <= nir_start <= piezo_stop:\n",
    "#             nir_timing_stack_number[i] = j + 1 # add 1 to make it 1-based indexing like in Julia\n",
    "#             break\n",
    "\n",
    "# # for each row in nir_timing, determine the row in confocal_timing that it falls into. this will give us the nir to confocal mapping, which we can use to check if the nir recording timing from di_nir matches the confocal stack timing from di_confocal.\n",
    "# nir_to_confocal_trigger_number = np.full(nir_timing.shape[0], np.nan, dtype=float)\n",
    "# for i in range(nir_timing.shape[0]):\n",
    "#     nir_start, nir_stop = nir_timing[i]\n",
    "#     for j in range(confocal_timing.shape[0]):\n",
    "#         confocal_start, confocal_stop = confocal_timing[j]\n",
    "#         if confocal_start <= nir_start <= confocal_stop:\n",
    "#             nir_to_confocal_trigger_number[i] = j + 1 # add 1 to make it 1-based indexing like in Julia\n",
    "#             break\n",
    "    \n",
    "# make a new array that is just confocal_timing_stack_number, but each set of rows that correspond to the same stack number get incremented starting with 0, and it resets at each stack number\n",
    "# this is just the plane number\n",
    "confocal_plane = generate_consecutive_counts(confocal_timing_stack_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8062d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "zoom = (0, daqmx_ai.shape[1])\n",
    "# zoom = (3257743-100000,3257743)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(15,6), sharex=True)\n",
    "axs[0].plot(ai_laser[zoom[0]:zoom[1]], label='Laser Signal', color='orange')\n",
    "# plot laser signal start idx\n",
    "axs[0].plot(confocal_start_sample_di - zoom[0], ai_laser[confocal_start_sample_di], 'o', label='Confocal Start (DI)', color='red')\n",
    "axs[1].plot(ai_piezo[zoom[0]:zoom[1]], label='Piezo Signal', color='blue')\n",
    "# axs[2].plot(ai_stim[zoom[0]:zoom[1]], label='Stimulus Signal', color='magenta')\n",
    "\n",
    "axs[2].plot(di_confocal[zoom[0]:zoom[1]], label='Confocal Camera DI', color='green')\n",
    "# plot confocal start idx\n",
    "axs[2].plot(confocal_start_sample_di - zoom[0], di_confocal[confocal_start_sample_di], 'o', label='Confocal Start (DI)', color='red')\n",
    "# plot confocal rise and fall times\n",
    "axs[2].plot(di_confocal_rise - zoom[0], di_confocal[di_confocal_rise], 'x', label='Confocal Rise (DI)', color='red')\n",
    "axs[2].plot(di_confocal_fall - zoom[0], di_confocal[di_confocal_fall], 'x', label='Confocal Fall (DI)', color='red')\n",
    "\n",
    "axs[3].plot(di_nir[zoom[0]:zoom[1]], label='NIR Camera DI', color='magenta')\n",
    "# # plot nir rise and fall times\n",
    "# axs[3].plot(di_nir_rise - zoom[0], di_nir[di_nir_rise], 'x', label='NIR Rise (DI)', color='black')\n",
    "# axs[3].plot(di_nir_fall - zoom[0], di_nir[di_nir_fall], 'x', label='NIR Fall (DI)', color='black')\n",
    "\n",
    "# # plot piezo peaks\n",
    "# # find number of peaks within zoom range\n",
    "# num_peaks_in_zoom = np.sum((peaks >= zoom[0]) & (peaks <= zoom[1]))\n",
    "# axs[1].plot(peaks[0:num_peaks_in_zoom]-zoom[0], ai_piezo[peaks[0:num_peaks_in_zoom]], 'o', label='Piezo End', color='red')\n",
    "# # plot piezo start from peak\n",
    "# axs[1].plot(np.array(piezo_start_idx_from_peak[0:num_peaks_in_zoom])-zoom[0], ai_piezo[piezo_start_idx_from_peak[0:num_peaks_in_zoom]], 'x', label='Piezo Start', color='red')\n",
    "# for ax in axs:\n",
    "#     ax.legend(loc='upper right', bbox_to_anchor=(1.35, 1), fontsize=12)\n",
    "#     ax.set_ylabel('Signal')\n",
    "# axs[-1].set_xlabel('Sample Index')\n",
    "# # plt.suptitle('Raw DAQ Signals (used for sync)', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, zoom[1]-zoom[0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f54587",
   "metadata": {},
   "source": [
    "## MAKE VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0adc7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shape:  (7974, 732, 968)\n",
      "Recording duration (s):  398.7\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_path, 'r') as f:\n",
    "    im = f.get('img_nir')[:] # THW\n",
    "sz = im.shape\n",
    "\n",
    "fps = 20.0  # frames per second\n",
    "\n",
    "print('Video shape: ', sz)\n",
    "\n",
    "nframes = im.shape[0]\n",
    "record_duration = nframes / fps # in seconds\n",
    "\n",
    "print('Recording duration (s): ', record_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86443012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving frame:  100  /  7974\n",
      "Saving frame:  200  /  7974\n",
      "Saving frame:  300  /  7974\n",
      "Saving frame:  400  /  7974\n",
      "Saving frame:  1100  /  7974\n",
      "Saving frame:  1300  /  7974\n",
      "Saving frame:  1800  /  7974\n",
      "Saving frame:  1900  /  7974\n",
      "Saving frame:  2000  /  7974\n",
      "Saving frame:  2100  /  7974\n",
      "Saving frame:  2200  /  7974\n",
      "Saving frame:  2300  /  7974\n",
      "Saving frame:  2400  /  7974\n",
      "Saving frame:  2500  /  7974\n",
      "Saving frame:  2600  /  7974\n",
      "Saving frame:  2700  /  7974\n",
      "Saving frame:  2800  /  7974\n",
      "Saving frame:  2900  /  7974\n",
      "Saving frame:  3000  /  7974\n",
      "Saving frame:  3300  /  7974\n",
      "Saving frame:  3400  /  7974\n",
      "Saving frame:  3500  /  7974\n",
      "Saving frame:  3600  /  7974\n",
      "Saving frame:  3700  /  7974\n",
      "Saving frame:  3800  /  7974\n",
      "Saving frame:  3900  /  7974\n",
      "Saving frame:  4000  /  7974\n",
      "Saving frame:  4600  /  7974\n",
      "Saving frame:  4700  /  7974\n",
      "Saving frame:  4800  /  7974\n",
      "Saving frame:  4900  /  7974\n",
      "Saving frame:  5000  /  7974\n",
      "Saving frame:  5100  /  7974\n",
      "Saving frame:  5400  /  7974\n",
      "Saving frame:  5500  /  7974\n",
      "Saving frame:  5600  /  7974\n",
      "Saving frame:  5700  /  7974\n",
      "Saving frame:  6000  /  7974\n",
      "Saving frame:  6300  /  7974\n",
      "Saving frame:  6400  /  7974\n",
      "Saving frame:  6500  /  7974\n",
      "Saving frame:  6600  /  7974\n",
      "Saving frame:  6700  /  7974\n",
      "Saving frame:  6800  /  7974\n",
      "Saving frame:  6900  /  7974\n",
      "Saving frame:  7000  /  7974\n",
      "Saving frame:  7100  /  7974\n",
      "Saving frame:  7200  /  7974\n",
      "Saving frame:  7300  /  7974\n",
      "Saving frame:  7400  /  7974\n",
      "Saving frame:  7600  /  7974\n",
      "Saving frame:  7700  /  7974\n",
      "Saving frame:  7800  /  7974\n",
      "Saving frame:  7900  /  7974\n",
      "Saved video to:  nir_video.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save im, which is of shape (frame, height, width) as a .mp4 video\n",
    "import cv2\n",
    "out_fn = 'nir_video.mp4'\n",
    "if not os.path.exists(PTH):\n",
    "    os.makedirs(PTH)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(out_fn, fourcc, fps, (sz[2], sz[1]), isColor=False)\n",
    "# save video\n",
    "for i in range(nframes):\n",
    "# for i in range(500):\n",
    "    # skip frame if recording has not started based on confocal_timing and nir_timing\n",
    "    nir_start, nir_end = nir_timing[i, 0], nir_timing[i, 1]\n",
    "    if nir_start < confocal_timing[0, 0]:\n",
    "        continue\n",
    "\n",
    "    # find the confocal_timing row for this nir frame\n",
    "    confocal_row = np.where((confocal_timing[:,0] <= nir_start) & (confocal_timing[:,1] >= nir_start))[0]\n",
    "    if len(confocal_row) == 0:\n",
    "        continue\n",
    "    confocal_row = confocal_row[0]\n",
    "    \n",
    "    stack_number = confocal_timing_stack_number[confocal_row]\n",
    "    plane_number = confocal_plane[confocal_row]\n",
    "\n",
    "    \n",
    "\n",
    "    # print status every 100 frames\n",
    "    if i % 100 == 0:\n",
    "        print('Saving frame: ', i, ' / ', nframes)\n",
    "    frame = im[i,:,:]\n",
    "    frame = (frame / np.max(frame) * 255).astype(np.uint8)\n",
    "    # overlay frame with text of frame number and time in seconds\n",
    "    time_sec = i / fps\n",
    "    text = f'Frame: {i}  Time: {time_sec:.2f} sec  Stack: {stack_number if not np.isnan(stack_number) else \"N/A\"}  Plane: {plane_number if not np.isnan(plane_number) else \"N/A\"} '\n",
    "    # text = f'Frame: {i}  Time: {time_sec:.2f} sec  Stack: {confocal_timing_stack_number[i] if i < len(confocal_timing_stack_number) else \"N/A\"}  Plane: {confocal_plane[i] if i < len(confocal_plane) else \"N/A\"} '\n",
    "    cv2.putText(frame, text, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255), 2)\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "    out.write(frame)\n",
    "\n",
    "\n",
    "\n",
    "out.release()\n",
    "print('Saved video to: ', out_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffe584",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- put all this code together to deliver a synced recording\n",
    "  - will need to figure out what params, data_dicts, and outputs from get_timing_info in the antsun notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
