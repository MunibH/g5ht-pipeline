{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1b7a0b",
   "metadata": {},
   "source": [
    "# G5HT-PIPELINE\n",
    "\n",
    "## TODO:\n",
    "\n",
    "1. I wonder if I computed a spline on each and every z slice and warped each, oriented each of them, and warped each of them, if the problem of weirdly sheared image stacks would be solved\n",
    "2. quick mp4 for all recordings\n",
    "   1. now working in engaging, works per one nd2 sbatch\n",
    "3. focus check for all recordings\n",
    "   1. maybe focus check can be used to specify which z slices are good to use and which frames are good to use\n",
    "4. for recordings starting in december 2025, need to trim first 2 rather than last 2 z slices\n",
    "5. flip worms so that VNC is always up\n",
    "6. fixed mask could be automated, but if not, make sure to save which index is fixed\n",
    "7. extract behavior\n",
    "8. posture similarity\n",
    "   1. posture might consist of the spline + thresholded z-stack\n",
    "      1. I'm thinking that the orientation shouldn't matter, but the z-planes in focus will, and curvature/spline of the head will\n",
    "      2. maybe need to actually interpolate to 117 z slices\n",
    "   2. sub registration problems\n",
    "   3. label each set of registered frames with one set of ROIs, or auto segment ROIs from each set of registered frames\n",
    "9.  track z over time, which zslices are consistent\n",
    "   1. focus + correlation\n",
    "10. beads -> train/test\n",
    "11. gfp+1 relative to rfp channel (might only apply to pre december 2025 recordings)\n",
    "12. wholistic \n",
    "    1.  parameter sweep, might change\n",
    "    2.  python version\n",
    "    3.  actually, wholistic might be tricky to use all the time, because it only works after parameter optimization, which I don't really know how to automate\n",
    "13. autocorr/scorr\n",
    "14. automate z slice trimming\n",
    "    1.  pre december 2025 (trim last 2 z slices)\n",
    "    2.  post december 2025 (trim first z slice)\n",
    "15. photobleaching estimation?\n",
    "    1.  record immo with serotonin\n",
    "    2.  at least do it for RFP\n",
    "16. try deltaF/F [ (F(t) - F0) / F0 ]\n",
    "18. maybe remove right-most part of the worm where spline is usually kinked\n",
    "19. port everything to engaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c1c6a",
   "metadata": {},
   "source": [
    "## CONDA ENVIRONMENTS\n",
    "\n",
    "For steps __1. preprocess__ and __2. mip__, `conda activate g5ht-pipeline`\n",
    "\n",
    "For step __3. segment__, `conda activate segment-torch` or `conda activate torchcu129`\n",
    "\n",
    "For step __4. spline, 5. orient, 6. warp, 7. reg__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e087d8",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21220aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import utils\n",
    "    is_torch_env = False\n",
    "except ImportError:\n",
    "    is_torch_env = True\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a415d0",
   "metadata": {},
   "source": [
    "## SPECIFY DATA TO PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfdcf1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004.nd2\n",
      "Num z-slices:  41\n",
      "Number of frames:  1200\n",
      "Height:  512\n",
      "width:  512\n",
      "Number of channels:  2\n",
      "Beads alignment file:  D:\\DATA\\g5ht-free\\20260123\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004_chan_alignment.nd2\n"
     ]
    }
   ],
   "source": [
    "# DATA_PTH = r'C:\\Users\\munib\\POSTDOC\\DATA\\fluorescent_beads_ch_align\\20251219'\n",
    "DATA_PTH = r'D:\\DATA\\g5ht-free\\20260123'\n",
    "\n",
    "INPUT_ND2 = 'date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004.nd2'\n",
    "\n",
    "INPUT_ND2_PTH = os.path.join(DATA_PTH, INPUT_ND2)\n",
    "\n",
    "NOISE_PTH = r'C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\noise\\noise_042925.tif'\n",
    "\n",
    "OUT_DIR = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "STACK_LENGTH = 41\n",
    "\n",
    "if not is_torch_env:\n",
    "    noise_stack = utils.get_noise_stack(NOISE_PTH, STACK_LENGTH)\n",
    "    num_frames, height, width, num_channels = utils.get_range_from_nd2(INPUT_ND2_PTH, stack_length=STACK_LENGTH) \n",
    "    beads_alignment_file = utils.get_beads_alignment_file(INPUT_ND2_PTH)\n",
    "else:\n",
    "    print(\"utils not loaded because conda environment doesn't have nd2reader installed. probably using torchcu129 env, which is totally fine for just doing the segmentation step\")\n",
    "\n",
    "print(INPUT_ND2)\n",
    "print('Num z-slices: ', STACK_LENGTH)\n",
    "if not is_torch_env:\n",
    "    print('Number of frames: ', num_frames)\n",
    "    print('Height: ', height)\n",
    "    print('width: ', width)\n",
    "    print('Number of channels: ', num_channels)\n",
    "    print('Beads alignment file: ', beads_alignment_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5702d46",
   "metadata": {},
   "source": [
    "## 0. PROCESS BEADS ALIGNMENT DATA (OPTIONAL, CHANGING THIS SO BEADS ARE PROCESSED SEAMLESSLY IN PIPELINE)\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "The registration parameters between green and red channels will be applied to worm recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d8333",
   "metadata": {},
   "source": [
    "### SHEAR CORRECT AND CHANNEL REGISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_parallel import main as preprocess_nd2_parallel\n",
    "_ = importlib.reload(sys.modules['preprocess_parallel'])\n",
    "\n",
    "num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "\n",
    "# # command-line arguments\n",
    "sys.argv = [\"\", beads_alignment_file, \"0\", str(num_frames_beads-1), NOISE_PTH, STACK_LENGTH, 5, num_frames_beads, height, width, num_channels]\n",
    "\n",
    "# # Call the main function\n",
    "preprocess_nd2_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57a3e8",
   "metadata": {},
   "source": [
    "### MIP\n",
    "\n",
    "This step saved the median channel registration parameters, need to do this somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mip import main as mip\n",
    "\n",
    "_ = importlib.reload(sys.modules['mip'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", beads_alignment_file, STACK_LENGTH, num_frames_beads, 2]\n",
    "\n",
    "# Call the main function\n",
    "mip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d28b53",
   "metadata": {},
   "source": [
    "## 1. SHEAR CORRECTION\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- shear corrects each volume\n",
    "  - depending on each exposure time, it can take roughly half a second between the first and last frames of a volume, so any movements need to be corrected for\n",
    "- creates one `.tif` for each volume and stores it in the `shear_corrected` directory\n",
    "\n",
    "##### TODO: should probably update stack length after shear correction since we cut it by 2, although not sure it's explicitly needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shear_correct\n",
    "_ = importlib.reload(sys.modules['shear_correct'])\n",
    "\n",
    "start_index = \"0\"\n",
    "end_index = str(num_frames-1)\n",
    "# start_index = \"800\"\n",
    "# end_index = \"803\"\n",
    "# cpu_count = str(int(os.cpu_count() / 2))\n",
    "cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "# sys.argv = [\"\", nd2 file, start_frame, end_frame, noise_pth, stack_length, n_workers, num_frames, height, width, num_channels]\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, start_index, end_index, NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels]\n",
    "\n",
    "# Call the main function\n",
    "shear_correct.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357bd32",
   "metadata": {},
   "source": [
    "## 2. CHANNEL ALIGNMENT\n",
    "\n",
    "` conda activate g5ht-pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0617b",
   "metadata": {},
   "source": [
    "### 2a. GET MEDIAN CHANNEL ALIGNMENT PARAMETERS FROM ALL FRAMES\n",
    "\n",
    "- If channel alignment file found, uses that, if not uses worm recording\n",
    "- creates a `.txt` file for each volume that contains elastix channel registration parameters\n",
    "- creates `chan_align_params.csv` and  `chan_align.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beads_alignment_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87844a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_channel_alignment\n",
    "import median_channel_alignment\n",
    "_ = importlib.reload(sys.modules['get_channel_alignment'])\n",
    "_ = importlib.reload(sys.modules['median_channel_alignment'])\n",
    "\n",
    "## set beads_alignment_file to None to use worm recording for channel alignment, even if beads file exists\n",
    "# beads_alignment_file = None\n",
    "\n",
    "start_index = \"0\"\n",
    "# cpu_count = str(int(os.cpu_count() / 2))\n",
    "cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "if beads_alignment_file is not None:\n",
    "    align_with_beads = True\n",
    "    num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "    sys.argv = [\"\", beads_alignment_file, start_index, str(num_frames_beads-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames_beads, height, width, num_channels, align_with_beads]\n",
    "else:\n",
    "    align_with_beads = False\n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, str(num_frames-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads]\n",
    "\n",
    "# # Call the main function\n",
    "get_channel_alignment.main()\n",
    "median_channel_alignment.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe582c3e",
   "metadata": {},
   "source": [
    "### 2b. APPLY MEDIAN CHANNEL ALIGNMENT PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b97453",
   "metadata": {},
   "source": [
    "- ouputs aligned volumes in `channel_aligned` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apply_channel_alignment\n",
    "_ = importlib.reload(sys.modules['apply_channel_alignment'])\n",
    "\n",
    "start_index = \"0\"\n",
    "# cpu_count = str(int(os.cpu_count() / 2))\n",
    "cpu_count = str(int(os.cpu_count()))\n",
    "\n",
    "# 0786 to 0799 are bad frames in worm005.nd2, copied 0785 for each of those frames\n",
    "\n",
    "if beads_alignment_file is not None:\n",
    "    align_with_beads = True\n",
    "    num_frames_beads, _, _, _ = utils.get_range_from_nd2(beads_alignment_file, stack_length=STACK_LENGTH) \n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, str(num_frames-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads, beads_alignment_file]\n",
    "else:\n",
    "    align_with_beads = False\n",
    "    sys.argv = [\"\", INPUT_ND2_PTH, start_index, str(num_frames-1), NOISE_PTH, STACK_LENGTH, cpu_count, num_frames, height, width, num_channels, align_with_beads]\n",
    "\n",
    "\n",
    "# Call the main function\n",
    "apply_channel_alignment.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0aece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create copies of 0785 and rename it to 0786 to 0799\n",
    "# import shutil\n",
    "# for i in range(786, 800):\n",
    "#     shutil.copyfile(r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251223\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm005\\channel_aligned\\0785.tif',\n",
    "#                     r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251223\\date-20251223_strain-ISg5HT_condition-starvedpatch_worm005\\channel_aligned\\{:04d}.tif'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc706e",
   "metadata": {},
   "source": [
    "### 2c. PLOT CHANNEL ALIGNMENT PARAMETER DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708547d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# make font sizes larger for visibility\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "try:\n",
    "    out_dir = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "    df = pd.read_csv(os.path.join(out_dir, 'chan_align_params.csv'))\n",
    "    params = ['TransformParameter_0', 'TransformParameter_1', 'TransformParameter_2', 'TransformParameter_3', 'TransformParameter_4', 'TransformParameter_5']\n",
    "    labels = ['Rx', 'Ry', 'Rz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "    # the xaxis limits for each subplot should be the same across figures\n",
    "\n",
    "    xlims = np.zeros((6,2))\n",
    "\n",
    "    plt.figure(figsize=(12,8), tight_layout=True)\n",
    "    for i,param in enumerate(params):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.hist(df[param], bins=30, color='red', alpha=0.6)\n",
    "        # plot the median value as a vertical line\n",
    "        median_value = df[param].median()\n",
    "        plt.axvline(median_value, color='black', linestyle='dashed', linewidth=2)\n",
    "        plt.xlabel(labels[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        # get xaxis limits\n",
    "        xlims[i,:] = plt.xlim()\n",
    "        # title is median value\n",
    "        plt.title(f'Median: {np.round(median_value,3)}', fontsize=14)\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"No chan_align_params.csv found for worm recording\")\n",
    "\n",
    "out_dir = os.path.splitext(INPUT_ND2_PTH)[0] + '_chan_alignment'\n",
    "df = pd.read_csv(os.path.join(out_dir, 'chan_align_params.csv'))\n",
    "params = ['TransformParameter_0', 'TransformParameter_1', 'TransformParameter_2', 'TransformParameter_3', 'TransformParameter_4', 'TransformParameter_5']\n",
    "labels = ['Rx', 'Ry', 'Rz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "plt.figure(figsize=(12,8), tight_layout=True)\n",
    "for i,param in enumerate(params):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist(df[param], bins=30, color='blue', alpha=0.6)\n",
    "    # plot the median value as a vertical line\n",
    "    median_value = df[param].median()\n",
    "    plt.axvline(median_value, color='black', linestyle='dashed', linewidth=2)\n",
    "    plt.xlabel(labels[i])\n",
    "    plt.ylabel('Frequency')\n",
    "    # apply xlims\n",
    "    # plt.xlim(xlims[i,0], xlims[i,1])\n",
    "    # title is median value, font size 14\n",
    "    plt.title(f'Median: {np.round(median_value,3)}', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f4640",
   "metadata": {},
   "source": [
    "## 3. BLEACH CORRECTION\n",
    "\n",
    "TODO:\n",
    "- per z slice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b283de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import bleach_correct\n",
    "_ = importlib.reload(sys.modules['bleach_correct'])\n",
    "\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "REG_DIR = 'channel_aligned' # 'channel_aligned' or 'tif' \n",
    "channels = 1\n",
    "method = 'block' # 'block' or 'exponential'\n",
    "mode = 'total' # 'total' or 'median'\n",
    "\n",
    "bleach_correct.correct_bleaching(os.path.join(PTH,REG_DIR), channels=channels, method=method, fbc=0.04, intensity_mode=mode)\n",
    "\n",
    "\n",
    "# # Correct RFP only with block method (default)\n",
    "# correct_bleaching(\"path/to/data\")\n",
    "\n",
    "# # Correct both channels with exponential fit\n",
    "# correct_bleaching(\"path/to/data\", channels=[0, 1], method='exponential')\n",
    "\n",
    "# # Command line\n",
    "# python bleach_correct.py path/to/data --channels 0 1 --method exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a097705",
   "metadata": {},
   "source": [
    "## 4. MIP\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `means.png`, `focus.png`, `mip.tif`, and `mip.mp4`, `focus_check.csv`\n",
    "\n",
    "##### TODO: \n",
    "- legend for focus.png, should be frame#\n",
    "- mip for xy, xz, zy\n",
    "- mip for several slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mip\n",
    "\n",
    "_ = importlib.reload(sys.modules['mip'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "framerate = 8\n",
    "tif_dir = 'bleach_corrected_RFP_block' # one of 'shear_corrected' 'channel_aligned' 'bleach_corrected_RFP_block'\n",
    "# tif_dir = 'channel_aligned_beads'\n",
    "rmax = 850\n",
    "gmax = 150\n",
    "mp4_quality = 10\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, tif_dir, STACK_LENGTH, num_frames, framerate, rmax, gmax, mp4_quality]\n",
    "\n",
    "# Call the main function\n",
    "mip.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563861dd",
   "metadata": {},
   "source": [
    "## 5 DRIFT ESTIMATION\n",
    "\n",
    "` conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs  `z_selection.csv`, `z_selection_diagnostics.png`, `sharpness.csv`\n",
    "\n",
    "TODO:\n",
    "- use z selection going forward\n",
    "- also use sharpness/focus (and other things) to determine good/bad frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drift_estimation\n",
    "\n",
    "_ = importlib.reload(sys.modules['drift_estimation'])\n",
    "_ = importlib.reload(sys.modules['utils'])\n",
    "\n",
    "# command-line arguments\n",
    "tif_dir = 'bleach_corrected_RFP_block' # one of 'shear_corrected' 'channel_aligned' 'bleach_corrected_RFP_block'\n",
    "\n",
    "sys.argv = [\"\", INPUT_ND2_PTH, tif_dir, STACK_LENGTH, num_frames]\n",
    "\n",
    "# Call the main function\n",
    "drift_estimation.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6588494",
   "metadata": {},
   "source": [
    "## 5. SEGMENT\n",
    "\n",
    "- outputs `label.tif`, contains segmented MIP for each volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f60d1",
   "metadata": {},
   "source": [
    "__on home pc__: \n",
    "`conda activate segment-torch`\n",
    "\n",
    "Uses a separate conda environment from the rest of the pipeline. create it using:\n",
    "`conda env create -f segment_torch.yml`\n",
    "\n",
    "__on lab pc__: \n",
    "`conda activate torchcu129`\n",
    "\n",
    "Uses a separate conda environment from the rest of the pipeline. create it following steps in:\n",
    "`segment_torch_cu129_environment.yml`\n",
    "\n",
    "### setup each time model weights change\n",
    "Need to set path to model weights as `CHECKPOINT` in `eval_torch.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segment.segment_torch\n",
    "_ = importlib.reload(sys.modules['segment.segment_torch'])\n",
    "\n",
    "MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'mip_bleach_corrected_RFP_block.tif')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", MIP_PTH]\n",
    "\n",
    "segment.segment_torch.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989c725",
   "metadata": {},
   "source": [
    "## 6. SPLINE\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `spline.json`, `spline.tif`, and `dilated.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac034f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spline\n",
    "_ = importlib.reload(sys.modules['spline'])\n",
    "\n",
    "LABEL_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'label.tif')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", LABEL_PTH]\n",
    "\n",
    "spline.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb7360",
   "metadata": {},
   "source": [
    "## 7. ORIENT\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- outputs `oriented.json`, `oriented.png`, `oriented_stack.tif`\n",
    "\n",
    "NOTE: `orient_v2.py` automated the process of finding orientation completely, whereas `orient.py` requires you to input the (x,y) nose location on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orient\n",
    "_ = importlib.reload(sys.modules['orient'])\n",
    "\n",
    "SPLINE_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'spline.json')\n",
    "nose_y = 250\n",
    "nose_x = 45\n",
    "\n",
    "# apply constraints\n",
    "# might need this when there are frames where the spline fitting fails and orientation is lost intermittently\n",
    "constrain_frame = 515\n",
    "constrain_frame_nose_y = 288\n",
    "constrain_frame_nose_x = 180\n",
    "\n",
    "# command-line arguments\n",
    "# sys.argv = [\"\", SPLINE_PTH, str(nose_y), str(nose_x)]\n",
    "sys.argv = [\"\", SPLINE_PTH, str(nose_y), str(nose_x), str(constrain_frame), str(constrain_frame_nose_y), str(constrain_frame_nose_x)]\n",
    "\n",
    "orient.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orient_v2 # tried to automate finding nose point, not working well at the moment\n",
    "_ = importlib.reload(sys.modules['orient_v2'])\n",
    "\n",
    "SPLINE_PTH = MIP_PTH = os.path.join(os.path.splitext(INPUT_ND2_PTH)[0], 'spline.json')\n",
    "\n",
    "# command-line arguments\n",
    "sys.argv = [\"\", SPLINE_PTH]\n",
    "\n",
    "orient_v2.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae94ec6",
   "metadata": {},
   "source": [
    "## 8. WARP\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "- ouputs: `warped/*.tif` and `masks/*.tif`\n",
    "\n",
    "TODO: parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1eac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 684/684 [1:08:33<00:00,  6.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import warp\n",
    "_ = importlib.reload(sys.modules['warp'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "start_index = 516\n",
    "end_index = num_frames\n",
    "\n",
    "for i in tqdm(range(start_index, end_index)):\n",
    "    # command-line arguments\n",
    "    sys.argv = [\"\", PTH, i]\n",
    "\n",
    "    warp.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ddcbc",
   "metadata": {},
   "source": [
    "## 9. REGISTER\n",
    "\n",
    "`conda activate g5ht-pipeline`\n",
    "\n",
    "__ALTERNATIVELY__: register using the wholistic registration algorithm, currently in MATLAB\n",
    "\n",
    "TODO: parallelize / make faster\n",
    "\n",
    "- pick a good representative fixed frame that you want to register everything to\n",
    "  - copy it to the main output folder and name it `fixed_xxxx.tif`\n",
    "  - copy the corresponding mask and name it `fixed_mask_xxxx.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b722e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 248/978 [1:30:28<3:43:36, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing index 469: D:\\a\\im\\build\\cp311-abi3-win_amd64\\_deps\\elx-src\\Core\\Main\\itkElastixRegistrationMethod.hxx:389:\n",
      "ITK ERROR: ElastixRegistrationMethod(0000020C65C2F9F0): Internal elastix error: See elastix log (use LogToConsoleOn() or LogToFileOn()).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 261/978 [1:35:38<4:47:10, 24.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing index 483: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0483.tif'\n",
      "Error processing index 484: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0484.tif'\n",
      "Error processing index 485: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0485.tif'\n",
      "Error processing index 486: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0486.tif'\n",
      "Error processing index 487: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0487.tif'\n",
      "Error processing index 488: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0488.tif'\n",
      "Error processing index 489: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0489.tif'\n",
      "Error processing index 490: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0490.tif'\n",
      "Error processing index 491: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0491.tif'\n",
      "Error processing index 492: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0492.tif'\n",
      "Error processing index 493: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0493.tif'\n",
      "Error processing index 494: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0494.tif'\n",
      "Error processing index 495: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0495.tif'\n",
      "Error processing index 496: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0496.tif'\n",
      "Error processing index 497: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0497.tif'\n",
      "Error processing index 498: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0498.tif'\n",
      "Error processing index 499: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0499.tif'\n",
      "Error processing index 500: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0500.tif'\n",
      "Error processing index 501: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0501.tif'\n",
      "Error processing index 502: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0502.tif'\n",
      "Error processing index 503: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0503.tif'\n",
      "Error processing index 504: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0504.tif'\n",
      "Error processing index 505: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0505.tif'\n",
      "Error processing index 506: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0506.tif'\n",
      "Error processing index 507: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0507.tif'\n",
      "Error processing index 508: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0508.tif'\n",
      "Error processing index 509: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0509.tif'\n",
      "Error processing index 510: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0510.tif'\n",
      "Error processing index 511: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0511.tif'\n",
      "Error processing index 512: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0512.tif'\n",
      "Error processing index 513: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0513.tif'\n",
      "Error processing index 514: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0514.tif'\n",
      "Error processing index 515: [Errno 2] No such file or directory: 'D:\\\\DATA\\\\g5ht-free\\\\20260123\\\\date-20260123_strain-ISg5HT-nsIS180_condition-fedpatch_worm004\\\\warped\\\\0515.tif'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 978/978 [6:22:36<00:00, 23.47s/it]   \n"
     ]
    }
   ],
   "source": [
    "import reg\n",
    "_ = importlib.reload(sys.modules['reg'])\n",
    "\n",
    "PTH = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "\n",
    "start_index = 222\n",
    "end_index = num_frames\n",
    "zoom = 1 # albert was using 3\n",
    "# zoom = 3\n",
    "\n",
    "for i in tqdm(range(start_index, end_index)):\n",
    "    # command-line arguments\n",
    "    try:\n",
    "        sys.argv = [\"\", PTH, i, str(zoom)]\n",
    "        reg.main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {i}: {e}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd56099",
   "metadata": {},
   "source": [
    "### REGISTER WITH GFP+1 TO RFP\n",
    "\n",
    "TRIM LAST RFP ZSLICE, TRIM FIRST GFP ZSLICE\n",
    "\n",
    "seems to be that as of 20251204, all recordings were taken such that the i zslice in red channel corresponds to i+1 zslice in green channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "from reg_gfp_indexing import main as reg_worm\n",
    "\n",
    "PTH = r'C:\\Users\\munib\\POSTDOC\\DATA\\g5ht-free\\20251028\\date-20251028_time-1500_strain-ISg5HT_condition-starvedpatch_worm001_aligned'\n",
    "\n",
    "for i in tqdm(range(1200)):\n",
    "    # command-line arguments\n",
    "    sys.argv = [\"\", PTH, i, \"1\"]\n",
    "    reg_worm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183d57b",
   "metadata": {},
   "source": [
    "### MAKE MOVIES OF REGISTERED DATA (see `reg_microfilm.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f91372",
   "metadata": {},
   "source": [
    "### REGISTER SINGLE FRAMES WITH ERROR LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "import itk\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from itk import image_view_from_array\n",
    "\n",
    "#get channels out of stacks\n",
    "def register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack):\n",
    "\tfixed_rfp = fixed_stack[:, 1].copy()\n",
    "\tmoving_gfp, moving_rfp = moving_stack[:, 0].copy(), moving_stack[:, 1].copy()\n",
    "\n",
    "\t#initialize registration parameters\n",
    "\tparameter_object = itk.ParameterObject.New()\n",
    "\tdefault_rigid_parameter_map = parameter_object.GetDefaultParameterMap('rigid', 4)\n",
    "\tparameter_object.AddParameterMap(default_rigid_parameter_map)\n",
    "\tdefault_affine_parameter_map = parameter_object.GetDefaultParameterMap('affine', 4)\n",
    "\tparameter_object.AddParameterMap(default_affine_parameter_map)\n",
    "\tdefault_bspline_128_parameter_map = parameter_object.GetDefaultParameterMap('bspline', 4, 128)\n",
    "\tparameter_object.AddParameterMap(default_bspline_128_parameter_map)\n",
    "\tdefault_bspline_64_parameter_map = parameter_object.GetDefaultParameterMap('bspline', 4, 64)\n",
    "\tparameter_object.AddParameterMap(default_bspline_64_parameter_map)\n",
    "\tdefault_bspline_32_parameter_map = parameter_object.GetDefaultParameterMap('bspline', 4, 32)\n",
    "\tparameter_object.AddParameterMap(default_bspline_32_parameter_map)\n",
    "\n",
    "\t#convert to itk images\n",
    "\tfixed_rfp = itk.image_view_from_array(fixed_rfp.astype(np.float32))\n",
    "\tmoving_rfp = itk.image_view_from_array(moving_rfp.astype(np.float32))\n",
    "\n",
    "\tfixed_mask_stack = itk.image_view_from_array(fixed_mask_stack.astype(np.ubyte))\n",
    "\tmoving_mask_stack = itk.image_view_from_array(moving_mask_stack.astype(np.ubyte))\n",
    "\n",
    "\t#register rfp first and then apply transform to gfp\n",
    "\n",
    "\tregistered_rfp, transform_parameters = itk.elastix_registration_method(fixed_rfp, moving_rfp, parameter_object,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   fixed_mask=fixed_mask_stack, moving_mask=moving_mask_stack,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   log_to_console=True)\n",
    "\tregistered_gfp = itk.transformix_filter(moving_gfp, transform_parameters)\n",
    "\n",
    "\t#initialize and fill output\n",
    "\toutput_stack = np.zeros((fixed_stack.shape[0], 2, 200, 500), np.uint16)\n",
    "\toutput_stack[:, 0] = np.clip(registered_gfp, 0, 4095)\n",
    "\toutput_stack[:, 1] = np.clip(registered_rfp, 0, 4095)\n",
    "\n",
    "\treturn output_stack\n",
    "    \n",
    "\t# # enablle elastic error logging\n",
    "\t# elastix_filter = itk.ElastixRegistrationMethod.New(fixed_rfp, moving_rfp, parameter_object,\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   fixed_mask=fixed_mask_stack, moving_mask=moving_mask_stack,\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   log_to_console=True) \n",
    "    \n",
    "\t# elastix_filter.SetParameterObject(parameter_object)\n",
    "\t# elastix_filter.SetNumberOfThreads(8)\n",
    "\t# elastix_filter.LogToConsoleOn()  # Enable console logging\n",
    "\t# elastix_filter.LogToFileOn()\n",
    "\t# elastix_filter.SetOutputDirectory(r\"C:\\Users\\munib\\POSTDOC\\CODE\\g5ht-pipeline\\logs\")\n",
    "\t# elastix_filter.Update()\n",
    "\n",
    "\t# return elastix_filter.GetOutput(), elastix_filter.GetTransformParameterObject()\n",
    "\n",
    "input_dir = os.path.splitext(INPUT_ND2_PTH)[0]\n",
    "warped_path = os.path.join(input_dir, 'warped')\n",
    "output_path = os.path.join(input_dir, 'registered_fixed_sweep')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "fixed_list = np.arange(0, 400, 50)\n",
    "mov_list = np.arange(0, 400, 30)\n",
    "\n",
    "for fixed in fixed_list:\n",
    "    for mov in mov_list:\n",
    "        print(f'Processing fixed: {fixed}, moving: {mov}')\n",
    "        \n",
    "\t\t# load stacks\n",
    "\t\n",
    "        moving_path = os.path.join(warped_path,f'{mov:04d}.tif')\n",
    "        moving_stack = tifffile.imread(moving_path).astype(np.float32)\n",
    "        fixed_path = os.path.join(warped_path,f'{fixed:04d}.tif')\n",
    "        fixed_stack = tifffile.imread(fixed_path).astype(np.float32)\n",
    "        fixed_mask_path = os.path.join(input_dir, 'masks', f'{fixed:04d}.tif')\n",
    "        fixed_mask = tifffile.imread(fixed_mask_path)\n",
    "        fixed_mask_stack = np.stack([fixed_mask] * fixed_stack.shape[0])\n",
    "\n",
    "        moving_mask_path = os.path.join(input_dir, 'masks', f'{mov:04d}.tif')\n",
    "        moving_mask = tifffile.imread(moving_mask_path)\n",
    "        moving_mask_stack = np.stack([moving_mask] * fixed_stack.shape[0])\n",
    "\n",
    "\n",
    "        output_stack = register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack)\n",
    "        # save output stack with fixed and moving indices in filename\n",
    "        output_file_path = os.path.join(output_path, f'fixed_{fixed:04d}_mov_{mov:04d}.tif')\n",
    "        tifffile.imwrite(output_file_path, output_stack, imagej=True)\n",
    "\n",
    "# fixed = 100\n",
    "# mov = 200\n",
    "\n",
    "# # load stacks\n",
    "# moving_path = os.path.join(warped_path,f'{mov:04d}.tif')\n",
    "# moving_stack = tifffile.imread(moving_path).astype(np.float32)\n",
    "# fixed_path = os.path.join(warped_path,f'{fixed:04d}.tif')\n",
    "# fixed_stack = tifffile.imread(fixed_path).astype(np.float32)\n",
    "# fixed_mask_path = os.path.join(input_dir, 'masks', f'{fixed:04d}.tif')\n",
    "# fixed_mask = tifffile.imread(fixed_mask_path)\n",
    "# fixed_mask_stack = np.stack([fixed_mask] * fixed_stack.shape[0])\n",
    "\n",
    "# moving_mask_path = os.path.join(input_dir, 'masks', f'{mov:04d}.tif')\n",
    "# moving_mask = tifffile.imread(moving_mask_path)\n",
    "# moving_mask_stack = np.stack([moving_mask] * fixed_stack.shape[0])\n",
    "\n",
    "\n",
    "# output_stack = register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stack = register_one(fixed_stack, fixed_mask_stack, moving_stack, moving_mask_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(fixed_stack[10,1,:,:])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(output_stack[10,1,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4144467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g5ht-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
